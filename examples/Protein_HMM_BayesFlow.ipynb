{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4400a643",
   "metadata": {},
   "source": [
    "# Protein Secondary Structure Inference with BayesFlow\n",
    "\n",
    "_Authors: Bhanu Prasanna, Simulation-Based Inference Course Project_\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates amortized Bayesian inference for protein secondary structure prediction using a two-state Hidden Markov Model (HMM) and BayesFlow. The goal is to train a neural network to predict state membership probabilities (alpha-helix vs. other) from amino acid sequences, essentially learning an amortized approximation to the Forward-Backward algorithm.\n",
    "\n",
    "### Problem Setup\n",
    "\n",
    "We use a two-state HMM where:\n",
    "- **State 0 (\"other\")**: Beta-sheets and random coils\n",
    "- **State 1 (\"alpha-helix\")**: Alpha-helix secondary structure\n",
    "\n",
    "The HMM has fixed emission and transition probabilities based on empirical data from protein structure analysis. Given an amino acid sequence, we want to infer the probability that each position belongs to an alpha-helix or other structure.\n",
    "\n",
    "### Approach\n",
    "\n",
    "1. **Simulator**: Generate amino acid sequences using the HMM generative model\n",
    "2. **Forward-Backward**: Compute true state probabilities for each sequence\n",
    "3. **BayesFlow**: Train a neural network to map sequences → state probabilities\n",
    "4. **Validation**: Compare predictions to known protein structures (e.g., human insulin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd121e3",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "294c220b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'tensorflow' backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set backend for BayesFlow (adjust as needed)\n",
    "if \"KERAS_BACKEND\" not in os.environ:\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # or \"jax\", \"torch\"\n",
    "else:\n",
    "    print(f\"Using '{os.environ['KERAS_BACKEND']}' backend\")\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = \"/Users/bhanuprasanna/Documents/TU Dortmund/SS 25 - Simulation Based Interference/final\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff65ae8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BayesFlowModel' from 'src.bayesflow.networks' (/Users/bhanuprasanna/Documents/TU Dortmund/SS 25 - Simulation Based Interference/final/src/bayesflow/networks.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbayesflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbf\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Project-specific imports\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbayesflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msimulator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProteinSimulator, create_protein_simulator\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhmm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotein_hmm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProteinHMM\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_state_probabilities, plot_sequence_alignment\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/TU Dortmund/SS 25 - Simulation Based Interference/final/src/bayesflow/__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mBayesFlow implementation for protein secondary structure inference.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnetworks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      6\u001b[39m     SummaryNetwork,\n\u001b[32m      7\u001b[39m     InferenceNetwork, \n\u001b[32m      8\u001b[39m     BayesFlowModel,\n\u001b[32m      9\u001b[39m     create_summary_network,\n\u001b[32m     10\u001b[39m     create_inference_network,\n\u001b[32m     11\u001b[39m     create_bayesflow_model,\n\u001b[32m     12\u001b[39m     compute_loss\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BayesFlowTrainer, run_training_pipeline\n\u001b[32m     17\u001b[39m __all__ = [\n\u001b[32m     18\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mSummaryNetwork\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     19\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mInferenceNetwork\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrun_training_pipeline\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     27\u001b[39m ]\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'BayesFlowModel' from 'src.bayesflow.networks' (/Users/bhanuprasanna/Documents/TU Dortmund/SS 25 - Simulation Based Interference/final/src/bayesflow/networks.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from typing import Dict, Tuple, Any\n",
    "\n",
    "# BayesFlow import\n",
    "import bayesflow as bf\n",
    "\n",
    "# Project-specific imports\n",
    "from src.bayesflow.simulator import ProteinSimulator, create_protein_simulator\n",
    "from src.hmm.protein_hmm import ProteinHMM\n",
    "from src.utils.visualization import plot_state_probabilities, plot_sequence_alignment\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ All imports successful!\")\n",
    "print(f\"✓ BayesFlow version: {bf.__version__}\")\n",
    "print(f\"✓ Using Keras backend: {os.environ.get('KERAS_BACKEND', 'default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7101e16d",
   "metadata": {},
   "source": [
    "## Simulator: Protein Secondary Structure HMM\n",
    "\n",
    "Our simulator generates amino acid sequences using a two-state HMM and computes state probabilities using the Forward-Backward algorithm. This creates training pairs of (sequence, state_probabilities) for the neural network to learn from.\n",
    "\n",
    "### HMM Configuration\n",
    "\n",
    "The HMM uses empirically-derived emission and transition probabilities:\n",
    "\n",
    "**Transition Probabilities:**\n",
    "- From \"other\" → \"alpha-helix\": 5%\n",
    "- From \"other\" → \"other\": 95%  \n",
    "- From \"alpha-helix\" → \"alpha-helix\": 90%\n",
    "- From \"alpha-helix\" → \"other\": 10%\n",
    "\n",
    "**Emission Probabilities:** Different probability distributions over 20 amino acids for each state, based on structural analysis of known proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34851dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the protein simulator\n",
    "simulator = create_protein_simulator(\n",
    "    min_length=50,\n",
    "    max_length=150,\n",
    "    fixed_length=False  # Variable length sequences for more realistic training\n",
    ")\n",
    "\n",
    "print(\"Simulator Configuration:\")\n",
    "info = simulator.get_simulator_info()\n",
    "for key, value in info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nAmino acids: {info['amino_acids']}\")\n",
    "print(f\"States: {info['states']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a38515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch of simulations to see the output format\n",
    "batch_size = 4\n",
    "sim_batch = simulator(batch_size=batch_size)\n",
    "\n",
    "print(\"Simulator Output Structure:\")\n",
    "print(f\"Keys: {list(sim_batch.keys())}\")\n",
    "print()\n",
    "\n",
    "print(\"Summary Conditions (sequences and metadata):\")\n",
    "summary_cond = sim_batch['summary_conditions']\n",
    "for key, value in summary_cond.items():\n",
    "    print(f\"  {key}: shape {value.shape}, dtype {value.dtype}\")\n",
    "\n",
    "print(f\"\\nParameters (state probabilities):\")\n",
    "params = sim_batch['parameters']\n",
    "print(f\"  state_probs: shape {params.shape}, dtype {params.dtype}\")\n",
    "\n",
    "print(f\"\\nExample sequence lengths: {summary_cond['lengths']}\")\n",
    "print(f\"Max sequence length in batch: {summary_cond['sequences'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7105c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a specific sequence from the batch\n",
    "seq_idx = 0\n",
    "seq_length = summary_cond['lengths'][seq_idx]\n",
    "sequence_indices = summary_cond['sequences'][seq_idx, :seq_length]\n",
    "state_probs = sim_batch['parameters'][seq_idx, :seq_length, :]\n",
    "mask = summary_cond['masks'][seq_idx, :seq_length]\n",
    "\n",
    "print(f\"Example Sequence {seq_idx + 1} (length: {seq_length}):\")\n",
    "print(f\"Sequence indices: {sequence_indices}\")\n",
    "print(f\"Mask: {mask}\")\n",
    "print(f\"State probabilities shape: {state_probs.shape}\")\n",
    "print(f\"State probs (first 10 positions):\")\n",
    "print(f\"  P(other): {state_probs[:10, 0]}\")\n",
    "print(f\"  P(alpha): {state_probs[:10, 1]}\")\n",
    "\n",
    "# Convert indices back to amino acids for display\n",
    "amino_acids = info['amino_acids']\n",
    "sequence_str = ''.join([amino_acids[i] for i in sequence_indices])\n",
    "print(f\"\\nAmino acid sequence: {sequence_str}\")\n",
    "\n",
    "# Quick visualization of state probabilities\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "# Plot state probabilities\n",
    "positions = np.arange(seq_length)\n",
    "ax1.plot(positions, state_probs[:, 0], 'b-', label='P(other)', linewidth=2)\n",
    "ax1.plot(positions, state_probs[:, 1], 'r-', label='P(alpha-helix)', linewidth=2)\n",
    "ax1.fill_between(positions, 0, state_probs[:, 1], alpha=0.3, color='red')\n",
    "ax1.set_ylabel('State Probability')\n",
    "ax1.set_title(f'State Probabilities for Sequence {seq_idx + 1}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot amino acid sequence as bars colored by most likely state\n",
    "most_likely_state = np.argmax(state_probs, axis=1)\n",
    "colors = ['blue' if s == 0 else 'red' for s in most_likely_state]\n",
    "ax2.bar(positions, np.ones(seq_length), color=colors, alpha=0.7)\n",
    "ax2.set_ylabel('Amino Acid')\n",
    "ax2.set_xlabel('Position')\n",
    "ax2.set_title('Most Likely State Assignment (Blue=Other, Red=Alpha-helix)')\n",
    "ax2.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339d922",
   "metadata": {},
   "source": [
    "## Adapter: Data Preparation for BayesFlow\n",
    "\n",
    "The adapter transforms simulator outputs into the format expected by BayesFlow networks. This includes:\n",
    "- Converting data types to float32 for deep learning\n",
    "- Standardizing sequences and masks \n",
    "- Renaming keys to match BayesFlow conventions\n",
    "- Preparing variable-length sequences for batch processing\n",
    "\n",
    "BayesFlow expects the following keys:\n",
    "- `summary_conditions`: Data to be processed by summary networks (sequences, masks, metadata)\n",
    "- `inference_variables`: Target variables to be predicted (state probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e08c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the adapter for preprocessing simulator outputs\n",
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    # Convert data types to deep learning friendly formats\n",
    "    .convert_dtype(\"int32\", \"float32\")  # sequences as float32\n",
    "    .convert_dtype(\"float64\", \"float32\")  # state probabilities as float32\n",
    "    \n",
    "    # Rename variables to match BayesFlow conventions\n",
    "    # summary_conditions contains the input data (sequences, masks, lengths)\n",
    "    .rename(\"summary_conditions\", \"summary_conditions\")\n",
    "    # inference_variables contains the target (state probabilities)\n",
    "    .rename(\"parameters\", \"inference_variables\")\n",
    ")\n",
    "\n",
    "print(\"Adapter configuration:\")\n",
    "print(adapter)\n",
    "\n",
    "# Test the adapter on our sample batch\n",
    "adapted_batch = adapter(sim_batch)\n",
    "\n",
    "print(\"\\nAfter adaptation:\")\n",
    "print(f\"Keys: {list(adapted_batch.keys())}\")\n",
    "print()\n",
    "\n",
    "for key, value in adapted_batch.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"{key}:\")\n",
    "        for subkey, subvalue in value.items():\n",
    "            print(f\"  {subkey}: shape {subvalue.shape}, dtype {subvalue.dtype}\")\n",
    "    else:\n",
    "        print(f\"{key}: shape {value.shape}, dtype {value.dtype}\")\n",
    "\n",
    "print(f\"\\nData ready for BayesFlow training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed23de6",
   "metadata": {},
   "source": [
    "## Generate Training and Validation Data\n",
    "\n",
    "For this example, we'll generate offline training and validation datasets. In practice, BayesFlow also supports online training where data is generated on-the-fly during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf2a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for data generation\n",
    "num_training_samples = 5000  # Moderate size for demo - increase for better performance\n",
    "num_validation_samples = 500\n",
    "batch_size = 64\n",
    "epochs = 30  # Moderate training for demo\n",
    "\n",
    "print(\"Generating training data...\")\n",
    "# Generate training data\n",
    "training_batches = []\n",
    "total_generated = 0\n",
    "while total_generated < num_training_samples:\n",
    "    current_batch_size = min(batch_size, num_training_samples - total_generated)\n",
    "    batch = simulator(batch_size=current_batch_size)\n",
    "    adapted_batch = adapter(batch)\n",
    "    training_batches.append(adapted_batch)\n",
    "    total_generated += current_batch_size\n",
    "    if total_generated % 1000 == 0:\n",
    "        print(f\"  Generated {total_generated}/{num_training_samples} samples\")\n",
    "\n",
    "# Combine batches into single training dataset\n",
    "def combine_batches(batches):\n",
    "    \"\"\"Combine list of batches into single dataset\"\"\"\n",
    "    combined = {}\n",
    "    for key in batches[0].keys():\n",
    "        if isinstance(batches[0][key], dict):\n",
    "            combined[key] = {}\n",
    "            for subkey in batches[0][key].keys():\n",
    "                combined[key][subkey] = np.concatenate([b[key][subkey] for b in batches], axis=0)\n",
    "        else:\n",
    "            combined[key] = np.concatenate([b[key] for b in batches], axis=0)\n",
    "    return combined\n",
    "\n",
    "training_data = combine_batches(training_batches)\n",
    "\n",
    "print(\"Generating validation data...\")\n",
    "validation_data = adapter(simulator(batch_size=num_validation_samples))\n",
    "\n",
    "print(f\"✓ Training data: {training_data['inference_variables'].shape[0]} samples\")\n",
    "print(f\"✓ Validation data: {validation_data['inference_variables'].shape[0]} samples\")\n",
    "\n",
    "# Quick statistics on sequence lengths\n",
    "train_lengths = training_data['summary_conditions']['lengths']\n",
    "val_lengths = validation_data['summary_conditions']['lengths']\n",
    "\n",
    "print(f\"\\nSequence length statistics:\")\n",
    "print(f\"Training - min: {train_lengths.min()}, max: {train_lengths.max()}, mean: {train_lengths.mean():.1f}\")\n",
    "print(f\"Validation - min: {val_lengths.min()}, max: {val_lengths.max()}, mean: {val_lengths.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8abc993",
   "metadata": {},
   "source": [
    "## Define and Configure Neural Network Approximator\n",
    "\n",
    "We'll set up a BayesFlow architecture consisting of:\n",
    "1. **Summary Network**: Processes variable-length amino acid sequences into fixed-size representations\n",
    "2. **Inference Network**: Maps sequence representations to state probability predictions\n",
    "\n",
    "For this protein sequence task, we'll use a Flow Matching network as the backbone, which can handle the complex multimodal nature of protein structure prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom summary network for protein sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "class ProteinSummaryNetwork(bf.networks.SummaryNetwork):\n",
    "    \"\"\"\n",
    "    Summary network for processing variable-length amino acid sequences.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Embedding layer for amino acid indices\n",
    "    2. LSTM for sequence processing  \n",
    "    3. Attention mechanism for important positions\n",
    "    4. Dense layers for final representation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size=20,\n",
    "                 embedding_dim=32,\n",
    "                 lstm_units=64,\n",
    "                 attention_units=32,\n",
    "                 summary_dim=64,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.attention_units = attention_units\n",
    "        self.summary_dim = summary_dim\n",
    "        \n",
    "        # Embedding for amino acid indices\n",
    "        self.embedding = keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            mask_zero=True,\n",
    "            name=\"amino_acid_embedding\"\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM for sequence processing\n",
    "        self.lstm = keras.layers.Bidirectional(\n",
    "            keras.layers.LSTM(lstm_units, return_sequences=True),\n",
    "            name=\"sequence_lstm\"\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = keras.layers.MultiHeadAttention(\n",
    "            num_heads=4,\n",
    "            key_dim=attention_units,\n",
    "            name=\"sequence_attention\"\n",
    "        )\n",
    "        \n",
    "        # Global pooling and final layers\n",
    "        self.global_pool = keras.layers.GlobalAveragePooling1D()\n",
    "        self.dense1 = keras.layers.Dense(summary_dim, activation='relu')\n",
    "        self.dropout = keras.layers.Dropout(0.1)\n",
    "        self.dense2 = keras.layers.Dense(summary_dim, activation='relu')\n",
    "        \n",
    "    def call(self, summary_conditions, **kwargs):\n",
    "        \"\"\"\n",
    "        Process protein sequences into fixed-size summaries.\n",
    "        \n",
    "        Args:\n",
    "            summary_conditions: Dict containing 'sequences', 'masks', 'lengths'\n",
    "        \"\"\"\n",
    "        sequences = summary_conditions['sequences']  # [batch, max_length]\n",
    "        masks = summary_conditions['masks']          # [batch, max_length]\n",
    "        \n",
    "        # Embed amino acid sequences\n",
    "        embedded = self.embedding(sequences)  # [batch, max_length, embedding_dim]\n",
    "        \n",
    "        # Apply sequence mask for padding\n",
    "        mask_expanded = tf.expand_dims(masks, -1)\n",
    "        embedded = embedded * mask_expanded\n",
    "        \n",
    "        # Process with LSTM\n",
    "        lstm_out = self.lstm(embedded)  # [batch, max_length, 2*lstm_units]\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attended = self.attention(lstm_out, lstm_out, attention_mask=masks)\n",
    "        \n",
    "        # Global pooling to get fixed-size representation\n",
    "        pooled = self.global_pool(attended)  # [batch, 2*lstm_units]\n",
    "        \n",
    "        # Final dense layers\n",
    "        x = self.dense1(pooled)\n",
    "        x = self.dropout(x, training=kwargs.get(\"stage\") == \"training\")\n",
    "        summary = self.dense2(x)  # [batch, summary_dim]\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Create the summary network\n",
    "summary_network = ProteinSummaryNetwork(\n",
    "    vocab_size=20,  # 20 amino acids\n",
    "    embedding_dim=32,\n",
    "    lstm_units=64,\n",
    "    attention_units=32,\n",
    "    summary_dim=64\n",
    ")\n",
    "\n",
    "print(\"✓ Summary network created\")\n",
    "print(f\"  Architecture: Embedding({20}) → LSTM({64}) → Attention → Dense({64})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a6b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the inference network\n",
    "# Flow Matching is well-suited for protein structure prediction due to its ability\n",
    "# to handle complex, multimodal posterior distributions\n",
    "inference_network = bf.networks.FlowMatching(\n",
    "    subnet=\"mlp\",\n",
    "    subnet_kwargs={\n",
    "        \"dropout\": 0.1,\n",
    "        \"widths\": (128, 128, 64)  # Network architecture for flow matching\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✓ Inference network created\")\n",
    "print(f\"  Type: Flow Matching\")\n",
    "print(f\"  Architecture: MLP with layers {(128, 128, 64)}\")\n",
    "\n",
    "# Test the networks with a small batch to ensure compatibility\n",
    "print(\"\\nTesting network compatibility...\")\n",
    "test_batch = adapter(simulator(batch_size=2))\n",
    "\n",
    "# Test summary network\n",
    "try:\n",
    "    summary_output = summary_network(test_batch['summary_conditions'])\n",
    "    print(f\"✓ Summary network test: input {test_batch['summary_conditions']['sequences'].shape} → output {summary_output.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Summary network test failed: {e}\")\n",
    "\n",
    "# Get expected shapes for inference network\n",
    "max_seq_len = test_batch['summary_conditions']['sequences'].shape[1]\n",
    "state_prob_dim = test_batch['inference_variables'].shape[-1]  # Should be 2 (binary states)\n",
    "\n",
    "print(f\"  Expected inference input dim: {state_prob_dim * max_seq_len}\")\n",
    "print(f\"  State probability dimensionality: {state_prob_dim}\")\n",
    "print(f\"  Maximum sequence length: {max_seq_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f2b4e1",
   "metadata": {},
   "source": [
    "## Workflow: Training the Posterior Approximator\n",
    "\n",
    "Now we'll combine all components into a BayesFlow workflow and train the network to learn the mapping from amino acid sequences to state probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BayesFlow workflow\n",
    "workflow = bf.BasicWorkflow(\n",
    "    simulator=simulator,\n",
    "    adapter=adapter,\n",
    "    inference_network=inference_network,\n",
    "    summary_network=summary_network,\n",
    "    # Standardize the state probabilities for better training stability\n",
    "    standardize=[\"inference_variables\"]\n",
    ")\n",
    "\n",
    "print(\"✓ Workflow created successfully\")\n",
    "print(\"  Components:\")\n",
    "print(f\"    - Simulator: {type(simulator).__name__}\")\n",
    "print(f\"    - Adapter: BayesFlow Adapter\")\n",
    "print(f\"    - Summary Network: {type(summary_network).__name__}\")\n",
    "print(f\"    - Inference Network: {type(inference_network).__name__}\")\n",
    "\n",
    "# Train the workflow\n",
    "print(f\"\\nStarting training...\")\n",
    "print(f\"  Training samples: {training_data['inference_variables'].shape[0]}\")\n",
    "print(f\"  Validation samples: {validation_data['inference_variables'].shape[0]}\")\n",
    "print(f\"  Epochs: {epochs}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "\n",
    "history = workflow.fit_offline(\n",
    "    data=training_data,\n",
    "    validation_data=validation_data,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"✓ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d656aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "f = bf.diagnostics.plots.loss(history, figsize=(12, 4))\n",
    "plt.suptitle(\"Training Progress: Protein HMM Posterior Approximation\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Extract some training metrics\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "best_val_loss = min(history.history['val_loss'])\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Final training loss: {final_train_loss:.4f}\")\n",
    "print(f\"  Final validation loss: {final_val_loss:.4f}\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Training epochs: {len(history.history['loss'])}\")\n",
    "\n",
    "if final_val_loss < final_train_loss * 2:\n",
    "    print(\"✓ Model appears well-trained (no severe overfitting)\")\n",
    "else:\n",
    "    print(\"⚠ Possible overfitting detected - consider more regularization or data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154655a4",
   "metadata": {},
   "source": [
    "## Validation: Posterior Diagnostics and Calibration\n",
    "\n",
    "We'll validate our trained model using simulation-based calibration (SBC) and other diagnostics to ensure the posterior approximation is accurate and well-calibrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bbace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data for validation\n",
    "num_test_samples = 300\n",
    "num_posterior_samples = 1000\n",
    "\n",
    "print(\"Generating test data for validation...\")\n",
    "test_data = adapter(simulator(batch_size=num_test_samples))\n",
    "\n",
    "print(\"Sampling from approximate posterior...\")\n",
    "posterior_samples = workflow.sample(\n",
    "    conditions=test_data, \n",
    "    num_samples=num_posterior_samples\n",
    ")\n",
    "\n",
    "print(f\"✓ Generated {num_posterior_samples} posterior samples for {num_test_samples} test cases\")\n",
    "print(f\"Posterior samples shape: {posterior_samples['inference_variables'].shape}\")\n",
    "\n",
    "# Run automated diagnostics\n",
    "print(\"\\nComputing diagnostic metrics...\")\n",
    "metrics = workflow.compute_default_diagnostics(test_data=num_test_samples)\n",
    "\n",
    "print(\"Diagnostic Results:\")\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    if isinstance(metric_value, (int, float)):\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric_name}: {metric_value}\")\n",
    "\n",
    "# Generate diagnostic plots\n",
    "print(\"\\nGenerating diagnostic plots...\")\n",
    "diagnostic_figures = workflow.plot_default_diagnostics(\n",
    "    test_data=num_test_samples,\n",
    "    calibration_ecdf_kwargs={\"difference\": True, \"figsize\": (15, 3)},\n",
    "    recovery_kwargs={\"figsize\": (15, 3)},\n",
    "    z_score_contraction_kwargs={\"figsize\": (15, 3)}\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cbf4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual validation: Compare predictions to ground truth for specific examples\n",
    "print(\"Manual Validation: Detailed comparison for selected sequences\")\n",
    "\n",
    "# Select a few test cases for detailed analysis\n",
    "test_indices = [0, 1, 2]\n",
    "for idx in test_indices:\n",
    "    print(f\"\\n--- Test Sequence {idx + 1} ---\")\n",
    "    \n",
    "    # Get true state probabilities\n",
    "    true_probs = test_data['inference_variables'][idx]\n",
    "    seq_length = int(test_data['summary_conditions']['lengths'][idx])\n",
    "    \n",
    "    # Get posterior samples for this sequence\n",
    "    predicted_probs = posterior_samples['inference_variables'][idx]  # [num_samples, max_len, 2]\n",
    "    \n",
    "    # Compute posterior statistics\n",
    "    mean_probs = np.mean(predicted_probs, axis=0)[:seq_length]  # [seq_length, 2]\n",
    "    std_probs = np.std(predicted_probs, axis=0)[:seq_length]   # [seq_length, 2]\n",
    "    \n",
    "    # Truncate true probabilities to actual sequence length\n",
    "    true_probs_truncated = true_probs[:seq_length]\n",
    "    \n",
    "    # Compute metrics\n",
    "    mse = np.mean((mean_probs - true_probs_truncated) ** 2)\n",
    "    mae = np.mean(np.abs(mean_probs - true_probs_truncated))\n",
    "    \n",
    "    print(f\"  Sequence length: {seq_length}\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    \n",
    "    # Plot comparison for this sequence\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    positions = np.arange(seq_length)\n",
    "    \n",
    "    # Plot state probabilities - Other state\n",
    "    axes[0].plot(positions, true_probs_truncated[:, 0], 'b-', linewidth=3, label='True P(other)')\n",
    "    axes[0].plot(positions, mean_probs[:, 0], 'r--', linewidth=2, label='Predicted P(other)')\n",
    "    axes[0].fill_between(positions, \n",
    "                        mean_probs[:, 0] - 2*std_probs[:, 0],\n",
    "                        mean_probs[:, 0] + 2*std_probs[:, 0],\n",
    "                        alpha=0.2, color='red', label='95% CI')\n",
    "    axes[0].set_ylabel('P(other)')\n",
    "    axes[0].set_title(f'Test Sequence {idx + 1}: Other State Probabilities')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot state probabilities - Alpha-helix state  \n",
    "    axes[1].plot(positions, true_probs_truncated[:, 1], 'b-', linewidth=3, label='True P(alpha-helix)')\n",
    "    axes[1].plot(positions, mean_probs[:, 1], 'r--', linewidth=2, label='Predicted P(alpha-helix)')\n",
    "    axes[1].fill_between(positions,\n",
    "                        mean_probs[:, 1] - 2*std_probs[:, 1], \n",
    "                        mean_probs[:, 1] + 2*std_probs[:, 1],\n",
    "                        alpha=0.2, color='red', label='95% CI')\n",
    "    axes[1].set_ylabel('P(alpha-helix)')\n",
    "    axes[1].set_xlabel('Sequence Position')\n",
    "    axes[1].set_title(f'Test Sequence {idx + 1}: Alpha-helix State Probabilities')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n✓ Manual validation completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
