{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1197c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:12:47.229537: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-07-13 15:12:47.229570: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-07-13 15:12:47.229577: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752412367.229588 6598229 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1752412367.229615 6598229 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "INFO:bayesflow:Using backend 'tensorflow'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.keras is using the 'tensorflow' backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import bayesflow as bf\n",
    "\n",
    "from hmmlearn import hmm\n",
    "from hmmlearn.hmm import CategoricalHMM\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "current_backend = tf.keras.backend.backend()\n",
    "print(f\"tf.keras is using the '{current_backend}' backend.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ebc9ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETER VALIDATION:\n",
      "Amino acids: 20 types\n",
      "Alpha emission sum: 1.000\n",
      "Other emission sum: 1.000\n",
      "Alpha transitions sum: 1.000\n",
      "Other transitions sum: 1.000\n",
      "Initial probs sum: 1.000\n",
      "\n",
      "âœ“ All probabilities are valid!\n"
     ]
    }
   ],
   "source": [
    "# HMM PARAMETERS FROM TASK DESCRIPTION\n",
    "\n",
    "# 20 amino acids in standard order\n",
    "AMINO_ACIDS = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I', \n",
    "               'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "\n",
    "# Emission probabilities from task tables\n",
    "# Alpha-helix state (state 0)\n",
    "EMISSION_ALPHA = [0.12, 0.06, 0.03, 0.05, 0.01, 0.09, 0.05, 0.04, 0.02, 0.07,\n",
    "                  0.12, 0.06, 0.03, 0.04, 0.02, 0.05, 0.04, 0.01, 0.03, 0.06]\n",
    "\n",
    "# Other state (state 1) \n",
    "EMISSION_OTHER = [0.06, 0.05, 0.05, 0.06, 0.02, 0.05, 0.03, 0.09, 0.03, 0.05,\n",
    "                  0.08, 0.06, 0.02, 0.04, 0.06, 0.07, 0.06, 0.01, 0.04, 0.07]\n",
    "\n",
    "# Transition probabilities from task description\n",
    "# [alpha->alpha, alpha->other]\n",
    "TRANS_FROM_ALPHA = [0.90, 0.10]\n",
    "# [other->alpha, other->other]  \n",
    "TRANS_FROM_OTHER = [0.05, 0.95]\n",
    "\n",
    "# Initial state probabilities (always starts in \"other\" state)\n",
    "INITIAL_PROBS = [0.0, 1.0]  # [alpha-helix, other]\n",
    "\n",
    "# Validation\n",
    "print(\"PARAMETER VALIDATION:\")\n",
    "print(f\"Amino acids: {len(AMINO_ACIDS)} types\")\n",
    "print(f\"Alpha emission sum: {sum(EMISSION_ALPHA):.3f}\")\n",
    "print(f\"Other emission sum: {sum(EMISSION_OTHER):.3f}\")\n",
    "print(f\"Alpha transitions sum: {sum(TRANS_FROM_ALPHA):.3f}\")\n",
    "print(f\"Other transitions sum: {sum(TRANS_FROM_OTHER):.3f}\")\n",
    "print(f\"Initial probs sum: {sum(INITIAL_PROBS):.3f}\")\n",
    "print(\"\\nâœ“ All probabilities are valid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a0f7228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING HMM CREATION:\n",
      "\n",
      "States: 2\n",
      "Features: 20\n",
      "Start probabilities: [0. 1.]\n",
      "Transition matrix shape: (2, 2)\n",
      "Emission matrix shape: (2, 20)\n",
      "\n",
      "Transition probabilities:\n",
      "From alpha-helix: [0.9 0.1]\n",
      "From other:      [0.05 0.95]\n",
      "\n",
      "Emission probabilities (first 5 amino acids):\n",
      "Alpha-helix: [0.12 0.06 0.03 0.05 0.01]\n",
      "Other:       [0.06 0.05 0.05 0.06 0.02]\n",
      "\n",
      "âœ“ HMM model created successfully!\n"
     ]
    }
   ],
   "source": [
    "# FIXED HMM MODEL CREATION\n",
    "\n",
    "def create_fixed_hmm():\n",
    "    \"\"\"\n",
    "    Create HMM with fixed parameters from task description.\n",
    "    \n",
    "    States: 0=alpha-helix, 1=other\n",
    "    Features: 20 amino acids (0-19 indices)\n",
    "    \n",
    "    Returns:\n",
    "        CategoricalHMM with fixed empirical parameters\n",
    "    \"\"\"\n",
    "    # Create model with fixed parameters (no learning)\n",
    "    model = hmm.CategoricalHMM(\n",
    "        n_components=2,        # 2 states: alpha-helix, other\n",
    "        n_features=20,         # 20 amino acids\n",
    "        params=\"\",             # Don't update any parameters\n",
    "        init_params=\"\",        # Don't initialize any parameters\n",
    "        algorithm=\"viterbi\",   # Use Viterbi algorithm for decoding\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Set fixed parameters from task description\n",
    "    model.startprob_ = np.array(INITIAL_PROBS)\n",
    "    model.transmat_ = np.array([TRANS_FROM_ALPHA, TRANS_FROM_OTHER])\n",
    "    model.emissionprob_ = np.array([EMISSION_ALPHA, EMISSION_OTHER])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test HMM creation\n",
    "print(\"TESTING HMM CREATION:\\n\")\n",
    "hmm_model = create_fixed_hmm()\n",
    "\n",
    "print(f\"States: {hmm_model.n_components}\")\n",
    "print(f\"Features: {hmm_model.n_features}\")\n",
    "print(f\"Start probabilities: {hmm_model.startprob_}\")\n",
    "print(f\"Transition matrix shape: {hmm_model.transmat_.shape}\")\n",
    "print(f\"Emission matrix shape: {hmm_model.emissionprob_.shape}\")\n",
    "\n",
    "print(\"\\nTransition probabilities:\")\n",
    "print(\"From alpha-helix:\", hmm_model.transmat_[0])\n",
    "print(\"From other:     \", hmm_model.transmat_[1])\n",
    "\n",
    "print(\"\\nEmission probabilities (first 5 amino acids):\")\n",
    "print(\"Alpha-helix:\", hmm_model.emissionprob_[0][:5])\n",
    "print(\"Other:      \", hmm_model.emissionprob_[1][:5])\n",
    "print(\"\\nâœ“ HMM model created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c5391f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING HMM DATA GENERATION:\n",
      "\n",
      "Amino acids shape: (20,)\n",
      "True states shape: (20,)\n",
      "State probabilities shape: (20, 2)\n",
      "\n",
      "First 10 amino acids (indices): [19 11  2 16 14 19  3  2  9  5]\n",
      "First 10 true states: [1 1 1 1 1 0 0 0 0 0]\n",
      "First 5 state probabilities:\n",
      "[[0.         1.        ]\n",
      " [0.01768884 0.98231116]\n",
      " [0.0253218  0.9746782 ]\n",
      " [0.03656372 0.96343628]\n",
      " [0.05153765 0.94846235]]\n",
      "\n",
      "State probabilities sum check: True\n",
      "First 10 amino acids (letters): ['V', 'K', 'N', 'T', 'P', 'V', 'D', 'N', 'I', 'E']\n",
      "\n",
      "âœ“ HMM data generation working correctly!\n"
     ]
    }
   ],
   "source": [
    "# HMM DATA GENERATION AND SIMULATOR FUNCTIONS\n",
    "\n",
    "def generate_amino_acid_sequence(n_samples=50, random_state=None):\n",
    "    \"\"\"\n",
    "    Generate amino acid sequences from the fixed HMM.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of amino acids to generate\n",
    "        random_state: Random state for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        dict with 'amino_acids', 'true_states', and 'state_probs'\n",
    "    \"\"\"\n",
    "    # Create the fixed HMM model\n",
    "    model = create_fixed_hmm()\n",
    "    \n",
    "    # Generate sequence from HMM\n",
    "    X, Z = model.sample(n_samples, random_state=random_state)\n",
    "    \n",
    "    # X is shape (n_samples, 1) - amino acid indices\n",
    "    # Z is shape (n_samples,) - true hidden states\n",
    "    amino_acids = X.flatten()  # Convert to 1D array of amino acid indices\n",
    "    \n",
    "    # Get state membership probabilities using Forward-Backward algorithm\n",
    "    # Need to reshape X for predict_proba (expects (n_samples, 1))\n",
    "    state_probs = model.predict_proba(X)  # Shape: (n_samples, n_states)\n",
    "    \n",
    "    return {\n",
    "        'amino_acids': amino_acids,       # Shape: (n_samples,) - amino acid indices (0-19)\n",
    "        'true_states': Z,                 # Shape: (n_samples,) - true hidden states (0=alpha, 1=other) \n",
    "        'state_probs': state_probs        # Shape: (n_samples, 2) - state membership probabilities\n",
    "    }\n",
    "\n",
    "# Test the data generation\n",
    "print(\"TESTING HMM DATA GENERATION:\\n\")\n",
    "test_data = generate_amino_acid_sequence(n_samples=20, random_state=42)\n",
    "\n",
    "print(f\"Amino acids shape: {test_data['amino_acids'].shape}\")\n",
    "print(f\"True states shape: {test_data['true_states'].shape}\")\n",
    "print(f\"State probabilities shape: {test_data['state_probs'].shape}\")\n",
    "\n",
    "print(f\"\\nFirst 10 amino acids (indices): {test_data['amino_acids'][:10]}\")\n",
    "print(f\"First 10 true states: {test_data['true_states'][:10]}\")\n",
    "print(f\"First 5 state probabilities:\\n{test_data['state_probs'][:5]}\")\n",
    "\n",
    "# Verify state probabilities sum to 1\n",
    "print(f\"\\nState probabilities sum check: {np.allclose(test_data['state_probs'].sum(axis=1), 1.0)}\")\n",
    "\n",
    "# Convert amino acid indices to actual amino acid letters for readability\n",
    "amino_acid_letters = [AMINO_ACIDS[idx] for idx in test_data['amino_acids'][:10]]\n",
    "print(f\"First 10 amino acids (letters): {amino_acid_letters}\")\n",
    "print(\"\\nâœ“ HMM data generation working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "512826f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING BAYESFLOW SIMULATOR:\n",
      "\n",
      "âœ“ BayesFlow LambdaSimulator created successfully!\n",
      "\n",
      "TESTING BAYESFLOW SIMULATOR:\n",
      "Simulation data keys: ['amino_acids', 'true_states', 'state_probs']\n",
      "Amino acids batch shape: (3, 15)\n",
      "True states batch shape: (3, 15)\n",
      "State probabilities batch shape: (3, 15, 2)\n",
      "\n",
      "First 2 sequences:\n",
      "\n",
      "Sequence 0:\n",
      "Amino acids: [ 7 16 11 15 18 10 19 14  5 17 15 13 14 15  3]\n",
      "True states: [1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "State probabilities shape: (15, 2)\n",
      "State probabilities sum check: True\n",
      "Sequnce length: 15\n",
      "\n",
      "Sequence 1:\n",
      "Amino acids: [10 19 16  7 11  0  4  1 10 19 19  5 14  3  7]\n",
      "True states: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "State probabilities shape: (15, 2)\n",
      "State probabilities sum check: True\n",
      "Sequnce length: 15\n",
      "Amino acid letters: ['G', 'T', 'K', 'S', 'Y', 'L', 'V', 'P', 'E', 'W', 'S', 'F', 'P', 'S', 'D']\n",
      "\n",
      "âœ“ BayesFlow simulator working correctly!\n"
     ]
    }
   ],
   "source": [
    "# BAYESFLOW SIMULATOR IMPLEMENTATION\n",
    "\n",
    "def hmm_simulator_function(batch_shape, sequence_length=50, **kwargs):\n",
    "    \"\"\"\n",
    "    Simulator function for BayesFlow that generates HMM data.\n",
    "    \n",
    "    This function will be wrapped by BayesFlow's LambdaSimulator.\n",
    "    \n",
    "    Args:\n",
    "        batch_shape: Shape of the batch to generate (from BayesFlow)\n",
    "        sequence_length: Length of amino acid sequences to generate\n",
    "        **kwargs: Additional keyword arguments\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with simulation outputs for BayesFlow\n",
    "    \"\"\"\n",
    "    # Handle both int and tuple batch_shape\n",
    "    if isinstance(batch_shape, int):\n",
    "        batch_size = batch_shape\n",
    "    else:\n",
    "        batch_size = batch_shape[0] if len(batch_shape) > 0 else 1\n",
    "    \n",
    "    # Generate multiple sequences\n",
    "    amino_acids_batch = []\n",
    "    true_states_batch = []\n",
    "    state_probs_batch = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Generate one sequence with different random state for each\n",
    "        data = generate_amino_acid_sequence(\n",
    "            n_samples=sequence_length, \n",
    "            random_state=np.random.randint(0, 10000)\n",
    "        )\n",
    "        \n",
    "        amino_acids_batch.append(data['amino_acids'])\n",
    "        true_states_batch.append(data['true_states'])\n",
    "        state_probs_batch.append(data['state_probs'])\n",
    "    \n",
    "    # Stack into batch format\n",
    "    return {\n",
    "        'amino_acids': np.array(amino_acids_batch),      # Shape: (batch_size, sequence_length)\n",
    "        'true_states': np.array(true_states_batch),      # Shape: (batch_size, sequence_length)\n",
    "        'state_probs': np.array(state_probs_batch),      # Shape: (batch_size, sequence_length, 2)\n",
    "    }\n",
    "\n",
    "# Create BayesFlow simulator\n",
    "print(\"CREATING BAYESFLOW SIMULATOR:\\n\")\n",
    "hmm_simulator = bf.simulators.LambdaSimulator(\n",
    "    sample_fn=hmm_simulator_function,\n",
    "    is_batched=True  # Our function handles batching internally\n",
    ")\n",
    "\n",
    "print(\"âœ“ BayesFlow LambdaSimulator created successfully!\")\n",
    "\n",
    "# Test the BayesFlow simulator\n",
    "print(\"\\nTESTING BAYESFLOW SIMULATOR:\")\n",
    "batch_size = 3\n",
    "sequence_length = 15\n",
    "\n",
    "# Sample from the simulator\n",
    "simulation_data = hmm_simulator.sample(\n",
    "    batch_shape=(batch_size,), \n",
    "    sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "print(f\"Simulation data keys: {list(simulation_data.keys())}\")\n",
    "print(f\"Amino acids batch shape: {simulation_data['amino_acids'].shape}\")\n",
    "print(f\"True states batch shape: {simulation_data['true_states'].shape}\")\n",
    "print(f\"State probabilities batch shape: {simulation_data['state_probs'].shape}\")\n",
    "\n",
    "# Show multiple sequences\n",
    "num_seq = 2\n",
    "print(f\"\\nFirst {num_seq} sequences:\")\n",
    "for i in range(num_seq):\n",
    "    amino_acids = simulation_data['amino_acids'][i]\n",
    "    true_states = simulation_data['true_states'][i]\n",
    "    state_probs = simulation_data['state_probs'][i]\n",
    "    \n",
    "    print(f\"\\nSequence {i}:\")\n",
    "    print(f\"Amino acids: {amino_acids}\")\n",
    "    print(f\"True states: {true_states}\")\n",
    "    print(f\"State probabilities shape: {state_probs.shape}\")\n",
    "    print(f\"State probabilities sum check: {np.allclose(state_probs.sum(axis=1), 1.0)}\")\n",
    "    print(f\"Sequnce length: {len(amino_acids)}\")\n",
    "\n",
    "# Convert first sequence to amino acid letters\n",
    "example_letters = [AMINO_ACIDS[idx] for idx in simulation_data['amino_acids'][0]]\n",
    "print(f\"Amino acid letters: {example_letters}\")\n",
    "\n",
    "print(\"\\nâœ“ BayesFlow simulator working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27566099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Custom ProteinSummaryNetwork class defined\n"
     ]
    }
   ],
   "source": [
    "# CUSTOM PROTEIN SUMMARY NETWORK\n",
    "\n",
    "class ProteinSummaryNetwork(bf.networks.SummaryNetwork):\n",
    "    \"\"\"\n",
    "    Custom summary network for protein amino acid sequences.\n",
    "    \n",
    "    This network is specifically designed for the protein secondary structure task:\n",
    "    - Embeds amino acid indices into dense representations\n",
    "    - Uses bidirectional LSTM to capture sequential dependencies\n",
    "    - Applies attention mechanism to focus on important positions\n",
    "    - Outputs summary statistics for the entire sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size=20,              # Number of amino acids\n",
    "                 embedding_dim=32,           # Amino acid embedding dimension\n",
    "                 lstm_units=64,              # LSTM hidden units\n",
    "                 attention_dim=32,           # Attention mechanism dimension\n",
    "                 summary_dim=64,             # Output summary dimension\n",
    "                 dropout_rate=0.1,           # Dropout rate\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.attention_dim = attention_dim\n",
    "        self.summary_dim = summary_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Amino acid embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            mask_zero=False,  # Don't mask zero values as amino acid 'A' has index 0\n",
    "            name='amino_acid_embedding'\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM for sequence processing\n",
    "        self.lstm = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(\n",
    "                lstm_units,\n",
    "                return_sequences=True,  # Return full sequence for attention\n",
    "                dropout=dropout_rate,\n",
    "                recurrent_dropout=dropout_rate,\n",
    "                name='sequence_lstm'\n",
    "            ),\n",
    "            name='bidirectional_lstm'\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism layers\n",
    "        self.attention_dense = tf.keras.layers.Dense(\n",
    "            attention_dim, \n",
    "            activation='tanh',\n",
    "            name='attention_dense'\n",
    "        )\n",
    "        self.attention_weights = tf.keras.layers.Dense(\n",
    "            1, \n",
    "            activation=None,  # Don't use softmax here, apply it later\n",
    "            name='attention_weights'\n",
    "        )\n",
    "        \n",
    "        # Final summary layers\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.summary_dense1 = tf.keras.layers.Dense(\n",
    "            summary_dim * 2,\n",
    "            activation='silu',\n",
    "            name='summary_dense1'\n",
    "        )\n",
    "        self.summary_dense2 = tf.keras.layers.Dense(\n",
    "            summary_dim,\n",
    "            activation='silu', \n",
    "            name='summary_dense2'\n",
    "        )\n",
    "        \n",
    "    def call(self, x, training=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass of the protein summary network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, sequence_length, 1) containing amino acid indices\n",
    "            training: Whether in training mode\n",
    "            \n",
    "        Returns:\n",
    "            Summary tensor of shape (batch_size, summary_dim)\n",
    "        \"\"\"\n",
    "        # Remove the last dimension if present: (batch_size, seq_len, 1) -> (batch_size, seq_len)\n",
    "        if x.shape[-1] == 1:\n",
    "            x = tf.squeeze(x, axis=-1)\n",
    "            \n",
    "        # Convert to integer indices for embedding\n",
    "        x = tf.cast(x, tf.int32)\n",
    "        \n",
    "        # Embed amino acid indices: (batch_size, seq_len) -> (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Process with bidirectional LSTM: (batch_size, seq_len, embedding_dim) -> (batch_size, seq_len, 2*lstm_units)\n",
    "        lstm_output = self.lstm(embedded, training=training)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        # Compute attention scores: (batch_size, seq_len, 2*lstm_units) -> (batch_size, seq_len, attention_dim)\n",
    "        attention_scores = self.attention_dense(lstm_output)\n",
    "        \n",
    "        # Compute attention weights: (batch_size, seq_len, attention_dim) -> (batch_size, seq_len, 1)\n",
    "        attention_logits = self.attention_weights(attention_scores)\n",
    "        \n",
    "        # Apply softmax along the sequence dimension to get proper attention weights\n",
    "        attention_weights = tf.nn.softmax(attention_logits, axis=1)  # Softmax over sequence dimension\n",
    "        \n",
    "        # Apply attention: weighted sum of LSTM outputs\n",
    "        # (batch_size, seq_len, 2*lstm_units) * (batch_size, seq_len, 1) -> (batch_size, 2*lstm_units)\n",
    "        attended_output = tf.reduce_sum(lstm_output * attention_weights, axis=1)\n",
    "        \n",
    "        # Apply dropout\n",
    "        attended_output = self.dropout(attended_output, training=training)\n",
    "        \n",
    "        # Generate final summary through dense layers\n",
    "        summary = self.summary_dense1(attended_output)\n",
    "        summary = self.dropout(summary, training=training)\n",
    "        summary = self.summary_dense2(summary)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"Return the configuration of the layer.\"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'lstm_units': self.lstm_units,\n",
    "            'attention_dim': self.attention_dim,\n",
    "            'summary_dim': self.summary_dim,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"Create layer from configuration.\"\"\"\n",
    "        return cls(**config)\n",
    "\n",
    "print(\"âœ“ Custom ProteinSummaryNetwork class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f015cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE WORKFLOW FOR BAYESFLOW\n",
    "\n",
    "class FlattenTransform(bf.adapters.transforms.Transform):\n",
    "    \"\"\"Custom transform to flatten inference variables from (batch, seq_len, 2) to (batch, seq_len*2)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        # Flatten the last two dimensions: (batch, seq_len, 2) -> (batch, seq_len*2)\n",
    "        return x.reshape(x.shape[0], -1).astype(np.float32)\n",
    "    \n",
    "    def inverse(self, x, **kwargs):\n",
    "        # For inverse, we would need to know the original shape\n",
    "        # This is not needed for our use case but required by the interface\n",
    "        raise NotImplementedError(\"Inverse transform not implemented for FlattenTransform\")\n",
    "\n",
    "def create_workflow():\n",
    "    \"\"\"\n",
    "    Create BayesFlow workflow with custom protein summary network\n",
    "    and properly configured inference network.\n",
    "    \"\"\"\n",
    "    print(\"Creating BayesFlow workflow...\\n\")\n",
    "    \n",
    "    # 1. USE EXISTING SIMULATOR\n",
    "    simulator = hmm_simulator\n",
    "    print(\"âœ“ Using existing HMM simulator\")\n",
    "    \n",
    "    # 2. CUSTOM SUMMARY NETWORK\n",
    "    protein_summary_net = ProteinSummaryNetwork(\n",
    "        vocab_size=20,\n",
    "        embedding_dim=32,\n",
    "        lstm_units=64,\n",
    "        attention_dim=32,\n",
    "        summary_dim=64,\n",
    "        name='ProteinSummaryNetwork'\n",
    "    )\n",
    "    print(\"âœ“ Custom summary network created\")\n",
    "    \n",
    "    # 3. PROPERLY CONFIGURED INFERENCE NETWORK\n",
    "    inference_net = bf.networks.FlowMatching(\n",
    "        subnet=\"mlp\",\n",
    "        base_distribution=\"normal\",\n",
    "    )\n",
    "    print(\"âœ“ Properly configured FlowMatching created\")\n",
    "    print(f\"  - Subnet: MLP\")\n",
    "    print(f\"  - Base distribution: Normal\")\n",
    "    \n",
    "    # inference_net = bf.networks.CouplingFlow(\n",
    "    #     subnet='mlp',           # Use MLP subnets\n",
    "    #     depth=4,               # Number of coupling layers\n",
    "    #     transform='affine',    # Affine coupling transforms  \n",
    "    #     permutation='random',  # Random permutations between layers\n",
    "    #     use_actnorm=True,      # Use activation normalization\n",
    "    #     base_distribution='normal',  # Normal base distribution\n",
    "    #     name='ProteinInferenceNetwork'\n",
    "    # )\n",
    "    # print(\"âœ“ Properly configured CouplingFlow created\")\n",
    "    # print(f\"  - Depth: 8 coupling layers\")\n",
    "    # print(f\"  - Transform: affine\")\n",
    "    # print(f\"  - Base distribution: normal\")\n",
    "    \n",
    "    # 4. ADAPTER (same as before)\n",
    "    adapter_transforms = [\n",
    "        bf.adapters.transforms.Rename(from_key='amino_acids', to_key='summary_variables'),\n",
    "        bf.adapters.transforms.Rename(from_key='state_probs', to_key='inference_variables'),\n",
    "        bf.adapters.transforms.Drop(keys=['true_states']),\n",
    "        bf.adapters.transforms.MapTransform({\n",
    "            'summary_variables': bf.adapters.transforms.ConvertDType(\n",
    "                from_dtype='int64', to_dtype='float32'\n",
    "            ),\n",
    "            'inference_variables': bf.adapters.transforms.ConvertDType(\n",
    "                from_dtype='float64', to_dtype='float32'\n",
    "            ),\n",
    "        }),\n",
    "        bf.adapters.transforms.MapTransform({\n",
    "            'inference_variables': FlattenTransform(),\n",
    "        }),\n",
    "    ]\n",
    "    \n",
    "    adapter = bf.Adapter(transforms=adapter_transforms)\n",
    "    print(\"âœ“ Adapter with transforms created\")\n",
    "    \n",
    "    # 5. CREATE WORKFLOW WITH PROPER PARAMETERS\n",
    "    workflow = bf.BasicWorkflow(\n",
    "        simulator=simulator,\n",
    "        adapter=adapter,\n",
    "        inference_network=inference_net,\n",
    "        summary_network=protein_summary_net,\n",
    "        initial_learning_rate=0.001,  # Learning rate\n",
    "        inference_variables=['inference_variables'],  # Specify which variables to infer\n",
    "        summary_variables=['summary_variables']       # Specify summary variables\n",
    "    )\n",
    "    print(\"âœ“ BayesFlow workflow created with proper configuration\")\n",
    "    \n",
    "    return workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6d77d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Training function defined\n"
     ]
    }
   ],
   "source": [
    "# TRAINING FUNCTION FOR CUSTOM PROTEIN WORKFLOW\n",
    "\n",
    "def train_protein_workflow(\n",
    "    workflow,\n",
    "    batch_size=16,\n",
    "    epochs=50,\n",
    "    print_every=10,\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the protein BayesFlow workflow with our custom summary network.\n",
    "    \n",
    "    Args:\n",
    "        workflow: The BayesFlow workflow to train\n",
    "        batch_size: Batch size for training\n",
    "        epochs: Number of training epochs\n",
    "        print_every: Print progress every N epochs\n",
    "        save_path: Path to save the trained model (optional)\n",
    "    \n",
    "    Returns:\n",
    "        training_history: Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting training for {epochs} epochs with batch size {batch_size}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    training_history = {\n",
    "        'epoch': [],\n",
    "        'loss': [],\n",
    "        'validation_loss': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Configure the workflow for training\n",
    "        config = {\n",
    "            'epochs': epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'validation_sims': 1000,  # Generate validation data\n",
    "            'checkpoint_interval': max(1, epochs // 10),  # Save checkpoints\n",
    "        }\n",
    "        \n",
    "        print(\"Training configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print()\n",
    "        \n",
    "        # Start online training\n",
    "        print(\"ðŸš€ Starting online training...\")\n",
    "        training_info = workflow.fit_online(\n",
    "            num_batches_per_epoch=100,\n",
    "            validation_data=20,\n",
    "            epochs=config['epochs'],\n",
    "            batch_size=config['batch_size'],\n",
    "            print_every=print_every\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Training completed successfully!\")\n",
    "        \n",
    "        # Extract training history if available\n",
    "        if hasattr(training_info, 'history') and training_info.history:\n",
    "            history = training_info.history\n",
    "            training_history['loss'] = history.get('loss', [])\n",
    "            training_history['validation_loss'] = history.get('val_loss', [])\n",
    "            training_history['epoch'] = list(range(1, len(training_history['loss']) + 1))\n",
    "        \n",
    "        # Save the model if path provided\n",
    "        if save_path:\n",
    "            print(f\"ðŸ’¾ Saving model to {save_path}\")\n",
    "            workflow.save_model(save_path)\n",
    "            \n",
    "        return training_history\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Training failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return training_history\n",
    "\n",
    "print(\"âœ“ Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27248ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bayesflow:Fitting on dataset instance of OnlineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BayesFlow workflow...\n",
      "\n",
      "âœ“ Using existing HMM simulator\n",
      "âœ“ Custom summary network created\n",
      "âœ“ Properly configured FlowMatching created\n",
      "  - Subnet: MLP\n",
      "  - Base distribution: Normal\n",
      "âœ“ Adapter with transforms created\n",
      "âœ“ BayesFlow workflow created with proper configuration\n",
      "Starting training for 15 epochs with batch size 32\n",
      "============================================================\n",
      "Training configuration:\n",
      "  epochs: 15\n",
      "  batch_size: 32\n",
      "  validation_sims: 1000\n",
      "  checkpoint_interval: 1\n",
      "\n",
      "ðŸš€ Starting online training...\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:12:50.977027: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m981s\u001b[0m 10s/step - loss: 4.8482 - val_loss: 1.3974\n",
      "Epoch 2/15\n",
      "\u001b[1m 21/100\u001b[0m \u001b[32mâ”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m12:58\u001b[0m 10s/step - loss: 1.6816"
     ]
    }
   ],
   "source": [
    "configured_workflow = create_workflow()\n",
    "\n",
    "history = train_protein_workflow(\n",
    "    workflow=configured_workflow,\n",
    "    batch_size=32,\n",
    "    epochs=15,\n",
    "    print_every=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac244e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ec5884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30865142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112db5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b3e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8436c8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model analysis and optimization functions defined\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff65da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (UnicodeEncodeError('utf-8', '# SIMPLIFIED PARAMETER ANALYSIS\\n\\ndef analyze_model_parameters_simple(workflow):\\n    \"\"\"\\n    Simplified parameter analysis that doesn\\'t require building the full model.\\n    \"\"\"\\n    print(\"ðŸ” ANALYZING MODEL PARAMETERS\")\\n    print(\"=\" * 50)\\n    \\n    # Get the approximator components\\n    approximator = workflow.approximator\\n    summary_network = approximator.summary_network\\n    inference_network = approximator.inference_network\\n    \\n    # Build summary network only (it\\'s easier to analyze)\\n    print(\"Building summary network...\")\\n    dummy_summary_input = tf.zeros((1, 50, 1))  \\n    _ = summary_network(dummy_summary_input)\\n    summary_params = summary_network.count_params()\\n    \\n    # Estimate inference network parameters based on configuration\\n    print(\"Estimating inference network parameters...\")\\n    \\n    # CouplingFlow with 8 layers, each layer has MLP subnets [128, 128]\\n    # Input dimension: 100, Output: 100\\n    # Each coupling layer processes half the dimensions (50)\\n    # MLP: 50 -> 128 -> 128 -> 50 (for mean) + 50 -> 128 -> 128 -> 50 (for log_scale)\\n    \\n    mlp_params_per_layer = (50 * 128 + 128) + (128 * 128 + 128) + (128 * 50 + 50)  # One MLP\\n    mlp_params_per_layer *= 2  # Two MLPs per coupling layer (mean and log_scale)\\n    coupling_layers = 8\\n    inference_params = mlp_params_per_layer * coupling_layers\\n    \\n    # Add permutation and other parameters (relatively small)\\n    inference_params += 1000  # Rough estimate for other components\\n    \\n    total_params = summary_params + inference_params\\n    \\n    print(\"\\\\n\\udcca PARAMETER BREAKDOWN:\")\\n    print(\"=\" * 30)\\n    print(f\"ðŸ§¬ Summary Network: {summary_params:,} parameters\")\\n    print(f\"ðŸ”„ Inference Network: ~{inference_params:,} parameters (estimated)\")\\n    print(f\"ðŸŽ¯ TOTAL: ~{total_params:,} parameters\")\\n    \\n    # Memory and complexity analysis\\n    batch_size = 32\\n    memory_per_sample = (50 + 100 + 64) * 4  # bytes\\n    memory_per_batch = memory_per_sample * batch_size / (1024**2)  # MB\\n    \\n    print(f\"\\\\nâ±ï¸ TRAINING COMPLEXITY:\")\\n    print(\"=\" * 25)\\n    print(f\"Memory per batch: ~{memory_per_batch:.1f} MB\")\\n    print(f\"Model size: {\\'LARGE\\' if total_params > 200000 else \\'MEDIUM\\' if total_params > 50000 else \\'SMALL\\'}\")\\n    \\n    # Time estimates\\n    if total_params > 500000:\\n        time_per_epoch = \"5-10 minutes\"\\n        total_time = \"75-150 minutes\"\\n    elif total_params > 200000:\\n        time_per_epoch = \"2-5 minutes\" \\n        total_time = \"30-75 minutes\"\\n    elif total_params > 50000:\\n        time_per_epoch = \"30-120 seconds\"\\n        total_time = \"7.5-30 minutes\"\\n    else:\\n        time_per_epoch = \"10-30 seconds\"\\n        total_time = \"2.5-7.5 minutes\"\\n        \\n    print(f\"Estimated time per epoch: {time_per_epoch}\")\\n    print(f\"Total for 15 epochs: {total_time}\")\\n    \\n    return {\\n        \\'summary_params\\': summary_params,\\n        \\'inference_params\\': inference_params,\\n        \\'total_params\\': total_params,\\n        \\'memory_per_batch_mb\\': memory_per_batch\\n    }\\n\\n# Run the analysis\\nprint(\"Analyzing your current workflow parameters...\")\\nparam_analysis = analyze_model_parameters_simple(configured_workflow)\\n\\nprint(f\"\\\\n\\udca1 SOLUTIONS FOR FASTER TRAINING:\")\\nprint(\"=\" * 40)\\n\\nif param_analysis[\\'total_params\\'] > 200000:\\n    print(\"ðŸ”¥ YOUR MODEL IS VERY LARGE - This explains the slow training!\")\\n    print(\"\\\\nðŸš€ Quick fixes:\")\\n    print(\"1. Use smaller batch size: 8-16 instead of 32\")\\n    print(\"2. Reduce epochs: 5-10 instead of 15\") \\n    print(\"3. Use fewer batches per epoch: 25-50 instead of 100\")\\n    print(\"4. Create lightweight workflow (see next cell)\")\\n    \\nelif param_analysis[\\'total_params\\'] > 50000:\\n    print(\"âš¡ Your model is moderately large\")\\n    print(\"\\\\nðŸŽ¯ Optimizations:\")\\n    print(\"1. Reduce batch size to 16\")\\n    print(\"2. Use 50 batches per epoch\")\\n    print(\"3. Train for 10 epochs initially\")\\n    \\nelse:\\n    print(\"âœ… Your model size is reasonable\")\\n    print(\"Training should be relatively fast\")\\n\\nprint(f\"\\\\nðŸ“Š COMPARISON:\")\\nprint(\"Current model: ~{:,} parameters\".format(param_analysis[\\'total_params\\']))\\nprint(\"Lightweight model: ~50,000 parameters (10x faster)\")', 1547, 1548, 'surrogates not allowed')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (UnicodeEncodeError('utf-8', '# SIMPLIFIED PARAMETER ANALYSIS\\n\\ndef analyze_model_parameters_simple(workflow):\\n    \"\"\"\\n    Simplified parameter analysis that doesn\\'t require building the full model.\\n    \"\"\"\\n    print(\"ðŸ” ANALYZING MODEL PARAMETERS\")\\n    print(\"=\" * 50)\\n    \\n    # Get the approximator components\\n    approximator = workflow.approximator\\n    summary_network = approximator.summary_network\\n    inference_network = approximator.inference_network\\n    \\n    # Build summary network only (it\\'s easier to analyze)\\n    print(\"Building summary network...\")\\n    dummy_summary_input = tf.zeros((1, 50, 1))  \\n    _ = summary_network(dummy_summary_input)\\n    summary_params = summary_network.count_params()\\n    \\n    # Estimate inference network parameters based on configuration\\n    print(\"Estimating inference network parameters...\")\\n    \\n    # CouplingFlow with 8 layers, each layer has MLP subnets [128, 128]\\n    # Input dimension: 100, Output: 100\\n    # Each coupling layer processes half the dimensions (50)\\n    # MLP: 50 -> 128 -> 128 -> 50 (for mean) + 50 -> 128 -> 128 -> 50 (for log_scale)\\n    \\n    mlp_params_per_layer = (50 * 128 + 128) + (128 * 128 + 128) + (128 * 50 + 50)  # One MLP\\n    mlp_params_per_layer *= 2  # Two MLPs per coupling layer (mean and log_scale)\\n    coupling_layers = 8\\n    inference_params = mlp_params_per_layer * coupling_layers\\n    \\n    # Add permutation and other parameters (relatively small)\\n    inference_params += 1000  # Rough estimate for other components\\n    \\n    total_params = summary_params + inference_params\\n    \\n    print(\"\\\\n\\udcca PARAMETER BREAKDOWN:\")\\n    print(\"=\" * 30)\\n    print(f\"ðŸ§¬ Summary Network: {summary_params:,} parameters\")\\n    print(f\"ðŸ”„ Inference Network: ~{inference_params:,} parameters (estimated)\")\\n    print(f\"ðŸŽ¯ TOTAL: ~{total_params:,} parameters\")\\n    \\n    # Memory and complexity analysis\\n    batch_size = 32\\n    memory_per_sample = (50 + 100 + 64) * 4  # bytes\\n    memory_per_batch = memory_per_sample * batch_size / (1024**2)  # MB\\n    \\n    print(f\"\\\\nâ±ï¸ TRAINING COMPLEXITY:\")\\n    print(\"=\" * 25)\\n    print(f\"Memory per batch: ~{memory_per_batch:.1f} MB\")\\n    print(f\"Model size: {\\'LARGE\\' if total_params > 200000 else \\'MEDIUM\\' if total_params > 50000 else \\'SMALL\\'}\")\\n    \\n    # Time estimates\\n    if total_params > 500000:\\n        time_per_epoch = \"5-10 minutes\"\\n        total_time = \"75-150 minutes\"\\n    elif total_params > 200000:\\n        time_per_epoch = \"2-5 minutes\" \\n        total_time = \"30-75 minutes\"\\n    elif total_params > 50000:\\n        time_per_epoch = \"30-120 seconds\"\\n        total_time = \"7.5-30 minutes\"\\n    else:\\n        time_per_epoch = \"10-30 seconds\"\\n        total_time = \"2.5-7.5 minutes\"\\n        \\n    print(f\"Estimated time per epoch: {time_per_epoch}\")\\n    print(f\"Total for 15 epochs: {total_time}\")\\n    \\n    return {\\n        \\'summary_params\\': summary_params,\\n        \\'inference_params\\': inference_params,\\n        \\'total_params\\': total_params,\\n        \\'memory_per_batch_mb\\': memory_per_batch\\n    }\\n\\n# Run the analysis\\nprint(\"Analyzing your current workflow parameters...\")\\nparam_analysis = analyze_model_parameters_simple(configured_workflow)\\n\\nprint(f\"\\\\n\\udca1 SOLUTIONS FOR FASTER TRAINING:\")\\nprint(\"=\" * 40)\\n\\nif param_analysis[\\'total_params\\'] > 200000:\\n    print(\"ðŸ”¥ YOUR MODEL IS VERY LARGE - This explains the slow training!\")\\n    print(\"\\\\nðŸš€ Quick fixes:\")\\n    print(\"1. Use smaller batch size: 8-16 instead of 32\")\\n    print(\"2. Reduce epochs: 5-10 instead of 15\") \\n    print(\"3. Use fewer batches per epoch: 25-50 instead of 100\")\\n    print(\"4. Create lightweight workflow (see next cell)\")\\n    \\nelif param_analysis[\\'total_params\\'] > 50000:\\n    print(\"âš¡ Your model is moderately large\")\\n    print(\"\\\\nðŸŽ¯ Optimizations:\")\\n    print(\"1. Reduce batch size to 16\")\\n    print(\"2. Use 50 batches per epoch\")\\n    print(\"3. Train for 10 epochs initially\")\\n    \\nelse:\\n    print(\"âœ… Your model size is reasonable\")\\n    print(\"Training should be relatively fast\")\\n\\nprint(f\"\\\\nðŸ“Š COMPARISON:\")\\nprint(\"Current model: ~{:,} parameters\".format(param_analysis[\\'total_params\\']))\\nprint(\"Lightweight model: ~50,000 parameters (10x faster)\")', 1547, 1548, 'surrogates not allowed')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'utf-8' codec can't encode character '\\udcca' in position 13: surrogates not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeEncodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3490\u001b[39m, in \u001b[36mInteractiveShell.transform_cell\u001b[39m\u001b[34m(self, raw_cell)\u001b[39m\n\u001b[32m   3477\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transform an input cell before parsing it.\u001b[39;00m\n\u001b[32m   3478\u001b[39m \n\u001b[32m   3479\u001b[39m \u001b[33;03mStatic transformations, implemented in IPython.core.inputtransformer2,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3487\u001b[39m \u001b[33;03msee :meth:`transform_ast`.\u001b[39;00m\n\u001b[32m   3488\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3489\u001b[39m \u001b[38;5;66;03m# Static input transformations\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3490\u001b[39m cell = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_transformer_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cell.splitlines()) == \u001b[32m1\u001b[39m:\n\u001b[32m   3493\u001b[39m     \u001b[38;5;66;03m# Dynamic transformations - only applied for single line commands\u001b[39;00m\n\u001b[32m   3494\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   3495\u001b[39m         \u001b[38;5;66;03m# use prefilter_lines to handle trailing newlines\u001b[39;00m\n\u001b[32m   3496\u001b[39m         \u001b[38;5;66;03m# restore trailing newline for ast.parse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/IPython/core/inputtransformer2.py:643\u001b[39m, in \u001b[36mTransformerManager.transform_cell\u001b[39m\u001b[34m(self, cell)\u001b[39m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cleanup_transforms + \u001b[38;5;28mself\u001b[39m.line_transforms:\n\u001b[32m    641\u001b[39m     lines = transform(lines)\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m lines = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_token_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(lines)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/IPython/core/inputtransformer2.py:628\u001b[39m, in \u001b[36mTransformerManager.do_token_transforms\u001b[39m\u001b[34m(self, lines)\u001b[39m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_token_transforms\u001b[39m(\u001b[38;5;28mself\u001b[39m, lines):\n\u001b[32m    627\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(TRANSFORM_LOOP_LIMIT):\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m         changed, lines = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_one_token_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    629\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m changed:\n\u001b[32m    630\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m lines\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/IPython/core/inputtransformer2.py:608\u001b[39m, in \u001b[36mTransformerManager.do_one_token_transform\u001b[39m\u001b[34m(self, lines)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_one_token_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, lines):\n\u001b[32m    595\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Find and run the transform earliest in the code.\u001b[39;00m\n\u001b[32m    596\u001b[39m \n\u001b[32m    597\u001b[39m \u001b[33;03m    Returns (changed, lines).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    606\u001b[39m \u001b[33;03m    a performance issue.\u001b[39;00m\n\u001b[32m    607\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m     tokens_by_line = \u001b[43mmake_tokens_by_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    609\u001b[39m     candidates = []\n\u001b[32m    610\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m transformer_cls \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.token_transformers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/IPython/core/inputtransformer2.py:532\u001b[39m, in \u001b[36mmake_tokens_by_line\u001b[39m\u001b[34m(lines)\u001b[39m\n\u001b[32m    530\u001b[39m parenlev = \u001b[32m0\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_tokens_catch_errors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__next__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_errors_to_catch\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexpected EOF\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokens_by_line\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mNEWLINE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mNL\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mparenlev\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/IPython/utils/tokenutil.py:45\u001b[39m, in \u001b[36mgenerate_tokens_catch_errors\u001b[39m\u001b[34m(readline, extra_errors_to_catch)\u001b[39m\n\u001b[32m     43\u001b[39m tokens: \u001b[38;5;28mlist\u001b[39m[TokenInfo] = []\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/tokenize.py:584\u001b[39m, in \u001b[36m_generate_tokens_from_c_tokenizer\u001b[39m\u001b[34m(source, encoding, extra_tokens)\u001b[39m\n\u001b[32m    582\u001b[39m     it = _tokenize.TokenizerIter(source, encoding=encoding, extra_tokens=extra_tokens)\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mTokenInfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_make\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mUnicodeEncodeError\u001b[39m: 'utf-8' codec can't encode character '\\udcca' in position 13: surrogates not allowed"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f5c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETER ANALYSIS\n",
      "==============================\n",
      "Summary Network: 79,233 parameters\n",
      "Inference Network: ~471,840 parameters (estimated)\n",
      "TOTAL: ~551,073 parameters\n",
      "\n",
      "WHY TRAINING IS SLOW:\n",
      "- Large model: ~600K+ parameters\n",
      "- Complex coupling flows: 8 deep layers\n",
      "- High dimensional output: 100 variables\n",
      "- Online data generation: New data each batch\n",
      "\n",
      "SOLUTIONS:\n",
      "1. Reduce batch size: 32 -> 8-16\n",
      "2. Fewer epochs: 15 -> 5-10\n",
      "3. Fewer batches per epoch: 100 -> 25-50\n",
      "4. Use lightweight model (next cells)\n",
      "\n",
      "TIME ESTIMATES:\n",
      "Current model: 3-7 minutes per epoch\n",
      "15 epochs: 45-105 minutes total\n",
      "Lightweight model: 30-60 seconds per epoch\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
