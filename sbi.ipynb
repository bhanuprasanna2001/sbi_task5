{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a836c975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 01:47:29.957019: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-07-09 01:47:29.957055: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-07-09 01:47:29.957059: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752018449.957075 5700552 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1752018449.957096 5700552 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "INFO:bayesflow:Using backend 'tensorflow'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.keras is using the 'tensorflow' backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import bayesflow as bf\n",
    "\n",
    "current_backend = tf.keras.backend.backend()\n",
    "print(f\"tf.keras is using the '{current_backend}' backend.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6eab7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (200, 1) Z shape: (200,)\n",
      "Post shape: (200, 2)\n",
      "Generated sequence (X): [[10]\n",
      " [ 1]\n",
      " [13]\n",
      " [14]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 0]\n",
      " [11]]\n",
      "State probabilities (post): [[1.         0.        ]\n",
      " [0.93485169 0.06514831]\n",
      " [0.8613024  0.1386976 ]\n",
      " [0.79142805 0.20857195]\n",
      " [0.80233001 0.19766999]\n",
      " [0.79194286 0.20805714]\n",
      " [0.77323791 0.22676209]\n",
      " [0.80297444 0.19702556]\n",
      " [0.84907351 0.15092649]\n",
      " [0.85705359 0.14294641]]\n"
     ]
    }
   ],
   "source": [
    "# Trial code to generate a sequence using a Hidden Markov Model (HMM)\n",
    "# This code uses the hmmlearn library to create a categorical HMM with fixed parameters.\n",
    "# It generates a sequence of observations and computes the posterior probabilities of the states.\n",
    "\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# 1) Initialize model\n",
    "model = hmm.CategoricalHMM(n_components=2, algorithm='viterbi',\n",
    "                           n_iter=0, init_params='')  # No EM\n",
    "\n",
    "# 2) Fix parameters:\n",
    "model.startprob_ = np.array([1.0, 0.0])        # always begin in “other”\n",
    "model.transmat_  = np.array([[0.95, 0.05],     # other → other/helix\n",
    "                             [0.10, 0.90]])    # helix → other/helix\n",
    "model.emissionprob_ = np.array([\n",
    "        [0.12, 0.06, 0.03, 0.05, 0.01, 0.09, 0.05, 0.04, 0.02, 0.07, 0.12, 0.06, 0.03, 0.04, 0.02, 0.05, 0.04, 0.01, 0.03, 0.06],\n",
    "        [0.06, 0.05, 0.05, 0.06, 0.02, 0.05, 0.03, 0.09, 0.03, 0.05, 0.08, 0.06, 0.02, 0.04, 0.06, 0.07, 0.06, 0.01, 0.04, 0.07]\n",
    "    ])          # 2×20 table from Task 1\n",
    "\n",
    "# 3) Generate a sequence + state probs:\n",
    "X, Z = model.sample(n_samples=200)\n",
    "post = model.predict_proba(X)\n",
    "\n",
    "print(\"X shape:\", X.shape, \"Z shape:\", Z.shape)\n",
    "print(\"Post shape:\", post.shape)\n",
    "print(\"Generated sequence (X):\", X[:10])\n",
    "print(\"State probabilities (post):\", post[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "705b0f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (4, 200)\n",
      "P.shape: (4, 200, 2)\n",
      "First sequence obs: [19 11  2 16 14 19  3  3 10  6]\n",
      "First sequence posteriors: [[1.         0.        ]\n",
      " [0.98569035 0.01430965]\n",
      " [0.98170387 0.01829613]\n",
      " [0.97700983 0.02299017]\n",
      " [0.97265727 0.02734273]\n",
      " [0.94451754 0.05548246]\n",
      " [0.91752723 0.08247277]\n",
      " [0.88835249 0.11164751]\n",
      " [0.85465933 0.14534067]\n",
      " [0.849613   0.150387  ]]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Implementing the HMM Simulator with hmmlearn.CategoricalHMM\n",
    "\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# 4.1 Define the fixed emission & transition matrices from Task 1\n",
    "n_states = 2           # 0 = \"other\", 1 = \"alpha‐helix\"\n",
    "n_amino_acids = 20     # 20 standard residues\n",
    "\n",
    "# Transition: rows sum to 1; row[0] = other→(other, helix), row[1] = helix→(other, helix)\n",
    "transmat = np.array([\n",
    "    [0.95, 0.05],   # other → other / helix\n",
    "    [0.10, 0.90],   # helix → other / helix\n",
    "])\n",
    "\n",
    "# Start in \"other\" with probability 1.0\n",
    "startprob = np.array([1.0, 0.0])\n",
    "\n",
    "# Emission probabilities: shape (n_states, n_amino_acids)\n",
    "# Row 0: \"other\"; Row 1: \"alpha‐helix\"\n",
    "emissionprob = np.array([\n",
    "    [0.06, 0.05, 0.05, 0.06, 0.02, 0.05, 0.03, 0.09, 0.03, 0.05,\n",
    "     0.08, 0.06, 0.02, 0.04, 0.06, 0.07, 0.06, 0.01, 0.04, 0.07],\n",
    "    [0.12, 0.06, 0.03, 0.05, 0.01, 0.09, 0.05, 0.04, 0.02, 0.07,\n",
    "     0.12, 0.06, 0.03, 0.04, 0.02, 0.05, 0.04, 0.01, 0.03, 0.06],\n",
    "])\n",
    "\n",
    "# 4.2 Build the CategoricalHMM with parameters frozen (no EM training)\n",
    "model = hmm.CategoricalHMM(\n",
    "    n_components=n_states,\n",
    "    algorithm='viterbi', # use forward–backward (\"viterbi\") for predict_proba\n",
    "    n_iter=0,            # skip EM\n",
    "    init_params=''       # do not re-initialize any parameters\n",
    ")\n",
    "\n",
    "model.startprob_    = startprob\n",
    "model.transmat_     = transmat\n",
    "model.emissionprob_ = emissionprob\n",
    "\n",
    "# 4.3 Simulator function\n",
    "def simulate_batch(sequence_length, batch_size, random_state=None):\n",
    "    \"\"\"\n",
    "    Simulate `batch_size` independent HMM sequences of length `sequence_length`.\n",
    "    Returns:\n",
    "      X_batch: np.ndarray, shape (batch_size, sequence_length), dtype=int\n",
    "               (integer‐encoded amino‐acid observations 0..19)\n",
    "      P_batch: np.ndarray, shape (batch_size, sequence_length, n_states),\n",
    "               posterior state-membership probabilities\n",
    "    \"\"\"\n",
    "    X_batch = np.zeros((batch_size, sequence_length), dtype=int)\n",
    "    P_batch = np.zeros((batch_size, sequence_length, n_states))\n",
    "\n",
    "    # Option A: call .sample for each sequence independently\n",
    "    for i in range(batch_size):\n",
    "        # sample returns (observations, latent_states)\n",
    "        X_i, Z_i = model.sample(n_samples=sequence_length, random_state=random_state)\n",
    "        # X_i is (sequence_length, 1) of ints in [0,19]; squeeze to 1D\n",
    "        X_i = X_i.squeeze().astype(int)\n",
    "\n",
    "        # compute posterior probabilities via forward–backward\n",
    "        post_i = model.predict_proba(X_i.reshape(-1, 1))\n",
    "\n",
    "        X_batch[i] = X_i\n",
    "        P_batch[i] = post_i\n",
    "\n",
    "    return X_batch, P_batch\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    seq_len   = 200\n",
    "    batch_sz  = 4\n",
    "    X, P = simulate_batch(seq_len, batch_sz, random_state=42)\n",
    "\n",
    "    print(\"X.shape:\", X.shape)       # (4, 200)\n",
    "    print(\"P.shape:\", P.shape)       # (4, 200, 2)\n",
    "    print(\"First sequence obs:\", X[0, :10])\n",
    "    print(\"First sequence posteriors:\", P[0, :10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90db9712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bayesflow.simulators import LambdaSimulator\n",
    "from bayesflow.approximators import ContinuousApproximator\n",
    "from bayesflow.adapters import Adapter\n",
    "from bayesflow.datasets import OnlineDataset\n",
    "\n",
    "# 4) Define a sample function for BayesFlow with fixed sequence length\n",
    "SEQUENCE_LENGTH = 200  # Define this as a constant\n",
    "\n",
    "def bayesflow_sample_fn(batch_shape, random_state=None):\n",
    "    \"\"\"\n",
    "    batch_shape : tuple[int,...], e.g. (batch_size,)\n",
    "    random_state : optional RNG seed\n",
    "    \n",
    "    Returns a dict mapping variable names → numpy arrays:\n",
    "      - \"observations\": int arrays of shape (batch_size, sequence_length)\n",
    "      - \"states\":       int arrays of shape (batch_size, sequence_length)\n",
    "    \"\"\"\n",
    "    # Determine batch size\n",
    "    batch_size = int(np.prod(batch_shape))\n",
    "    \n",
    "    # Simulate with fixed sequence length\n",
    "    X_batch, P_batch = simulate_batch(\n",
    "        sequence_length=SEQUENCE_LENGTH,\n",
    "        batch_size=batch_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # CRITICAL FIX: Ensure observations are integers in the correct range\n",
    "    # Convert to int32 and ensure values are in [0, 19] range\n",
    "    X_batch = X_batch.astype(np.int32)\n",
    "    P_batch = P_batch.astype(np.float32)\n",
    "    \n",
    "    # Debug: Check the data types and ranges\n",
    "    # print(f\"X_batch dtype: {X_batch.dtype}\")\n",
    "    # print(f\"X_batch shape: {X_batch.shape}\")\n",
    "    # print(f\"X_batch min: {X_batch.min()}, max: {X_batch.max()}\")\n",
    "    # print(f\"P_batch dtype: {P_batch.dtype}\")\n",
    "    # print(f\"P_batch shape: {P_batch.shape}\")\n",
    "    \n",
    "    # Ensure observations are in valid range [0, 19]\n",
    "    if X_batch.min() < 0 or X_batch.max() >= 20:\n",
    "        raise ValueError(f\"Observations must be in range [0, 19], got [{X_batch.min()}, {X_batch.max()}]\")\n",
    "    \n",
    "    return {\n",
    "        \"observations\": X_batch,   # integer codes\n",
    "        \"posteriors\":   P_batch    # continuous [0,1] probabilities\n",
    "    }\n",
    "\n",
    "# 5) Wrap in LambdaSimulator\n",
    "simulator = LambdaSimulator(\n",
    "    sample_fn=bayesflow_sample_fn,\n",
    "    is_batched=True        # our sample_fn handles the full batch in one call\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf93d34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final working version ===\n",
      "Keys in batch: dict_keys(['inference_variables', 'summary_variables'])\n",
      "summary_variables shape:    (64, 200, 20)\n",
      "inference_variables shape: (64, 400)\n",
      "summary_variables dtype:    float32\n",
      "inference_variables dtype:  float32\n",
      "\n",
      "Verifying one-hot encoding:\n",
      "summary_variables sum along last axis (should be all 1s): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "inference_variables range: 0.0 to 1.0\n",
      "\n",
      "✅ Success! Your BayesFlow adapter is working correctly.\n"
     ]
    }
   ],
   "source": [
    "# 1) Build the base adapter: map your raw simulator output\n",
    "#    - \"posteriors\" → \"inference_variables\" \n",
    "#    - \"observations\" → \"summary_variables\"\n",
    "adapter = ContinuousApproximator.build_adapter(\n",
    "    inference_variables=[\"posteriors\"],\n",
    "    summary_variables=[\"observations\"]\n",
    ")\n",
    "\n",
    "# 2) CRITICAL FIX: Convert summary_variables back to integers and squeeze extra dimension\n",
    "# The base adapter converts int32 to float32 and adds dimension, we need to reverse this\n",
    "from bayesflow.adapters.transforms import Transform\n",
    "from bayesflow.adapters.transforms.map_transform import MapTransform\n",
    "import numpy as np\n",
    "\n",
    "class IntegerSqueezeTransform(Transform):\n",
    "    \"\"\"Custom transform to convert float32 back to int32 and squeeze extra dimension\"\"\"\n",
    "    \n",
    "    def forward(self, data, **kwargs):\n",
    "        # Convert to int32 and squeeze the last dimension if it's size 1\n",
    "        if data.ndim == 3 and data.shape[-1] == 1:\n",
    "            data = data.squeeze(-1)  # Remove last dimension\n",
    "        return data.astype(np.int32)\n",
    "    \n",
    "    def inverse(self, data, **kwargs):\n",
    "        # Add back the dimension and convert to float32\n",
    "        return data.astype(np.float32)[..., np.newaxis]\n",
    "    \n",
    "class ReshapeTransform(Transform):\n",
    "    \"\"\"Custom transform to reshape posteriors from (batch, seq, states) to (batch, seq*states)\"\"\"\n",
    "    \n",
    "    def forward(self, data, **kwargs):\n",
    "        batch_size = data.shape[0]\n",
    "        return data.reshape(batch_size, -1)  # Flatten last two dimensions\n",
    "    \n",
    "    def inverse(self, data, **kwargs):\n",
    "        batch_size = data.shape[0]\n",
    "        return data.reshape(batch_size, SEQUENCE_LENGTH, n_states)\n",
    "\n",
    "# Create a MapTransform to apply only to summary_variables\n",
    "integer_squeeze_map = MapTransform(\n",
    "    transform_map={\"summary_variables\": IntegerSqueezeTransform()}\n",
    ")\n",
    "\n",
    "reshape_map = MapTransform(\n",
    "    transform_map={\"inference_variables\": ReshapeTransform()}\n",
    ")\n",
    "\n",
    "# Add the transform to the adapter\n",
    "adapter = adapter.append(integer_squeeze_map)\n",
    "adapter = adapter.append(reshape_map)\n",
    "\n",
    "# 3) One-hot encode the summary inputs (now named \"summary_variables\")\n",
    "adapter = adapter.one_hot(\n",
    "    keys=\"summary_variables\",\n",
    "    num_classes=20\n",
    ")\n",
    "\n",
    "# 4) Convert all floats to float32 for TensorFlow\n",
    "adapter = adapter.convert_dtype(\n",
    "    from_dtype=\"float64\",\n",
    "    to_dtype=\"float32\"\n",
    ")\n",
    "\n",
    "# 1) Re-initialize the dataset\n",
    "dataset = OnlineDataset(\n",
    "    simulator=simulator,          # your LambdaSimulator\n",
    "    batch_size=64,                # sequences per batch\n",
    "    num_batches=1000,             # batches per epoch\n",
    "    adapter=adapter,              # adapter with integer squeeze fix\n",
    "    stage=\"training\",             # adapter stage\n",
    ")\n",
    "\n",
    "# 2) Test the final working version\n",
    "print(\"=== Final working version ===\")\n",
    "batch = dataset[0]\n",
    "\n",
    "# 3) Inspect the final batch\n",
    "print(\"Keys in batch:\", batch.keys())  \n",
    "print(\"summary_variables shape:   \", batch[\"summary_variables\"].shape)    # → (64, 200, 20)\n",
    "print(\"inference_variables shape:\", batch[\"inference_variables\"].shape)  # → (64, 200, 2)\n",
    "print(\"summary_variables dtype:   \", batch[\"summary_variables\"].dtype)    # → float32\n",
    "print(\"inference_variables dtype: \", batch[\"inference_variables\"].dtype)  # → float32\n",
    "\n",
    "# 4) Verify the one-hot encoding worked correctly\n",
    "print(\"\\nVerifying one-hot encoding:\")\n",
    "print(\"summary_variables sum along last axis (should be all 1s):\", \n",
    "      batch[\"summary_variables\"].sum(axis=-1)[0, :10])  # First 10 timesteps of first sequence\n",
    "print(\"inference_variables range:\", \n",
    "      batch[\"inference_variables\"].min(), \"to\", batch[\"inference_variables\"].max())\n",
    "\n",
    "print(\"\\n✅ Success! Your BayesFlow adapter is working correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12120f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy_summary.shape = (8, 64)\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Define the Summary Network (TimeSeriesNetwork)\n",
    "\n",
    "import tensorflow as tf\n",
    "from bayesflow.networks import TimeSeriesNetwork\n",
    "\n",
    "# 7.1 Instantiate a TimeSeriesNetwork to embed (batch, 200, 20) → (batch, 64)\n",
    "summary_network = TimeSeriesNetwork(\n",
    "    summary_dim=64,               # final embedding size per sequence :contentReference[oaicite:0]{index=0}\n",
    "    filters=[32, 64],             # two Conv1D layers with 32 and 64 filters :contentReference[oaicite:1]{index=1}\n",
    "    kernel_sizes=[3, 3],          # 3-length kernels for each conv layer :contentReference[oaicite:2]{index=2}\n",
    "    strides=[1, 1],               # unit strides for convolution :contentReference[oaicite:3]{index=3}\n",
    "    activation=\"mish\",            # Mish activation in conv layers :contentReference[oaicite:4]{index=4}\n",
    "    kernel_initializer=\"glorot_uniform\",  # Xavier uniform initialization :contentReference[oaicite:5]{index=5}\n",
    "    groups=None,                  # no group normalization :contentReference[oaicite:6]{index=6}\n",
    "    recurrent_type=\"gru\",         # use GRU in the recurrent module :contentReference[oaicite:7]{index=7}\n",
    "    recurrent_dim=128,            # 128 hidden units in GRU :contentReference[oaicite:8]{index=8}\n",
    "    bidirectional=True,           # bidirectional recurrence :contentReference[oaicite:9]{index=9}\n",
    "    dropout=0.05,                 # 5% dropout in the recurrent module :contentReference[oaicite:10]{index=10}\n",
    "    skip_steps=4                  # skip-connections every 4 timesteps :contentReference[oaicite:11]{index=11}\n",
    ")\n",
    "\n",
    "# 7.2 Sanity-check: pass a dummy batch through the summary network\n",
    "dummy_input = tf.random.uniform((8, 200, 20), dtype=tf.float32)\n",
    "dummy_summary = summary_network(dummy_input)\n",
    "print(\"dummy_summary.shape =\", dummy_summary.shape)  # expects (8, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae2f5cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Velocity shape: (8, 400)\n",
      "Available metrics: ['loss']\n",
      "Loss value: tf.Tensor(4.453058, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Step 8.1: Define a FlowMatching-based inference network\n",
    "\n",
    "import tensorflow as tf                                        # standard TensorFlow import :contentReference[oaicite:3]{index=3}\n",
    "from bayesflow.networks import FlowMatching, MLP               # import FlowMatching :contentReference[oaicite:4]{index=4}\n",
    "\n",
    "# 1. Instantiate the FlowMatching inference network\n",
    "inference_network = FlowMatching(\n",
    "    subnet=MLP(                                                # use a custom MLP subnet :contentReference[oaicite:5]{index=5}\n",
    "        widths=[128, 128, SEQUENCE_LENGTH * n_states],         # two hidden layers + output layer (200*2=400 dims) :contentReference[oaicite:6]{index=6}\n",
    "        activation=\"relu\",                                     # ReLU activation instead of default 'mish' :contentReference[oaicite:7]{index=7}\n",
    "        dropout=0.1                                            # 10% dropout for regularization :contentReference[oaicite:8]{index=8}\n",
    "    ),\n",
    "    base_distribution=\"normal\",                                # draw samples from a standard normal :contentReference[oaicite:9]{index=9}\n",
    "    use_optimal_transport=False,                               # disable optimal transport for speed :contentReference[oaicite:10]{index=10}\n",
    "    loss_fn=\"mse\",                                             # mean squared error loss :contentReference[oaicite:11]{index=11}\n",
    "    integrate_kwargs={\"method\": \"euler\", \"steps\": 100},        # override integration settings (default: 100 steps, Euler) :contentReference[oaicite:12]{index=12}\n",
    "    optimal_transport_kwargs=None                              # leave OT settings as None :contentReference[oaicite:13]{index=13}\n",
    ")\n",
    "\n",
    "# 2. Create dummy data for testing\n",
    "batch_size, summary_dim = 8, 64\n",
    "positions, state_dim   = 200, 2\n",
    "\n",
    "dummy_summary = tf.random.uniform(\n",
    "    (batch_size, summary_dim), dtype=tf.float32\n",
    ")\n",
    "dummy_target = tf.random.uniform(\n",
    "    (batch_size, positions, state_dim), dtype=tf.float32\n",
    ")\n",
    "\n",
    "# Flatten the target to shape (batch_size, positions*state_dim)\n",
    "dummy_xz = tf.reshape(dummy_target, (batch_size, -1))         # → (8, 400)\n",
    "\n",
    "# 3. Build the network with correct input shapes\n",
    "inference_network.build(\n",
    "    xz_shape=dummy_xz.shape,                                  # specify flattened target shape :contentReference[oaicite:14]{index=14}\n",
    "    conditions_shape=dummy_summary.shape                      # specify summary shape :contentReference[oaicite:15]{index=15}\n",
    ")\n",
    "\n",
    "# 4. Compute the velocity field at random times\n",
    "t = tf.random.uniform((batch_size,), dtype=tf.float32)       # sample t ∼ Uniform(0,1) :contentReference[oaicite:16]{index=16}\n",
    "velocity = inference_network.velocity(\n",
    "    xz=dummy_xz,\n",
    "    time=t,\n",
    "    conditions=dummy_summary,\n",
    "    training=False\n",
    ")\n",
    "print(\"Velocity shape:\", velocity.shape)                     # Expected: (8, 400)\n",
    "\n",
    "# 5. Compute training metrics (e.g., loss)\n",
    "metrics = inference_network.compute_metrics(\n",
    "    x=dummy_xz,\n",
    "    conditions=dummy_summary\n",
    ")\n",
    "print(\"Available metrics:\", list(metrics.keys()))\n",
    "print(\"Loss value:\", metrics[\"loss\"])                        # MSE between predicted and actual velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08df202e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bayesflow:Fitting on dataset instance of OnlineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting training ===\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 01:47:32.985580: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 195ms/step - loss: 1.4114\n",
      "Epoch 2/2\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 206ms/step - loss: 1.1010\n",
      "✅ Training completed successfully!\n",
      "\n",
      "=== Training metrics ===\n",
      "loss: 1.1010\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Assemble and compile the Amortized Posterior Estimator\n",
    "\n",
    "import tensorflow as tf\n",
    "from bayesflow.approximators import ContinuousApproximator\n",
    "\n",
    "# 9.1 Instantiate the approximator\n",
    "approximator = ContinuousApproximator(\n",
    "    adapter=adapter,                                     # data preprocessing pipeline :contentReference[oaicite:5]{index=5}\n",
    "    inference_network=inference_network,                 # FlowMatching or MLP from Step 8 :contentReference[oaicite:6]{index=6}\n",
    "    summary_network=summary_network,                     # TimeSeriesNetwork from Step 7 :contentReference[oaicite:7]{index=7}\n",
    "    standardize=\"all\"                                    # auto‐standardize all inputs :contentReference[oaicite:8]{index=8}\n",
    ")\n",
    "\n",
    "# 9.2 Configure optimizer, loss, and metrics\n",
    "approximator.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    ")\n",
    "\n",
    "# 9.3 Train on the simulated dataset from Step 6\n",
    "print(\"\\n=== Starting training ===\")\n",
    "history = approximator.fit(\n",
    "    dataset=dataset,\n",
    "    epochs=2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"✅ Training completed successfully!\")\n",
    "\n",
    "# Alternative approach if you need to track specific metrics during training:\n",
    "# You can access the training history and compute metrics manually\n",
    "print(\"\\n=== Training metrics ===\")\n",
    "if hasattr(history, 'history'):\n",
    "    for metric_name, values in history.history.items():\n",
    "        print(f\"{metric_name}: {values[-1]:.4f}\")\n",
    "\n",
    "# After training, use `approximator.infer()` or `approximator.predict()` for new sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411b01b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38fb254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b76af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b11fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82448087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111379df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e55b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6235e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a800a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015da08d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a457ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bf06f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7cce0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d4dec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e99ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e130f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3a40881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from hmmlearn import hmm\n",
    "\n",
    "# # 1) Define HMM parameters (fixed)\n",
    "# n_states       = 2    # 0=\"other\", 1=\"alpha-helix\"\n",
    "# n_observations = 20   # 20 amino acids\n",
    "\n",
    "# startprob = np.array([1.0, 0.0])  # always start in \"other\"\n",
    "# transmat  = np.array([[0.95, 0.05],  # other→(other,helix)\n",
    "#                       [0.10, 0.90]]) # helix→(other,helix)\n",
    "# emissionprob = np.array([\n",
    "#     # other\n",
    "#     [0.06, 0.05, 0.05, 0.06, 0.02, 0.05, 0.03, 0.09, 0.03, 0.05,\n",
    "#      0.08, 0.06, 0.02, 0.04, 0.06, 0.07, 0.06, 0.01, 0.04, 0.07],\n",
    "#     # alpha-helix\n",
    "#     [0.12, 0.06, 0.03, 0.05, 0.01, 0.09, 0.05, 0.04, 0.02, 0.07,\n",
    "#      0.12, 0.06, 0.03, 0.04, 0.02, 0.05, 0.04, 0.01, 0.03, 0.06],\n",
    "# ])\n",
    "\n",
    "# # 2) Build a no-EM CategoricalHMM for decoding\n",
    "# model = hmm.CategoricalHMM(\n",
    "#     n_components=n_states,\n",
    "#     algorithm='viterbi',   # use Viterbi for predict()\n",
    "#     n_iter=0,              # skip EM\n",
    "#     init_params=''         # keep our fixed parameters\n",
    "# )\n",
    "# model.startprob_    = startprob\n",
    "# model.transmat_     = transmat\n",
    "# model.emissionprob_ = emissionprob\n",
    "\n",
    "# # 3) Custom-loop simulator\n",
    "# def simulate_loop(sequence_length: int, batch_size: int, random_state=None):\n",
    "#     \"\"\"\n",
    "#     Simulate `batch_size` sequences of length `sequence_length` by hand,\n",
    "#     then decode their most likely state-sequences with Viterbi.\n",
    "    \n",
    "#     Returns:\n",
    "#       X_batch: shape (batch_size, sequence_length), int observations 0..19\n",
    "#       Z_batch: shape (batch_size, sequence_length), int Viterbi state path 0/1\n",
    "#     \"\"\"\n",
    "#     # Initialize RNG\n",
    "#     rng = np.random.default_rng(random_state)  # recommended over np.random.*\n",
    "    \n",
    "#     X_batch = np.zeros((batch_size, sequence_length), dtype=int)\n",
    "#     Z_batch = np.zeros((batch_size, sequence_length), dtype=int)\n",
    "    \n",
    "#     for i in range(batch_size):\n",
    "#         # a) Sample latent state path and emissions\n",
    "#         states = np.empty(sequence_length, dtype=int)\n",
    "#         obs    = np.empty(sequence_length, dtype=int)\n",
    "        \n",
    "#         state = 0  # start in \"other\"\n",
    "#         for t in range(sequence_length):\n",
    "#             # emit an amino acid given current state\n",
    "#             obs[t] = rng.choice(\n",
    "#                 n_observations,\n",
    "#                 p=emissionprob[state]\n",
    "#             )  # numpy.random.choice :contentReference[oaicite:3]{index=3}\n",
    "            \n",
    "#             states[t] = state\n",
    "#             # transition to next state\n",
    "#             state = rng.choice(n_states, p=transmat[state])\n",
    "        \n",
    "#         # b) Viterbi‐decode the *observations* (not the true states)\n",
    "#         #    predict() uses Viterbi by default when algorithm='viterbi'\n",
    "#         decoded = model.predict(obs.reshape(-1, 1))\n",
    "#         X_batch[i] = obs\n",
    "#         Z_batch[i] = decoded  # most likely state path :contentReference[oaicite:4]{index=4}\n",
    "    \n",
    "#     return X_batch, Z_batch\n",
    "\n",
    "# # Quick sanity check\n",
    "# if __name__ == \"__main__\":\n",
    "#     Xb, Zb = simulate_loop(sequence_length=100, batch_size=2, random_state=123)\n",
    "#     print(\"Xb.shape:\", Xb.shape, \"Zb.shape:\", Zb.shape)\n",
    "#     print(\"First 10 obs:\", Xb[0, :10])\n",
    "#     print(\"First 10 decoded states:\", Zb[0, :10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
