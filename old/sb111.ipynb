{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa8134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 12:53:13.976800: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-07-13 12:53:13.976837: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-07-13 12:53:13.976842: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752403993.976855 6233715 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1752403993.976878 6233715 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "INFO:bayesflow:Using backend 'tensorflow'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.keras is using the 'tensorflow' backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import bayesflow as bf\n",
    "\n",
    "from hmmlearn import hmm\n",
    "from hmmlearn.hmm import CategoricalHMM\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "current_backend = tf.keras.backend.backend()\n",
    "print(f\"tf.keras is using the '{current_backend}' backend.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1997ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETER VALIDATION:\n",
      "Amino acids: 20 types\n",
      "Alpha emission sum: 1.000\n",
      "Other emission sum: 1.000\n",
      "Alpha transitions sum: 1.000\n",
      "Other transitions sum: 1.000\n",
      "Initial probs sum: 1.000\n",
      "\n",
      "✓ All probabilities are valid!\n"
     ]
    }
   ],
   "source": [
    "# HMM PARAMETERS FROM TASK DESCRIPTION\n",
    "\n",
    "# 20 amino acids in standard order\n",
    "AMINO_ACIDS = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I', \n",
    "               'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "\n",
    "# Emission probabilities from task tables\n",
    "# Alpha-helix state (state 0)\n",
    "EMISSION_ALPHA = [0.12, 0.06, 0.03, 0.05, 0.01, 0.09, 0.05, 0.04, 0.02, 0.07,\n",
    "                  0.12, 0.06, 0.03, 0.04, 0.02, 0.05, 0.04, 0.01, 0.03, 0.06]\n",
    "\n",
    "# Other state (state 1) \n",
    "EMISSION_OTHER = [0.06, 0.05, 0.05, 0.06, 0.02, 0.05, 0.03, 0.09, 0.03, 0.05,\n",
    "                  0.08, 0.06, 0.02, 0.04, 0.06, 0.07, 0.06, 0.01, 0.04, 0.07]\n",
    "\n",
    "# Transition probabilities from task description\n",
    "# [alpha->alpha, alpha->other]\n",
    "TRANS_FROM_ALPHA = [0.90, 0.10]\n",
    "# [other->alpha, other->other]  \n",
    "TRANS_FROM_OTHER = [0.05, 0.95]\n",
    "\n",
    "# Initial state probabilities (always starts in \"other\" state)\n",
    "INITIAL_PROBS = [0.0, 1.0]  # [alpha-helix, other]\n",
    "\n",
    "# Validation\n",
    "print(\"PARAMETER VALIDATION:\")\n",
    "print(f\"Amino acids: {len(AMINO_ACIDS)} types\")\n",
    "print(f\"Alpha emission sum: {sum(EMISSION_ALPHA):.3f}\")\n",
    "print(f\"Other emission sum: {sum(EMISSION_OTHER):.3f}\")\n",
    "print(f\"Alpha transitions sum: {sum(TRANS_FROM_ALPHA):.3f}\")\n",
    "print(f\"Other transitions sum: {sum(TRANS_FROM_OTHER):.3f}\")\n",
    "print(f\"Initial probs sum: {sum(INITIAL_PROBS):.3f}\")\n",
    "print(\"\\n✓ All probabilities are valid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d71a0e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING HMM CREATION:\n",
      "\n",
      "States: 2\n",
      "Features: 20\n",
      "Start probabilities: [0. 1.]\n",
      "Transition matrix shape: (2, 2)\n",
      "Emission matrix shape: (2, 20)\n",
      "\n",
      "Transition probabilities:\n",
      "From alpha-helix: [0.9 0.1]\n",
      "From other:      [0.05 0.95]\n",
      "\n",
      "Emission probabilities (first 5 amino acids):\n",
      "Alpha-helix: [0.12 0.06 0.03 0.05 0.01]\n",
      "Other:       [0.06 0.05 0.05 0.06 0.02]\n",
      "\n",
      "✓ HMM model created successfully!\n"
     ]
    }
   ],
   "source": [
    "# FIXED HMM MODEL CREATION\n",
    "\n",
    "def create_fixed_hmm():\n",
    "    \"\"\"\n",
    "    Create HMM with fixed parameters from task description.\n",
    "    \n",
    "    States: 0=alpha-helix, 1=other\n",
    "    Features: 20 amino acids (0-19 indices)\n",
    "    \n",
    "    Returns:\n",
    "        CategoricalHMM with fixed empirical parameters\n",
    "    \"\"\"\n",
    "    # Create model with fixed parameters (no learning)\n",
    "    model = hmm.CategoricalHMM(\n",
    "        n_components=2,        # 2 states: alpha-helix, other\n",
    "        n_features=20,         # 20 amino acids\n",
    "        params=\"\",             # Don't update any parameters\n",
    "        init_params=\"\",        # Don't initialize any parameters\n",
    "        algorithm=\"viterbi\",   # Use Viterbi algorithm for decoding\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Set fixed parameters from task description\n",
    "    model.startprob_ = np.array(INITIAL_PROBS)\n",
    "    model.transmat_ = np.array([TRANS_FROM_ALPHA, TRANS_FROM_OTHER])\n",
    "    model.emissionprob_ = np.array([EMISSION_ALPHA, EMISSION_OTHER])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test HMM creation\n",
    "print(\"TESTING HMM CREATION:\\n\")\n",
    "hmm_model = create_fixed_hmm()\n",
    "\n",
    "print(f\"States: {hmm_model.n_components}\")\n",
    "print(f\"Features: {hmm_model.n_features}\")\n",
    "print(f\"Start probabilities: {hmm_model.startprob_}\")\n",
    "print(f\"Transition matrix shape: {hmm_model.transmat_.shape}\")\n",
    "print(f\"Emission matrix shape: {hmm_model.emissionprob_.shape}\")\n",
    "\n",
    "print(\"\\nTransition probabilities:\")\n",
    "print(\"From alpha-helix:\", hmm_model.transmat_[0])\n",
    "print(\"From other:     \", hmm_model.transmat_[1])\n",
    "\n",
    "print(\"\\nEmission probabilities (first 5 amino acids):\")\n",
    "print(\"Alpha-helix:\", hmm_model.emissionprob_[0][:5])\n",
    "print(\"Other:      \", hmm_model.emissionprob_[1][:5])\n",
    "print(\"\\n✓ HMM model created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9231677e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING HMM DATA GENERATION:\n",
      "\n",
      "Amino acids shape: (20,)\n",
      "True states shape: (20,)\n",
      "State probabilities shape: (20, 2)\n",
      "\n",
      "First 10 amino acids (indices): [19 11  2 16 14 19  3  2  9  5]\n",
      "First 10 true states: [1 1 1 1 1 0 0 0 0 0]\n",
      "First 5 state probabilities:\n",
      "[[0.         1.        ]\n",
      " [0.01768884 0.98231116]\n",
      " [0.0253218  0.9746782 ]\n",
      " [0.03656372 0.96343628]\n",
      " [0.05153765 0.94846235]]\n",
      "\n",
      "State probabilities sum check: True\n",
      "First 10 amino acids (letters): ['V', 'K', 'N', 'T', 'P', 'V', 'D', 'N', 'I', 'E']\n",
      "\n",
      "✓ HMM data generation working correctly!\n"
     ]
    }
   ],
   "source": [
    "# HMM DATA GENERATION AND SIMULATOR FUNCTIONS\n",
    "\n",
    "def generate_amino_acid_sequence(n_samples=50, random_state=None):\n",
    "    \"\"\"\n",
    "    Generate amino acid sequences from the fixed HMM.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of amino acids to generate\n",
    "        random_state: Random state for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        dict with 'amino_acids', 'true_states', and 'state_probs'\n",
    "    \"\"\"\n",
    "    # Create the fixed HMM model\n",
    "    model = create_fixed_hmm()\n",
    "    \n",
    "    # Generate sequence from HMM\n",
    "    X, Z = model.sample(n_samples, random_state=random_state)\n",
    "    \n",
    "    # X is shape (n_samples, 1) - amino acid indices\n",
    "    # Z is shape (n_samples,) - true hidden states\n",
    "    amino_acids = X.flatten()  # Convert to 1D array of amino acid indices\n",
    "    \n",
    "    # Get state membership probabilities using Forward-Backward algorithm\n",
    "    # Need to reshape X for predict_proba (expects (n_samples, 1))\n",
    "    state_probs = model.predict_proba(X)  # Shape: (n_samples, n_states)\n",
    "    \n",
    "    return {\n",
    "        'amino_acids': amino_acids,       # Shape: (n_samples,) - amino acid indices (0-19)\n",
    "        'true_states': Z,                 # Shape: (n_samples,) - true hidden states (0=alpha, 1=other) \n",
    "        'state_probs': state_probs        # Shape: (n_samples, 2) - state membership probabilities\n",
    "    }\n",
    "\n",
    "# Test the data generation\n",
    "print(\"TESTING HMM DATA GENERATION:\\n\")\n",
    "test_data = generate_amino_acid_sequence(n_samples=20, random_state=42)\n",
    "\n",
    "print(f\"Amino acids shape: {test_data['amino_acids'].shape}\")\n",
    "print(f\"True states shape: {test_data['true_states'].shape}\")\n",
    "print(f\"State probabilities shape: {test_data['state_probs'].shape}\")\n",
    "\n",
    "print(f\"\\nFirst 10 amino acids (indices): {test_data['amino_acids'][:10]}\")\n",
    "print(f\"First 10 true states: {test_data['true_states'][:10]}\")\n",
    "print(f\"First 5 state probabilities:\\n{test_data['state_probs'][:5]}\")\n",
    "\n",
    "# Verify state probabilities sum to 1\n",
    "print(f\"\\nState probabilities sum check: {np.allclose(test_data['state_probs'].sum(axis=1), 1.0)}\")\n",
    "\n",
    "# Convert amino acid indices to actual amino acid letters for readability\n",
    "amino_acid_letters = [AMINO_ACIDS[idx] for idx in test_data['amino_acids'][:10]]\n",
    "print(f\"First 10 amino acids (letters): {amino_acid_letters}\")\n",
    "print(\"\\n✓ HMM data generation working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93f2cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING BAYESFLOW SIMULATOR:\n",
      "\n",
      "✓ BayesFlow LambdaSimulator created successfully!\n",
      "\n",
      "TESTING BAYESFLOW SIMULATOR:\n",
      "Simulation data keys: ['amino_acids', 'true_states', 'state_probs']\n",
      "Amino acids batch shape: (3, 15)\n",
      "True states batch shape: (3, 15)\n",
      "State probabilities batch shape: (3, 15, 2)\n",
      "\n",
      "First 2 sequences:\n",
      "\n",
      "Sequence 0:\n",
      "Amino acids: [19  1  3  0 16 12 12 14 11  8 15  3 15  7 17]\n",
      "True states: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "State probabilities shape: (15, 2)\n",
      "State probabilities sum check: True\n",
      "Sequnce length: 15\n",
      "\n",
      "Sequence 1:\n",
      "Amino acids: [ 7  0  4  0 15 14  9  2  5 15 12  6  4  2  1]\n",
      "True states: [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      "State probabilities shape: (15, 2)\n",
      "State probabilities sum check: True\n",
      "Sequnce length: 15\n",
      "Amino acid letters: ['V', 'R', 'D', 'A', 'T', 'M', 'M', 'P', 'K', 'H', 'S', 'D', 'S', 'G', 'W']\n",
      "\n",
      "✓ BayesFlow simulator working correctly!\n"
     ]
    }
   ],
   "source": [
    "# BAYESFLOW SIMULATOR IMPLEMENTATION\n",
    "\n",
    "def hmm_simulator_function(batch_shape, sequence_length=50, **kwargs):\n",
    "    \"\"\"\n",
    "    Simulator function for BayesFlow that generates HMM data.\n",
    "    \n",
    "    This function will be wrapped by BayesFlow's LambdaSimulator.\n",
    "    \n",
    "    Args:\n",
    "        batch_shape: Shape of the batch to generate (from BayesFlow)\n",
    "        sequence_length: Length of amino acid sequences to generate\n",
    "        **kwargs: Additional keyword arguments\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with simulation outputs for BayesFlow\n",
    "    \"\"\"\n",
    "    # Handle both int and tuple batch_shape\n",
    "    if isinstance(batch_shape, int):\n",
    "        batch_size = batch_shape\n",
    "    else:\n",
    "        batch_size = batch_shape[0] if len(batch_shape) > 0 else 1\n",
    "    \n",
    "    # Generate multiple sequences\n",
    "    amino_acids_batch = []\n",
    "    true_states_batch = []\n",
    "    state_probs_batch = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Generate one sequence with different random state for each\n",
    "        data = generate_amino_acid_sequence(\n",
    "            n_samples=sequence_length, \n",
    "            random_state=np.random.randint(0, 10000)\n",
    "        )\n",
    "        \n",
    "        amino_acids_batch.append(data['amino_acids'])\n",
    "        true_states_batch.append(data['true_states'])\n",
    "        state_probs_batch.append(data['state_probs'])\n",
    "    \n",
    "    # Stack into batch format\n",
    "    return {\n",
    "        'amino_acids': np.array(amino_acids_batch),      # Shape: (batch_size, sequence_length)\n",
    "        'true_states': np.array(true_states_batch),      # Shape: (batch_size, sequence_length)\n",
    "        'state_probs': np.array(state_probs_batch),      # Shape: (batch_size, sequence_length, 2)\n",
    "    }\n",
    "\n",
    "# Create BayesFlow simulator\n",
    "print(\"CREATING BAYESFLOW SIMULATOR:\\n\")\n",
    "hmm_simulator = bf.simulators.LambdaSimulator(\n",
    "    sample_fn=hmm_simulator_function,\n",
    "    is_batched=True  # Our function handles batching internally\n",
    ")\n",
    "\n",
    "print(\"✓ BayesFlow LambdaSimulator created successfully!\")\n",
    "\n",
    "# Test the BayesFlow simulator\n",
    "print(\"\\nTESTING BAYESFLOW SIMULATOR:\")\n",
    "batch_size = 3\n",
    "sequence_length = 15\n",
    "\n",
    "# Sample from the simulator\n",
    "simulation_data = hmm_simulator.sample(\n",
    "    batch_shape=(batch_size,), \n",
    "    sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "print(f\"Simulation data keys: {list(simulation_data.keys())}\")\n",
    "print(f\"Amino acids batch shape: {simulation_data['amino_acids'].shape}\")\n",
    "print(f\"True states batch shape: {simulation_data['true_states'].shape}\")\n",
    "print(f\"State probabilities batch shape: {simulation_data['state_probs'].shape}\")\n",
    "\n",
    "# Show multiple sequences\n",
    "num_seq = 2\n",
    "print(f\"\\nFirst {num_seq} sequences:\")\n",
    "for i in range(num_seq):\n",
    "    amino_acids = simulation_data['amino_acids'][i]\n",
    "    true_states = simulation_data['true_states'][i]\n",
    "    state_probs = simulation_data['state_probs'][i]\n",
    "    \n",
    "    print(f\"\\nSequence {i}:\")\n",
    "    print(f\"Amino acids: {amino_acids}\")\n",
    "    print(f\"True states: {true_states}\")\n",
    "    print(f\"State probabilities shape: {state_probs.shape}\")\n",
    "    print(f\"State probabilities sum check: {np.allclose(state_probs.sum(axis=1), 1.0)}\")\n",
    "    print(f\"Sequnce length: {len(amino_acids)}\")\n",
    "\n",
    "# Convert first sequence to amino acid letters\n",
    "example_letters = [AMINO_ACIDS[idx] for idx in simulation_data['amino_acids'][0]]\n",
    "print(f\"Amino acid letters: {example_letters}\")\n",
    "\n",
    "print(\"\\n✓ BayesFlow simulator working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd8ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Custom ProteinSummaryNetwork class defined\n"
     ]
    }
   ],
   "source": [
    "# CUSTOM PROTEIN SUMMARY NETWORK\n",
    "\n",
    "class ProteinSummaryNetwork(bf.networks.SummaryNetwork):\n",
    "    \"\"\"\n",
    "    Custom summary network for protein amino acid sequences.\n",
    "    \n",
    "    This network is specifically designed for the protein secondary structure task:\n",
    "    - Embeds amino acid indices into dense representations\n",
    "    - Uses bidirectional LSTM to capture sequential dependencies\n",
    "    - Applies attention mechanism to focus on important positions\n",
    "    - Outputs summary statistics for the entire sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size=20,              # Number of amino acids\n",
    "                 embedding_dim=32,           # Amino acid embedding dimension\n",
    "                 lstm_units=64,              # LSTM hidden units\n",
    "                 attention_dim=32,           # Attention mechanism dimension\n",
    "                 summary_dim=64,             # Output summary dimension\n",
    "                 dropout_rate=0.1,           # Dropout rate\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.attention_dim = attention_dim\n",
    "        self.summary_dim = summary_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Amino acid embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            mask_zero=False,  # Don't mask zero values as amino acid 'A' has index 0\n",
    "            name='amino_acid_embedding'\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM for sequence processing\n",
    "        self.lstm = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(\n",
    "                lstm_units,\n",
    "                return_sequences=True,  # Return full sequence for attention\n",
    "                dropout=dropout_rate,\n",
    "                recurrent_dropout=dropout_rate,\n",
    "                name='sequence_lstm'\n",
    "            ),\n",
    "            name='bidirectional_lstm'\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism layers\n",
    "        self.attention_dense = tf.keras.layers.Dense(\n",
    "            attention_dim, \n",
    "            activation='tanh',\n",
    "            name='attention_dense'\n",
    "        )\n",
    "        self.attention_weights = tf.keras.layers.Dense(\n",
    "            1, \n",
    "            activation=None,  # Don't use softmax here, apply it later\n",
    "            name='attention_weights'\n",
    "        )\n",
    "        \n",
    "        # Final summary layers\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.summary_dense1 = tf.keras.layers.Dense(\n",
    "            summary_dim * 2,\n",
    "            activation='silu',\n",
    "            name='summary_dense1'\n",
    "        )\n",
    "        self.summary_dense2 = tf.keras.layers.Dense(\n",
    "            summary_dim,\n",
    "            activation='silu', \n",
    "            name='summary_dense2'\n",
    "        )\n",
    "        \n",
    "    def call(self, x, training=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass of the protein summary network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, sequence_length, 1) containing amino acid indices\n",
    "            training: Whether in training mode\n",
    "            \n",
    "        Returns:\n",
    "            Summary tensor of shape (batch_size, summary_dim)\n",
    "        \"\"\"\n",
    "        # Remove the last dimension if present: (batch_size, seq_len, 1) -> (batch_size, seq_len)\n",
    "        if x.shape[-1] == 1:\n",
    "            x = tf.squeeze(x, axis=-1)\n",
    "            \n",
    "        # Convert to integer indices for embedding\n",
    "        x = tf.cast(x, tf.int32)\n",
    "        \n",
    "        # Embed amino acid indices: (batch_size, seq_len) -> (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Process with bidirectional LSTM: (batch_size, seq_len, embedding_dim) -> (batch_size, seq_len, 2*lstm_units)\n",
    "        lstm_output = self.lstm(embedded, training=training)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        # Compute attention scores: (batch_size, seq_len, 2*lstm_units) -> (batch_size, seq_len, attention_dim)\n",
    "        attention_scores = self.attention_dense(lstm_output)\n",
    "        \n",
    "        # Compute attention weights: (batch_size, seq_len, attention_dim) -> (batch_size, seq_len, 1)\n",
    "        attention_logits = self.attention_weights(attention_scores)\n",
    "        \n",
    "        # Apply softmax along the sequence dimension to get proper attention weights\n",
    "        attention_weights = tf.nn.softmax(attention_logits, axis=1)  # Softmax over sequence dimension\n",
    "        \n",
    "        # Apply attention: weighted sum of LSTM outputs\n",
    "        # (batch_size, seq_len, 2*lstm_units) * (batch_size, seq_len, 1) -> (batch_size, 2*lstm_units)\n",
    "        attended_output = tf.reduce_sum(lstm_output * attention_weights, axis=1)\n",
    "        \n",
    "        # Apply dropout\n",
    "        attended_output = self.dropout(attended_output, training=training)\n",
    "        \n",
    "        # Generate final summary through dense layers\n",
    "        summary = self.summary_dense1(attended_output)\n",
    "        summary = self.dropout(summary, training=training)\n",
    "        summary = self.summary_dense2(summary)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"Return the configuration of the layer.\"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'lstm_units': self.lstm_units,\n",
    "            'attention_dim': self.attention_dim,\n",
    "            'summary_dim': self.summary_dim,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"Create layer from configuration.\"\"\"\n",
    "        return cls(**config)\n",
    "\n",
    "print(\"✓ Custom ProteinSummaryNetwork class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9de9857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training function defined\n"
     ]
    }
   ],
   "source": [
    "# TRAINING FUNCTION FOR CUSTOM PROTEIN WORKFLOW\n",
    "\n",
    "def train_protein_workflow(\n",
    "    workflow,\n",
    "    batch_size=16,\n",
    "    epochs=50,\n",
    "    print_every=10,\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the protein BayesFlow workflow with our custom summary network.\n",
    "    \n",
    "    Args:\n",
    "        workflow: The BayesFlow workflow to train\n",
    "        batch_size: Batch size for training\n",
    "        epochs: Number of training epochs\n",
    "        print_every: Print progress every N epochs\n",
    "        save_path: Path to save the trained model (optional)\n",
    "    \n",
    "    Returns:\n",
    "        training_history: Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting training for {epochs} epochs with batch size {batch_size}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    training_history = {\n",
    "        'epoch': [],\n",
    "        'loss': [],\n",
    "        'validation_loss': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Configure the workflow for training\n",
    "        config = {\n",
    "            'epochs': epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'validation_sims': 1000,  # Generate validation data\n",
    "            'checkpoint_interval': max(1, epochs // 10),  # Save checkpoints\n",
    "        }\n",
    "        \n",
    "        print(\"Training configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print()\n",
    "        \n",
    "        # Start online training\n",
    "        print(\"🚀 Starting online training...\")\n",
    "        training_info = workflow.fit_online(\n",
    "            epochs=config['epochs'],\n",
    "            batch_size=config['batch_size'],\n",
    "            print_every=print_every\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Training completed successfully!\")\n",
    "        \n",
    "        # Extract training history if available\n",
    "        if hasattr(training_info, 'history') and training_info.history:\n",
    "            history = training_info.history\n",
    "            training_history['loss'] = history.get('loss', [])\n",
    "            training_history['validation_loss'] = history.get('val_loss', [])\n",
    "            training_history['epoch'] = list(range(1, len(training_history['loss']) + 1))\n",
    "        \n",
    "        # Save the model if path provided\n",
    "        if save_path:\n",
    "            print(f\"💾 Saving model to {save_path}\")\n",
    "            workflow.save_model(save_path)\n",
    "            \n",
    "        return training_history\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return training_history\n",
    "\n",
    "print(\"✓ Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8575201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Creating corrected protein BayesFlow workflow...\n",
      "✓ Using existing HMM simulator\n",
      "✓ Custom summary network created\n",
      "✓ Inference network created\n",
      "✓ Adapter with transforms created\n",
      "✓ BayesFlow workflow created\n",
      "============================================================\n",
      "🎉 Corrected protein BayesFlow workflow created successfully!\n"
     ]
    }
   ],
   "source": [
    "# CORRECTED PROTEIN BAYESFLOW WORKFLOW CREATION\n",
    "\n",
    "# First, let's create a custom flattening transform\n",
    "class FlattenTransform(bf.adapters.transforms.Transform):\n",
    "    \"\"\"Custom transform to flatten inference variables from (batch, seq_len, 2) to (batch, seq_len*2)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        # Flatten the last two dimensions: (batch, seq_len, 2) -> (batch, seq_len*2)\n",
    "        return x.reshape(x.shape[0], -1).astype(np.float32)\n",
    "    \n",
    "    def inverse(self, x, **kwargs):\n",
    "        # For inverse, we would need to know the original shape\n",
    "        # This is not needed for our use case but required by the interface\n",
    "        raise NotImplementedError(\"Inverse transform not implemented for FlattenTransform\")\n",
    "\n",
    "def create_corrected_protein_workflow():\n",
    "    \"\"\"\n",
    "    Create BayesFlow workflow with custom protein summary network and proper adapter.\n",
    "    \"\"\"\n",
    "    print(\"Creating corrected protein BayesFlow workflow...\")\n",
    "    \n",
    "    # 1. USE EXISTING SIMULATOR\n",
    "    simulator = hmm_simulator\n",
    "    print(\"✓ Using existing HMM simulator\")\n",
    "    \n",
    "    # 2. CUSTOM SUMMARY NETWORK\n",
    "    protein_summary_net = ProteinSummaryNetwork(\n",
    "        vocab_size=20,\n",
    "        embedding_dim=32,\n",
    "        lstm_units=64,\n",
    "        attention_dim=32,\n",
    "        summary_dim=64,\n",
    "        name='ProteinSummaryNetwork'\n",
    "    )\n",
    "    print(\"✓ Custom summary network created\")\n",
    "    \n",
    "    # 3. INFERENCE NETWORK\n",
    "    inference_net = bf.networks.CouplingFlow(\n",
    "        num_params=100,  # Flattened state probabilities: 50 positions * 2 states = 100\n",
    "        num_coupling_layers=8,\n",
    "        coupling_settings={'units': [128, 128], 'activation': 'silu'},\n",
    "        name='ProteinInferenceNetwork'\n",
    "    )\n",
    "    print(\"✓ Inference network created\")\n",
    "    \n",
    "    # 4. ADAPTER WITH CORRECT TRANSFORMS AND FLATTENING\n",
    "    adapter_transforms = [\n",
    "        # Rename variables to BayesFlow conventions\n",
    "        bf.adapters.transforms.Rename(from_key='amino_acids', to_key='summary_variables'),\n",
    "        bf.adapters.transforms.Rename(from_key='state_probs', to_key='inference_variables'),\n",
    "        \n",
    "        # Drop unused variables\n",
    "        bf.adapters.transforms.Drop(keys=['true_states']),\n",
    "        \n",
    "        # Convert data types\n",
    "        bf.adapters.transforms.MapTransform({\n",
    "            'summary_variables': bf.adapters.transforms.ConvertDType(\n",
    "                from_dtype='int64', to_dtype='float32'\n",
    "            ),\n",
    "            'inference_variables': bf.adapters.transforms.ConvertDType(\n",
    "                from_dtype='float64', to_dtype='float32'\n",
    "            ),\n",
    "        }),\n",
    "        \n",
    "        # Flatten inference variables using our custom transform\n",
    "        bf.adapters.transforms.MapTransform({\n",
    "            'inference_variables': FlattenTransform(),\n",
    "        }),\n",
    "    ]\n",
    "    \n",
    "    adapter = bf.Adapter(transforms=adapter_transforms)\n",
    "    print(\"✓ Adapter with transforms created\")\n",
    "    \n",
    "    # 5. CREATE WORKFLOW\n",
    "    workflow = bf.BasicWorkflow(\n",
    "        simulator=simulator,\n",
    "        adapter=adapter,\n",
    "        inference_network=inference_net,\n",
    "        summary_network=protein_summary_net\n",
    "    )\n",
    "    print(\"✓ BayesFlow workflow created\")\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "# Create the corrected workflow\n",
    "print(\"=\" * 60)\n",
    "corrected_protein_workflow = create_corrected_protein_workflow()\n",
    "print(\"=\" * 60)\n",
    "print(\"🎉 Corrected protein BayesFlow workflow created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c72730e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bayesflow:Fitting on dataset instance of OnlineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "INFO:bayesflow:Building on a test batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing training with corrected workflow...\n",
      "==================================================\n",
      "1. Testing simulation...\n",
      "✓ Simulation successful\n",
      "   Data keys: ['amino_acids', 'true_states', 'state_probs']\n",
      "   amino_acids: shape (2, 50), dtype int64\n",
      "   true_states: shape (2, 50), dtype int64\n",
      "   state_probs: shape (2, 50, 2), dtype float64\n",
      "\n",
      "2. Testing data adaptation...\n",
      "✓ Adaptation successful\n",
      "   Adapted keys: ['summary_variables', 'inference_variables']\n",
      "   summary_variables: shape (2, 50), dtype float32\n",
      "   inference_variables: shape (2, 100), dtype float32\n",
      "\n",
      "3. Testing training...\n",
      "Starting training for 3 epochs with batch size 4\n",
      "============================================================\n",
      "Training configuration:\n",
      "  epochs: 3\n",
      "  batch_size: 4\n",
      "  validation_sims: 1000\n",
      "  checkpoint_interval: 1\n",
      "\n",
      "🚀 Starting online training...\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 12:56:53.871214: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  7/100\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15:56\u001b[0m 10s/step - loss: 149.5542"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m3. Testing training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     test_history = \u001b[43mtrain_protein_workflow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mworkflow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorrected_protein_workflow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Small batch for testing\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Just a few epochs for testing\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Training test successful!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mtrain_protein_workflow\u001b[39m\u001b[34m(workflow, batch_size, epochs, print_every, save_path)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Start online training\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🚀 Starting online training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m training_info = \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_online\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mepochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_every\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Training completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Extract training history if available\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/workflows/basic_workflow.py:784\u001b[39m, in \u001b[36mBasicWorkflow.fit_online\u001b[39m\u001b[34m(self, epochs, num_batches_per_epoch, batch_size, keep_optimizer, validation_data, augmentations, **kwargs)\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    737\u001b[39m \u001b[33;03mTrain the approximator using an online data-generating process. The dataset is dynamically generated during\u001b[39;00m\n\u001b[32m    738\u001b[39m \u001b[33;03mtraining, making this approach suitable for scenarios where generating new simulations is computationally cheap.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    773\u001b[39m \u001b[33;03m    metric evolution over epochs.\u001b[39;00m\n\u001b[32m    774\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    776\u001b[39m dataset = OnlineDataset(\n\u001b[32m    777\u001b[39m     simulator=\u001b[38;5;28mself\u001b[39m.simulator,\n\u001b[32m    778\u001b[39m     batch_size=batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    781\u001b[39m     augmentations=augmentations,\n\u001b[32m    782\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43monline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_optimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/workflows/basic_workflow.py:954\u001b[39m, in \u001b[36mBasicWorkflow._fit\u001b[39m\u001b[34m(self, dataset, epochs, strategy, keep_optimizer, validation_data, **kwargs)\u001b[39m\n\u001b[32m    951\u001b[39m     \u001b[38;5;28mself\u001b[39m.approximator.compile(optimizer=\u001b[38;5;28mself\u001b[39m.optimizer, metrics=kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m     \u001b[38;5;28mself\u001b[39m.history = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapproximator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    957\u001b[39m     \u001b[38;5;28mself\u001b[39m._on_training_finished()\n\u001b[32m    958\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.history\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/approximators/continuous_approximator.py:316\u001b[39m, in \u001b[36mContinuousApproximator.fit\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    265\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[33;03m    Trains the approximator on the provided dataset or on-demand data generated from the given simulator.\u001b[39;00m\n\u001b[32m    267\u001b[39m \u001b[33;03m    If `dataset` is not provided, a dataset is built from the `simulator`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m \u001b[33;03m        If both `dataset` and `simulator` are provided or neither is provided.\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madapter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/approximators/approximator.py:139\u001b[39m, in \u001b[36mApproximator.fit\u001b[39m\u001b[34m(self, dataset, simulator, **kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m     mock_data_shapes = keras.tree.map_structure(keras.ops.shape, mock_data)\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m.build(mock_data_shapes)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/approximators/backend_approximators/backend_approximator.py:20\u001b[39m, in \u001b[36mBackendApproximator.fit\u001b[39m\u001b[34m(self, dataset, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, *, dataset: keras.utils.PyDataset, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfilter_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# TEST TRAINING WITH CORRECTED WORKFLOW\n",
    "\n",
    "print(\"🧪 Testing training with corrected workflow...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test simulation first\n",
    "print(\"1. Testing simulation...\")\n",
    "test_sim_data = corrected_protein_workflow.simulate(2)\n",
    "print(\"✓ Simulation successful\")\n",
    "print(\"   Data keys:\", list(test_sim_data.keys()))\n",
    "for key, value in test_sim_data.items():\n",
    "    print(f\"   {key}: shape {value.shape}, dtype {value.dtype}\")\n",
    "\n",
    "# Test adaptation\n",
    "print(\"\\n2. Testing data adaptation...\")\n",
    "test_adapted = corrected_protein_workflow.adapter(test_sim_data)\n",
    "print(\"✓ Adaptation successful\")\n",
    "print(\"   Adapted keys:\", list(test_adapted.keys()))\n",
    "for key, value in test_adapted.items():\n",
    "    print(f\"   {key}: shape {value.shape}, dtype {value.dtype}\")\n",
    "\n",
    "# Now test a few training epochs\n",
    "print(\"\\n3. Testing training...\")\n",
    "try:\n",
    "    test_history = train_protein_workflow(\n",
    "        workflow=corrected_protein_workflow,\n",
    "        batch_size=4,  # Small batch for testing\n",
    "        epochs=3,      # Just a few epochs for testing\n",
    "        print_every=1\n",
    "    )\n",
    "    print(\"✅ Training test successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Training test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL TRAINING SESSION\n",
    "\n",
    "print(\"🚀 Starting full training session...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train for more epochs to see meaningful learning\n",
    "full_training_history = train_protein_workflow(\n",
    "    workflow=corrected_protein_workflow,\n",
    "    batch_size=16,     # Reasonable batch size\n",
    "    epochs=20,         # More epochs for better learning\n",
    "    print_every=5      # Progress updates every 5 epochs\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display training summary\n",
    "if full_training_history and full_training_history['loss']:\n",
    "    print(f\"📊 Training Summary:\")\n",
    "    print(f\"   Total epochs: {len(full_training_history['loss'])}\")\n",
    "    print(f\"   Final loss: {full_training_history['loss'][-1]:.4f}\")\n",
    "    print(f\"   Loss reduction: {full_training_history['loss'][0]:.4f} → {full_training_history['loss'][-1]:.4f}\")\n",
    "    \n",
    "    # Simple plot of training loss\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(full_training_history['epoch'], full_training_history['loss'], 'b-', linewidth=2, label='Training Loss')\n",
    "    plt.title('BayesFlow Training Progress', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ No training history available to display\")\n",
    "\n",
    "print(\"\\n🧬 The protein secondary structure BayesFlow model is now trained!\")\n",
    "print(\"📈 Ready for posterior inference on new amino acid sequences!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84764596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 INVESTIGATING COUPLINGFLOW IMPLEMENTATION\n",
      "============================================================\n",
      "1. CouplingFlow class signature:\n",
      "(self, subnet: str | type = 'mlp', depth: int = 6, transform: str = 'affine', permutation: str | None = 'random', use_actnorm: bool = True, base_distribution: str = 'normal', subnet_kwargs: dict[str, any] = None, transform_kwargs: dict[str, any] = None, **kwargs)\n",
      "\n",
      "2. CouplingFlow class docstring:\n",
      "(IN) Implements a coupling flow as a sequence of dual couplings with permutations and activation\n",
      "    normalization. Incorporates ideas from [1-5].\n",
      "\n",
      "    [1] Kingma, D. P., & Dhariwal, P. (2018).\n",
      "    Glow: Generative flow with invertible 1x1 convolutions.\n",
      "    Advances in Neural Information Processing Systems, 31.\n",
      "\n",
      "    [2] Durkan, C., Bekasov, A., Murray, I., & Papamakarios, G. (2019).\n",
      "    Neural spline flows. Advances in Neural Information Processing Systems, 32.\n",
      "\n",
      "    [3] Ardizzone, L., Kruse, J., Lüth, C., Bracher, N., Rother, C., & Köthe, U. (2020).\n",
      "    Conditional invertible neural networks for diverse image-to-image translation.\n",
      "    In DAGM German Conference on Pattern Recognition (pp. 373-387). Springer, Cham.\n",
      "\n",
      "    [4] Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., & Köthe, U. (2020).\n",
      "    BayesFlow: Learning complex stochastic simulators with invertible neural networks.\n",
      "    IEEE Transactions on Neural Networks and Learning Systems.\n",
      "\n",
      "    [5] Alexanderson, S., & Henter, G. E. (2020).\n",
      "    Robust model training and generalisation with Studentising flows.\n",
      "    arXiv preprint arXiv:2006.06599.\n",
      "    \n",
      "\n",
      "3. CouplingFlow available attributes/methods:\n",
      "['add_loss', 'add_metric', 'add_variable', 'add_weight', 'build', 'build_from_config', 'call', 'compute_dtype', 'compute_mask', 'compute_metrics', 'compute_output_shape', 'compute_output_spec', 'count_params', 'dtype', 'dtype_policy', 'from_config', 'get_build_config', 'get_config', 'get_weights', 'input', 'input_dtype', 'input_spec', 'load_own_variables', 'log_prob', 'losses', 'metrics', 'metrics_variables', 'non_trainable_variables', 'non_trainable_weights', 'output', 'path', 'quantization_mode', 'quantize', 'quantized_build', 'quantized_call', 'rematerialized_call', 'sample', 'save_own_variables', 'set_weights', 'stateless_call', 'supports_masking', 'symbolic_call', 'trainable', 'trainable_variables', 'trainable_weights', 'variable_dtype', 'variables', 'weights']\n",
      "\n",
      "4. Let's check what networks are available in BayesFlow:\n",
      "Available networks: ['ConsistencyModel', 'CouplingFlow', 'DeepSet', 'FlowMatching', 'FusionNetwork', 'FusionTransformer', 'InferenceNetwork', 'MLP', 'PointInferenceNetwork', 'Sequential', 'SetTransformer', 'SummaryNetwork', 'TimeSeriesNetwork', 'TimeSeriesTransformer', 'consistency_models', 'coupling_flow', 'deep_set', 'embeddings', 'flow_matching', 'fusion_network', 'inference_network', 'mlp', 'point_inference_network', 'residual', 'sequential', 'standardization', 'summary_network', 'time_series_network', 'transformers']\n",
      "\n",
      "5. Checking if there are other coupling-related networks:\n",
      "Coupling/Flow networks: ['CouplingFlow', 'FlowMatching', 'coupling_flow', 'flow_matching']\n"
     ]
    }
   ],
   "source": [
    "# INVESTIGATING BAYESFLOW COUPLINGFLOW PARAMETERS\n",
    "\n",
    "print(\"🔍 INVESTIGATING COUPLINGFLOW IMPLEMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check the CouplingFlow class signature and documentation\n",
    "print(\"1. CouplingFlow class signature:\")\n",
    "import inspect\n",
    "print(inspect.signature(bf.networks.CouplingFlow.__init__))\n",
    "\n",
    "print(\"\\n2. CouplingFlow class docstring:\")\n",
    "print(bf.networks.CouplingFlow.__doc__)\n",
    "\n",
    "print(\"\\n3. CouplingFlow available attributes/methods:\")\n",
    "coupling_flow_attrs = [attr for attr in dir(bf.networks.CouplingFlow) if not attr.startswith('_')]\n",
    "print(coupling_flow_attrs)\n",
    "\n",
    "print(\"\\n4. Let's check what networks are available in BayesFlow:\")\n",
    "available_networks = [attr for attr in dir(bf.networks) if not attr.startswith('_')]\n",
    "print(\"Available networks:\", available_networks)\n",
    "\n",
    "print(\"\\n5. Checking if there are other coupling-related networks:\")\n",
    "coupling_networks = [attr for attr in available_networks if 'coup' in attr.lower() or 'flow' in attr.lower()]\n",
    "print(\"Coupling/Flow networks:\", coupling_networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb3dc52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 UNDERSTANDING BAYESFLOW PARAMETER HANDLING\n",
      "============================================================\n",
      "1. BasicWorkflow signature:\n",
      "(self, simulator: bayesflow.simulators.simulator.Simulator = None, adapter: bayesflow.adapters.adapter.Adapter = None, inference_network: bayesflow.networks.inference_network.InferenceNetwork | str = 'coupling_flow', summary_network: bayesflow.networks.summary_network.SummaryNetwork | str = None, initial_learning_rate: float = 0.0005, optimizer: keras.src.optimizers.optimizer.Optimizer | type = None, checkpoint_filepath: str = None, checkpoint_name: str = 'model', save_weights_only: bool = False, save_best_only: bool = False, inference_variables: collections.abc.Sequence[str] | str = None, inference_conditions: collections.abc.Sequence[str] | str = None, summary_variables: collections.abc.Sequence[str] | str = None, standardize: collections.abc.Sequence[str] | str | None = 'inference_variables', **kwargs)\n",
      "\n",
      "2. Let's look at how CouplingFlow works with our current data:\n",
      "First, let's see our current data dimensions again:\n",
      "Simulated data:\n",
      "  amino_acids: (2, 50)\n",
      "  true_states: (2, 50)\n",
      "  state_probs: (2, 50, 2)\n",
      "\n",
      "Adapted data:\n",
      "  summary_variables: (2, 50)\n",
      "  inference_variables: (2, 100)\n",
      "\n",
      "3. The key insight: BayesFlow workflows automatically determine dimensions!\n",
      "   - BasicWorkflow builds the network based on actual data shapes\n",
      "   - The inference network gets built when it sees the inference_variables\n",
      "   - No need to specify num_params explicitly!\n",
      "\n",
      "4. Let's create a properly configured CouplingFlow:\n",
      "✓ Correct CouplingFlow created with proper parameters:\n",
      "  - Subnet type: mlp\n",
      "  - Depth (coupling layers): 8\n",
      "  - Transform: affine\n",
      "  - Subnet units: [128, 128]\n",
      "  - Activation: silu\n"
     ]
    }
   ],
   "source": [
    "# UNDERSTANDING HOW BAYESFLOW HANDLES PARAMETER DIMENSIONS\n",
    "\n",
    "print(\"🔍 UNDERSTANDING BAYESFLOW PARAMETER HANDLING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Let's check what a BasicWorkflow expects for inference networks\n",
    "print(\"1. BasicWorkflow signature:\")\n",
    "print(inspect.signature(bf.BasicWorkflow.__init__))\n",
    "\n",
    "print(\"\\n2. Let's look at how CouplingFlow works with our current data:\")\n",
    "print(\"First, let's see our current data dimensions again:\")\n",
    "\n",
    "# Test our simulator and adapter\n",
    "test_sim = corrected_protein_workflow.simulate(2)\n",
    "test_adapted = corrected_protein_workflow.adapter(test_sim)\n",
    "\n",
    "print(\"Simulated data:\")\n",
    "for key, value in test_sim.items():\n",
    "    print(f\"  {key}: {value.shape}\")\n",
    "\n",
    "print(\"\\nAdapted data:\")\n",
    "for key, value in test_adapted.items():\n",
    "    print(f\"  {key}: {value.shape}\")\n",
    "\n",
    "print(\"\\n3. The key insight: BayesFlow workflows automatically determine dimensions!\")\n",
    "print(\"   - BasicWorkflow builds the network based on actual data shapes\")\n",
    "print(\"   - The inference network gets built when it sees the inference_variables\")\n",
    "print(\"   - No need to specify num_params explicitly!\")\n",
    "\n",
    "print(\"\\n4. Let's create a properly configured CouplingFlow:\")\n",
    "\n",
    "# Create a new CouplingFlow with correct parameters\n",
    "correct_coupling_flow = bf.networks.CouplingFlow(\n",
    "    subnet='mlp',           # Use MLP subnets\n",
    "    depth=8,               # 8 coupling layers (equivalent to num_coupling_layers)\n",
    "    transform='affine',    # Affine coupling transforms\n",
    "    permutation='random',  # Random permutations between layers\n",
    "    subnet_kwargs={        # Arguments for the MLP subnets\n",
    "        'units': [128, 128],  # Hidden units in MLP\n",
    "        'activation': 'silu'  # Activation function\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✓ Correct CouplingFlow created with proper parameters:\")\n",
    "print(f\"  - Subnet type: mlp\")\n",
    "print(f\"  - Depth (coupling layers): 8\")\n",
    "print(f\"  - Transform: affine\")\n",
    "print(f\"  - Subnet units: [128, 128]\")\n",
    "print(f\"  - Activation: silu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8932561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Creating properly configured BayesFlow workflow...\n",
      "✓ Using existing HMM simulator\n",
      "✓ Custom summary network created\n",
      "✓ Properly configured CouplingFlow created\n",
      "  - Depth: 8 coupling layers\n",
      "  - Subnet: MLP with units [128, 128]\n",
      "  - Transform: affine\n",
      "  - Base distribution: normal\n",
      "✓ Adapter with transforms created\n",
      "✓ BayesFlow workflow created with proper configuration\n",
      "======================================================================\n",
      "🎉 PROPERLY CONFIGURED WORKFLOW CREATED!\n",
      "\n",
      "📋 Key corrections made:\n",
      "  ✅ Used correct CouplingFlow parameters (depth, subnet_kwargs)\n",
      "  ✅ Removed non-existent num_params parameter\n",
      "  ✅ BayesFlow will auto-determine dimensions from data\n",
      "  ✅ Specified inference_variables and summary_variables explicitly\n"
     ]
    }
   ],
   "source": [
    "# CORRECTED WORKFLOW WITH PROPER COUPLINGFLOW PARAMETERS\n",
    "\n",
    "def create_properly_configured_workflow():\n",
    "    \"\"\"\n",
    "    Create BayesFlow workflow with correctly configured CouplingFlow.\n",
    "    \"\"\"\n",
    "    print(\"Creating properly configured BayesFlow workflow...\")\n",
    "    \n",
    "    # 1. USE EXISTING SIMULATOR\n",
    "    simulator = hmm_simulator\n",
    "    print(\"✓ Using existing HMM simulator\")\n",
    "    \n",
    "    # 2. CUSTOM SUMMARY NETWORK\n",
    "    protein_summary_net = ProteinSummaryNetwork(\n",
    "        vocab_size=20,\n",
    "        embedding_dim=32,\n",
    "        lstm_units=64,\n",
    "        attention_dim=32,\n",
    "        summary_dim=64,\n",
    "        name='ProteinSummaryNetwork'\n",
    "    )\n",
    "    print(\"✓ Custom summary network created\")\n",
    "    \n",
    "    # 3. PROPERLY CONFIGURED INFERENCE NETWORK\n",
    "    inference_net = bf.networks.CouplingFlow(\n",
    "        subnet='mlp',           # Use MLP subnets\n",
    "        depth=8,               # Number of coupling layers\n",
    "        transform='affine',    # Affine coupling transforms  \n",
    "        permutation='random',  # Random permutations between layers\n",
    "        use_actnorm=True,      # Use activation normalization\n",
    "        base_distribution='normal',  # Normal base distribution\n",
    "        subnet_kwargs={        # Configuration for MLP subnets\n",
    "            'units': [128, 128],     # Hidden layer sizes\n",
    "            'activation': 'silu',    # Activation function\n",
    "            'dropout': 0.1           # Dropout rate\n",
    "        },\n",
    "        name='ProteinInferenceNetwork'\n",
    "    )\n",
    "    print(\"✓ Properly configured CouplingFlow created\")\n",
    "    print(f\"  - Depth: 8 coupling layers\")\n",
    "    print(f\"  - Subnet: MLP with units [128, 128]\")\n",
    "    print(f\"  - Transform: affine\")\n",
    "    print(f\"  - Base distribution: normal\")\n",
    "    \n",
    "    # 4. ADAPTER (same as before)\n",
    "    adapter_transforms = [\n",
    "        bf.adapters.transforms.Rename(from_key='amino_acids', to_key='summary_variables'),\n",
    "        bf.adapters.transforms.Rename(from_key='state_probs', to_key='inference_variables'),\n",
    "        bf.adapters.transforms.Drop(keys=['true_states']),\n",
    "        bf.adapters.transforms.MapTransform({\n",
    "            'summary_variables': bf.adapters.transforms.ConvertDType(\n",
    "                from_dtype='int64', to_dtype='float32'\n",
    "            ),\n",
    "            'inference_variables': bf.adapters.transforms.ConvertDType(\n",
    "                from_dtype='float64', to_dtype='float32'\n",
    "            ),\n",
    "        }),\n",
    "        bf.adapters.transforms.MapTransform({\n",
    "            'inference_variables': FlattenTransform(),\n",
    "        }),\n",
    "    ]\n",
    "    \n",
    "    adapter = bf.Adapter(transforms=adapter_transforms)\n",
    "    print(\"✓ Adapter with transforms created\")\n",
    "    \n",
    "    # 5. CREATE WORKFLOW WITH PROPER PARAMETERS\n",
    "    workflow = bf.BasicWorkflow(\n",
    "        simulator=simulator,\n",
    "        adapter=adapter,\n",
    "        inference_network=inference_net,\n",
    "        summary_network=protein_summary_net,\n",
    "        initial_learning_rate=0.001,  # Learning rate\n",
    "        inference_variables=['inference_variables'],  # Specify which variables to infer\n",
    "        summary_variables=['summary_variables']       # Specify summary variables\n",
    "    )\n",
    "    print(\"✓ BayesFlow workflow created with proper configuration\")\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "# Create the properly configured workflow\n",
    "print(\"=\" * 70)\n",
    "properly_configured_workflow = create_properly_configured_workflow()\n",
    "print(\"=\" * 70)\n",
    "print(\"🎉 PROPERLY CONFIGURED WORKFLOW CREATED!\")\n",
    "print(\"\\n📋 Key corrections made:\")\n",
    "print(\"  ✅ Used correct CouplingFlow parameters (depth, subnet_kwargs)\")\n",
    "print(\"  ✅ Removed non-existent num_params parameter\")\n",
    "print(\"  ✅ BayesFlow will auto-determine dimensions from data\")\n",
    "print(\"  ✅ Specified inference_variables and summary_variables explicitly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f28a171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bayesflow:Fitting on dataset instance of OnlineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "INFO:bayesflow:Building on a test batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 TESTING PROPERLY CONFIGURED WORKFLOW\n",
      "==================================================\n",
      "1. Testing simulation and adaptation...\n",
      "✓ Simulation and adaptation successful\n",
      "Raw simulation data shapes:\n",
      "  amino_acids: (2, 50)\n",
      "  true_states: (2, 50)\n",
      "  state_probs: (2, 50, 2)\n",
      "\n",
      "Adapted data shapes:\n",
      "  summary_variables: (2, 50)\n",
      "  inference_variables: (2, 100)\n",
      "\n",
      "2. Testing training with properly configured workflow...\n",
      "Starting training for 2 epochs with batch size 4\n",
      "============================================================\n",
      "Training configuration:\n",
      "  epochs: 2\n",
      "  batch_size: 4\n",
      "  validation_sims: 1000\n",
      "  checkpoint_interval: 1\n",
      "\n",
      "🚀 Starting online training...\n",
      "Epoch 1/2\n",
      "Epoch 1/2\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1042s\u001b[0m 10s/step - loss: 41.6534\n",
      "Epoch 2/2\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1042s\u001b[0m 10s/step - loss: 41.6534\n",
      "Epoch 2/2\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1009s\u001b[0m 10s/step - loss: -40.8455\n",
      "✅ Training completed successfully!\n",
      "✅ TRAINING TEST SUCCESSFUL!\n",
      "🎯 The workflow is now properly configured and ready for full training!\n",
      "\n",
      "==================================================\n",
      "📊 SUMMARY OF CORRECTIONS:\n",
      "==================================================\n",
      "❌ BEFORE (incorrect parameters):\n",
      "  - num_params=100\n",
      "  - num_coupling_layers=8\n",
      "  - coupling_settings={'units': [128, 128], 'activation': 'silu'}\n",
      "\n",
      "✅ AFTER (correct parameters):\n",
      "  - depth=8  # Number of coupling layers\n",
      "  - subnet='mlp'  # Type of subnet\n",
      "  - subnet_kwargs={'units': [128, 128], 'activation': 'silu'}\n",
      "  - transform='affine'  # Coupling transform type\n",
      "  - BayesFlow auto-determines parameter dimensions from data!\n",
      "\n",
      "🔑 KEY INSIGHT: BayesFlow CouplingFlow doesn't need explicit parameter\n",
      "   dimensions - it builds them automatically based on the actual data shapes!\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1009s\u001b[0m 10s/step - loss: -40.8455\n",
      "✅ Training completed successfully!\n",
      "✅ TRAINING TEST SUCCESSFUL!\n",
      "🎯 The workflow is now properly configured and ready for full training!\n",
      "\n",
      "==================================================\n",
      "📊 SUMMARY OF CORRECTIONS:\n",
      "==================================================\n",
      "❌ BEFORE (incorrect parameters):\n",
      "  - num_params=100\n",
      "  - num_coupling_layers=8\n",
      "  - coupling_settings={'units': [128, 128], 'activation': 'silu'}\n",
      "\n",
      "✅ AFTER (correct parameters):\n",
      "  - depth=8  # Number of coupling layers\n",
      "  - subnet='mlp'  # Type of subnet\n",
      "  - subnet_kwargs={'units': [128, 128], 'activation': 'silu'}\n",
      "  - transform='affine'  # Coupling transform type\n",
      "  - BayesFlow auto-determines parameter dimensions from data!\n",
      "\n",
      "🔑 KEY INSIGHT: BayesFlow CouplingFlow doesn't need explicit parameter\n",
      "   dimensions - it builds them automatically based on the actual data shapes!\n"
     ]
    }
   ],
   "source": [
    "# TESTING THE PROPERLY CONFIGURED WORKFLOW\n",
    "\n",
    "print(\"🧪 TESTING PROPERLY CONFIGURED WORKFLOW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test simulation and adaptation\n",
    "print(\"1. Testing simulation and adaptation...\")\n",
    "test_sim_data = properly_configured_workflow.simulate(2)\n",
    "test_adapted_data = properly_configured_workflow.adapter(test_sim_data)\n",
    "\n",
    "print(\"✓ Simulation and adaptation successful\")\n",
    "print(\"Raw simulation data shapes:\")\n",
    "for key, value in test_sim_data.items():\n",
    "    print(f\"  {key}: {value.shape}\")\n",
    "\n",
    "print(\"\\nAdapted data shapes:\")\n",
    "for key, value in test_adapted_data.items():\n",
    "    print(f\"  {key}: {value.shape}\")\n",
    "\n",
    "# Quick training test\n",
    "print(\"\\n2. Testing training with properly configured workflow...\")\n",
    "try:\n",
    "    quick_test_history = train_protein_workflow(\n",
    "        workflow=properly_configured_workflow,\n",
    "        batch_size=4,\n",
    "        epochs=2,  # Just 2 epochs for quick test\n",
    "        print_every=1\n",
    "    )\n",
    "    print(\"✅ TRAINING TEST SUCCESSFUL!\")\n",
    "    print(\"🎯 The workflow is now properly configured and ready for full training!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Training test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"📊 SUMMARY OF CORRECTIONS:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"❌ BEFORE (incorrect parameters):\")\n",
    "print(\"  - num_params=100\")\n",
    "print(\"  - num_coupling_layers=8\") \n",
    "print(\"  - coupling_settings={'units': [128, 128], 'activation': 'silu'}\")\n",
    "print()\n",
    "print(\"✅ AFTER (correct parameters):\")\n",
    "print(\"  - depth=8  # Number of coupling layers\")\n",
    "print(\"  - subnet='mlp'  # Type of subnet\")\n",
    "print(\"  - subnet_kwargs={'units': [128, 128], 'activation': 'silu'}\")\n",
    "print(\"  - transform='affine'  # Coupling transform type\")\n",
    "print(\"  - BayesFlow auto-determines parameter dimensions from data!\")\n",
    "print()\n",
    "print(\"🔑 KEY INSIGHT: BayesFlow CouplingFlow doesn't need explicit parameter\")\n",
    "print(\"   dimensions - it builds them automatically based on the actual data shapes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdee00c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae2e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2773ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f4783e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
