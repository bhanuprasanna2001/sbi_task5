{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b338777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 12:48:19.733365: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-07-13 12:48:19.733402: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-07-13 12:48:19.733408: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752403699.733424 6220395 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1752403699.733443 6220395 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "INFO:bayesflow:Using backend 'tensorflow'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.keras is using the 'tensorflow' backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import bayesflow as bf\n",
    "\n",
    "from hmmlearn import hmm\n",
    "from hmmlearn.hmm import CategoricalHMM\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "current_backend = tf.keras.backend.backend()\n",
    "print(f\"tf.keras is using the '{current_backend}' backend.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb96b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETER VALIDATION:\n",
      "Amino acids: 20 types\n",
      "Alpha emission sum: 1.000\n",
      "Other emission sum: 1.000\n",
      "Alpha transitions sum: 1.000\n",
      "Other transitions sum: 1.000\n",
      "Initial probs sum: 1.000\n",
      "\n",
      "‚úì All probabilities are valid!\n"
     ]
    }
   ],
   "source": [
    "# HMM PARAMETERS FROM TASK DESCRIPTION\n",
    "\n",
    "# 20 amino acids in standard order\n",
    "AMINO_ACIDS = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I', \n",
    "               'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "\n",
    "# Emission probabilities from task tables\n",
    "# Alpha-helix state (state 0)\n",
    "EMISSION_ALPHA = [0.12, 0.06, 0.03, 0.05, 0.01, 0.09, 0.05, 0.04, 0.02, 0.07,\n",
    "                  0.12, 0.06, 0.03, 0.04, 0.02, 0.05, 0.04, 0.01, 0.03, 0.06]\n",
    "\n",
    "# Other state (state 1) \n",
    "EMISSION_OTHER = [0.06, 0.05, 0.05, 0.06, 0.02, 0.05, 0.03, 0.09, 0.03, 0.05,\n",
    "                  0.08, 0.06, 0.02, 0.04, 0.06, 0.07, 0.06, 0.01, 0.04, 0.07]\n",
    "\n",
    "# Transition probabilities from task description\n",
    "# [alpha->alpha, alpha->other]\n",
    "TRANS_FROM_ALPHA = [0.90, 0.10]\n",
    "# [other->alpha, other->other]  \n",
    "TRANS_FROM_OTHER = [0.05, 0.95]\n",
    "\n",
    "# Initial state probabilities (always starts in \"other\" state)\n",
    "INITIAL_PROBS = [0.0, 1.0]  # [alpha-helix, other]\n",
    "\n",
    "# Validation\n",
    "print(\"PARAMETER VALIDATION:\")\n",
    "print(f\"Amino acids: {len(AMINO_ACIDS)} types\")\n",
    "print(f\"Alpha emission sum: {sum(EMISSION_ALPHA):.3f}\")\n",
    "print(f\"Other emission sum: {sum(EMISSION_OTHER):.3f}\")\n",
    "print(f\"Alpha transitions sum: {sum(TRANS_FROM_ALPHA):.3f}\")\n",
    "print(f\"Other transitions sum: {sum(TRANS_FROM_OTHER):.3f}\")\n",
    "print(f\"Initial probs sum: {sum(INITIAL_PROBS):.3f}\")\n",
    "print(\"\\n‚úì All probabilities are valid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f45f77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING HMM CREATION:\n",
      "\n",
      "States: 2\n",
      "Features: 20\n",
      "Start probabilities: [0. 1.]\n",
      "Transition matrix shape: (2, 2)\n",
      "Emission matrix shape: (2, 20)\n",
      "\n",
      "Transition probabilities:\n",
      "From alpha-helix: [0.9 0.1]\n",
      "From other:      [0.05 0.95]\n",
      "\n",
      "Emission probabilities (first 5 amino acids):\n",
      "Alpha-helix: [0.12 0.06 0.03 0.05 0.01]\n",
      "Other:       [0.06 0.05 0.05 0.06 0.02]\n",
      "\n",
      "‚úì HMM model created successfully!\n"
     ]
    }
   ],
   "source": [
    "# FIXED HMM MODEL CREATION\n",
    "\n",
    "def create_fixed_hmm():\n",
    "    \"\"\"\n",
    "    Create HMM with fixed parameters from task description.\n",
    "    \n",
    "    States: 0=alpha-helix, 1=other\n",
    "    Features: 20 amino acids (0-19 indices)\n",
    "    \n",
    "    Returns:\n",
    "        CategoricalHMM with fixed empirical parameters\n",
    "    \"\"\"\n",
    "    # Create model with fixed parameters (no learning)\n",
    "    model = hmm.CategoricalHMM(\n",
    "        n_components=2,        # 2 states: alpha-helix, other\n",
    "        n_features=20,         # 20 amino acids\n",
    "        params=\"\",             # Don't update any parameters\n",
    "        init_params=\"\",        # Don't initialize any parameters\n",
    "        algorithm=\"viterbi\",   # Use Viterbi algorithm for decoding\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Set fixed parameters from task description\n",
    "    model.startprob_ = np.array(INITIAL_PROBS)\n",
    "    model.transmat_ = np.array([TRANS_FROM_ALPHA, TRANS_FROM_OTHER])\n",
    "    model.emissionprob_ = np.array([EMISSION_ALPHA, EMISSION_OTHER])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test HMM creation\n",
    "print(\"TESTING HMM CREATION:\\n\")\n",
    "hmm_model = create_fixed_hmm()\n",
    "\n",
    "print(f\"States: {hmm_model.n_components}\")\n",
    "print(f\"Features: {hmm_model.n_features}\")\n",
    "print(f\"Start probabilities: {hmm_model.startprob_}\")\n",
    "print(f\"Transition matrix shape: {hmm_model.transmat_.shape}\")\n",
    "print(f\"Emission matrix shape: {hmm_model.emissionprob_.shape}\")\n",
    "\n",
    "print(\"\\nTransition probabilities:\")\n",
    "print(\"From alpha-helix:\", hmm_model.transmat_[0])\n",
    "print(\"From other:     \", hmm_model.transmat_[1])\n",
    "\n",
    "print(\"\\nEmission probabilities (first 5 amino acids):\")\n",
    "print(\"Alpha-helix:\", hmm_model.emissionprob_[0][:5])\n",
    "print(\"Other:      \", hmm_model.emissionprob_[1][:5])\n",
    "print(\"\\n‚úì HMM model created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8ea2b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING HMM DATA GENERATION:\n",
      "\n",
      "Amino acids shape: (20,)\n",
      "True states shape: (20,)\n",
      "State probabilities shape: (20, 2)\n",
      "\n",
      "First 10 amino acids (indices): [19 11  2 16 14 19  3  2  9  5]\n",
      "First 10 true states: [1 1 1 1 1 0 0 0 0 0]\n",
      "First 5 state probabilities:\n",
      "[[0.         1.        ]\n",
      " [0.01768884 0.98231116]\n",
      " [0.0253218  0.9746782 ]\n",
      " [0.03656372 0.96343628]\n",
      " [0.05153765 0.94846235]]\n",
      "\n",
      "State probabilities sum check: True\n",
      "First 10 amino acids (letters): ['V', 'K', 'N', 'T', 'P', 'V', 'D', 'N', 'I', 'E']\n",
      "\n",
      "‚úì HMM data generation working correctly!\n"
     ]
    }
   ],
   "source": [
    "# HMM DATA GENERATION AND SIMULATOR FUNCTIONS\n",
    "\n",
    "def generate_amino_acid_sequence(n_samples=50, random_state=None):\n",
    "    \"\"\"\n",
    "    Generate amino acid sequences from the fixed HMM.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of amino acids to generate\n",
    "        random_state: Random state for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        dict with 'amino_acids', 'true_states', and 'state_probs'\n",
    "    \"\"\"\n",
    "    # Create the fixed HMM model\n",
    "    model = create_fixed_hmm()\n",
    "    \n",
    "    # Generate sequence from HMM\n",
    "    X, Z = model.sample(n_samples, random_state=random_state)\n",
    "    \n",
    "    # X is shape (n_samples, 1) - amino acid indices\n",
    "    # Z is shape (n_samples,) - true hidden states\n",
    "    amino_acids = X.flatten()  # Convert to 1D array of amino acid indices\n",
    "    \n",
    "    # Get state membership probabilities using Forward-Backward algorithm\n",
    "    # Need to reshape X for predict_proba (expects (n_samples, 1))\n",
    "    state_probs = model.predict_proba(X)  # Shape: (n_samples, n_states)\n",
    "    \n",
    "    return {\n",
    "        'amino_acids': amino_acids,       # Shape: (n_samples,) - amino acid indices (0-19)\n",
    "        'true_states': Z,                 # Shape: (n_samples,) - true hidden states (0=alpha, 1=other) \n",
    "        'state_probs': state_probs        # Shape: (n_samples, 2) - state membership probabilities\n",
    "    }\n",
    "\n",
    "# Test the data generation\n",
    "print(\"TESTING HMM DATA GENERATION:\\n\")\n",
    "test_data = generate_amino_acid_sequence(n_samples=20, random_state=42)\n",
    "\n",
    "print(f\"Amino acids shape: {test_data['amino_acids'].shape}\")\n",
    "print(f\"True states shape: {test_data['true_states'].shape}\")\n",
    "print(f\"State probabilities shape: {test_data['state_probs'].shape}\")\n",
    "\n",
    "print(f\"\\nFirst 10 amino acids (indices): {test_data['amino_acids'][:10]}\")\n",
    "print(f\"First 10 true states: {test_data['true_states'][:10]}\")\n",
    "print(f\"First 5 state probabilities:\\n{test_data['state_probs'][:5]}\")\n",
    "\n",
    "# Verify state probabilities sum to 1\n",
    "print(f\"\\nState probabilities sum check: {np.allclose(test_data['state_probs'].sum(axis=1), 1.0)}\")\n",
    "\n",
    "# Convert amino acid indices to actual amino acid letters for readability\n",
    "amino_acid_letters = [AMINO_ACIDS[idx] for idx in test_data['amino_acids'][:10]]\n",
    "print(f\"First 10 amino acids (letters): {amino_acid_letters}\")\n",
    "print(\"\\n‚úì HMM data generation working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a4dd8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING BAYESFLOW SIMULATOR:\n",
      "\n",
      "‚úì BayesFlow LambdaSimulator created successfully!\n",
      "\n",
      "TESTING BAYESFLOW SIMULATOR:\n",
      "Simulation data keys: ['amino_acids', 'true_states', 'state_probs']\n",
      "Amino acids batch shape: (3, 15)\n",
      "True states batch shape: (3, 15)\n",
      "State probabilities batch shape: (3, 15, 2)\n",
      "\n",
      "First 2 sequences:\n",
      "\n",
      "Sequence 0:\n",
      "Amino acids: [16  1 10 13 15  0 10  2 19  6 16  0 19 19  4]\n",
      "True states: [1 1 1 1 1 1 0 0 0 0 0 0 1 1 1]\n",
      "State probabilities shape: (15, 2)\n",
      "State probabilities sum check: True\n",
      "Sequnce length: 15\n",
      "\n",
      "Sequence 1:\n",
      "Amino acids: [10  2  7 10 14 17 19 12 14 18 15 19  9  9 14]\n",
      "True states: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "State probabilities shape: (15, 2)\n",
      "State probabilities sum check: True\n",
      "Sequnce length: 15\n",
      "Amino acid letters: ['T', 'R', 'L', 'F', 'S', 'A', 'L', 'N', 'V', 'Q', 'T', 'A', 'V', 'V', 'C']\n",
      "\n",
      "‚úì BayesFlow simulator working correctly!\n"
     ]
    }
   ],
   "source": [
    "# BAYESFLOW SIMULATOR IMPLEMENTATION\n",
    "\n",
    "def hmm_simulator_function(batch_shape, sequence_length=50, **kwargs):\n",
    "    \"\"\"\n",
    "    Simulator function for BayesFlow that generates HMM data.\n",
    "    \n",
    "    This function will be wrapped by BayesFlow's LambdaSimulator.\n",
    "    \n",
    "    Args:\n",
    "        batch_shape: Shape of the batch to generate (from BayesFlow)\n",
    "        sequence_length: Length of amino acid sequences to generate\n",
    "        **kwargs: Additional keyword arguments\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with simulation outputs for BayesFlow\n",
    "    \"\"\"\n",
    "    # Handle both int and tuple batch_shape\n",
    "    if isinstance(batch_shape, int):\n",
    "        batch_size = batch_shape\n",
    "    else:\n",
    "        batch_size = batch_shape[0] if len(batch_shape) > 0 else 1\n",
    "    \n",
    "    # Generate multiple sequences\n",
    "    amino_acids_batch = []\n",
    "    true_states_batch = []\n",
    "    state_probs_batch = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Generate one sequence with different random state for each\n",
    "        data = generate_amino_acid_sequence(\n",
    "            n_samples=sequence_length, \n",
    "            random_state=np.random.randint(0, 10000)\n",
    "        )\n",
    "        \n",
    "        amino_acids_batch.append(data['amino_acids'])\n",
    "        true_states_batch.append(data['true_states'])\n",
    "        state_probs_batch.append(data['state_probs'])\n",
    "    \n",
    "    # Stack into batch format\n",
    "    return {\n",
    "        'amino_acids': np.array(amino_acids_batch),      # Shape: (batch_size, sequence_length)\n",
    "        'true_states': np.array(true_states_batch),      # Shape: (batch_size, sequence_length)\n",
    "        'state_probs': np.array(state_probs_batch),      # Shape: (batch_size, sequence_length, 2)\n",
    "    }\n",
    "\n",
    "# Create BayesFlow simulator\n",
    "print(\"CREATING BAYESFLOW SIMULATOR:\\n\")\n",
    "hmm_simulator = bf.simulators.LambdaSimulator(\n",
    "    sample_fn=hmm_simulator_function,\n",
    "    is_batched=True  # Our function handles batching internally\n",
    ")\n",
    "\n",
    "print(\"‚úì BayesFlow LambdaSimulator created successfully!\")\n",
    "\n",
    "# Test the BayesFlow simulator\n",
    "print(\"\\nTESTING BAYESFLOW SIMULATOR:\")\n",
    "batch_size = 3\n",
    "sequence_length = 15\n",
    "\n",
    "# Sample from the simulator\n",
    "simulation_data = hmm_simulator.sample(\n",
    "    batch_shape=(batch_size,), \n",
    "    sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "print(f\"Simulation data keys: {list(simulation_data.keys())}\")\n",
    "print(f\"Amino acids batch shape: {simulation_data['amino_acids'].shape}\")\n",
    "print(f\"True states batch shape: {simulation_data['true_states'].shape}\")\n",
    "print(f\"State probabilities batch shape: {simulation_data['state_probs'].shape}\")\n",
    "\n",
    "# Show multiple sequences\n",
    "num_seq = 2\n",
    "print(f\"\\nFirst {num_seq} sequences:\")\n",
    "for i in range(num_seq):\n",
    "    amino_acids = simulation_data['amino_acids'][i]\n",
    "    true_states = simulation_data['true_states'][i]\n",
    "    state_probs = simulation_data['state_probs'][i]\n",
    "    \n",
    "    print(f\"\\nSequence {i}:\")\n",
    "    print(f\"Amino acids: {amino_acids}\")\n",
    "    print(f\"True states: {true_states}\")\n",
    "    print(f\"State probabilities shape: {state_probs.shape}\")\n",
    "    print(f\"State probabilities sum check: {np.allclose(state_probs.sum(axis=1), 1.0)}\")\n",
    "    print(f\"Sequnce length: {len(amino_acids)}\")\n",
    "\n",
    "# Convert first sequence to amino acid letters\n",
    "example_letters = [AMINO_ACIDS[idx] for idx in simulation_data['amino_acids'][0]]\n",
    "print(f\"Amino acid letters: {example_letters}\")\n",
    "\n",
    "print(\"\\n‚úì BayesFlow simulator working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41678b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETTING UP BAYESFLOW WORKFLOW COMPONENTS:\n",
      "Inference variables: ['state_probs']\n",
      "Summary variables: ['amino_acids']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Forward transformation must be a NumPy Universal Function (ufunc).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSummary variables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSUMMARY_VARIABLES\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Create the adapter for data processing\u001b[39;00m\n\u001b[32m     12\u001b[39m adapter = bf.adapters.Adapter([\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# First, convert specific arrays to float32\u001b[39;00m\n\u001b[32m     14\u001b[39m     bf.adapters.transforms.MapTransform({\n\u001b[32m     15\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mstate_probs\u001b[39m\u001b[33m'\u001b[39m: bf.adapters.transforms.ConvertDType(from_dtype=\u001b[33m'\u001b[39m\u001b[33mfloat64\u001b[39m\u001b[33m'\u001b[39m, to_dtype=\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     16\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mamino_acids\u001b[39m\u001b[33m'\u001b[39m: bf.adapters.transforms.ConvertDType(from_dtype=\u001b[33m'\u001b[39m\u001b[33mint64\u001b[39m\u001b[33m'\u001b[39m, to_dtype=\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     17\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtrue_states\u001b[39m\u001b[33m'\u001b[39m: bf.adapters.transforms.ConvertDType(from_dtype=\u001b[33m'\u001b[39m\u001b[33mint64\u001b[39m\u001b[33m'\u001b[39m, to_dtype=\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     18\u001b[39m     }),\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Flatten amino acids to 1D for MLP processing: (batch_size, seq_len) -> (batch_size, seq_len*1)\u001b[39;00m\n\u001b[32m     21\u001b[39m     bf.adapters.transforms.MapTransform({\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mamino_acids\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mbf\u001b[49m\u001b[43m.\u001b[49m\u001b[43madapters\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNumpyTransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     23\u001b[39m     }),\n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Flatten state probabilities for inference: (batch_size, seq_len, 2) -> (batch_size, seq_len*2)  \u001b[39;00m\n\u001b[32m     26\u001b[39m     bf.adapters.transforms.MapTransform({\n\u001b[32m     27\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mstate_probs\u001b[39m\u001b[33m'\u001b[39m: bf.adapters.transforms.NumpyTransform(\u001b[38;5;28;01mlambda\u001b[39;00m x: x.reshape(x.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)),\n\u001b[32m     28\u001b[39m     }),\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# Rename variables to match BayesFlow conventions\u001b[39;00m\n\u001b[32m     31\u001b[39m     bf.adapters.transforms.Rename(from_key=\u001b[33m'\u001b[39m\u001b[33mstate_probs\u001b[39m\u001b[33m'\u001b[39m, to_key=\u001b[33m'\u001b[39m\u001b[33minference_variables\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     32\u001b[39m     bf.adapters.transforms.Rename(from_key=\u001b[33m'\u001b[39m\u001b[33mamino_acids\u001b[39m\u001b[33m'\u001b[39m, to_key=\u001b[33m'\u001b[39m\u001b[33msummary_variables\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     33\u001b[39m \n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Drop true_states as we don't need it for training\u001b[39;00m\n\u001b[32m     35\u001b[39m     bf.adapters.transforms.Drop(keys=[\u001b[33m'\u001b[39m\u001b[33mtrue_states\u001b[39m\u001b[33m'\u001b[39m]),\n\u001b[32m     36\u001b[39m ])\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úì Adapter created with data transforms\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Create summary network for processing amino acid sequences\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Using MLP instead of DeepSet for simpler shape handling\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/adapters/transforms/numpy_transform.py:50\u001b[39m, in \u001b[36mNumpyTransform.__init__\u001b[39m\u001b[34m(self, forward, inverse)\u001b[39m\n\u001b[32m     47\u001b[39m     forward = \u001b[38;5;28mgetattr\u001b[39m(np, forward)\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(forward, np.ufunc):\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mForward transformation must be a NumPy Universal Function (ufunc).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inverse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m forward \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.INVERSE_METHODS:\n",
      "\u001b[31mValueError\u001b[39m: Forward transformation must be a NumPy Universal Function (ufunc)."
     ]
    }
   ],
   "source": [
    "# BAYESFLOW WORKFLOW SETUP\n",
    "\n",
    "# Define the data variables for the adapter\n",
    "INFERENCE_VARIABLES = ['state_probs']  # What we want to infer (state membership probabilities)\n",
    "SUMMARY_VARIABLES = ['amino_acids']     # What the summary network processes (amino acid sequences)\n",
    "\n",
    "print(\"SETTING UP BAYESFLOW WORKFLOW COMPONENTS:\")\n",
    "print(f\"Inference variables: {INFERENCE_VARIABLES}\")\n",
    "print(f\"Summary variables: {SUMMARY_VARIABLES}\")\n",
    "\n",
    "# Create the adapter for data processing\n",
    "adapter = bf.adapters.Adapter([\n",
    "    # First, convert specific arrays to float32\n",
    "    bf.adapters.transforms.MapTransform({\n",
    "        'state_probs': bf.adapters.transforms.ConvertDType(from_dtype='float64', to_dtype='float32'),\n",
    "        'amino_acids': bf.adapters.transforms.ConvertDType(from_dtype='int64', to_dtype='float32'),\n",
    "        'true_states': bf.adapters.transforms.ConvertDType(from_dtype='int64', to_dtype='float32'),\n",
    "    }),\n",
    "    \n",
    "    # Flatten amino acids to 1D for MLP processing: (batch_size, seq_len) -> (batch_size, seq_len*1)\n",
    "    bf.adapters.transforms.MapTransform({\n",
    "        'amino_acids': bf.adapters.transforms.NumpyTransform(lambda x: x.reshape(x.shape[0], -1)),\n",
    "    }),\n",
    "    \n",
    "    # Flatten state probabilities for inference: (batch_size, seq_len, 2) -> (batch_size, seq_len*2)  \n",
    "    bf.adapters.transforms.MapTransform({\n",
    "        'state_probs': bf.adapters.transforms.NumpyTransform(lambda x: x.reshape(x.shape[0], -1)),\n",
    "    }),\n",
    "    \n",
    "    # Rename variables to match BayesFlow conventions\n",
    "    bf.adapters.transforms.Rename(from_key='state_probs', to_key='inference_variables'),\n",
    "    bf.adapters.transforms.Rename(from_key='amino_acids', to_key='summary_variables'),\n",
    "    \n",
    "    # Drop true_states as we don't need it for training\n",
    "    bf.adapters.transforms.Drop(keys=['true_states']),\n",
    "])\n",
    "\n",
    "print(\"‚úì Adapter created with data transforms\")\n",
    "\n",
    "# Create summary network for processing amino acid sequences\n",
    "# Using MLP instead of DeepSet for simpler shape handling\n",
    "summary_network = bf.networks.MLP(\n",
    "    widths=[128, 128, 64],       # Hidden layer widths\n",
    "    activation='silu',           # Activation function\n",
    "    dropout=0.1,                 # Dropout rate\n",
    "    kernel_initializer='he_normal'\n",
    ")\n",
    "\n",
    "print(\"‚úì Summary network (DeepSet) created\")\n",
    "\n",
    "# Create inference network for posterior approximation\n",
    "# CouplingFlow is excellent for complex posterior distributions\n",
    "inference_network = bf.networks.CouplingFlow(\n",
    "    depth=8,                     # Number of coupling layers\n",
    "    transform='affine',          # Type of coupling transformation\n",
    "    permutation='random',        # Permutation between layers\n",
    "    use_actnorm=True,           # Use activation normalization\n",
    "    base_distribution='normal',  # Base distribution \n",
    "    subnet='mlp',               # Subnet architecture\n",
    "    subnet_kwargs={             # Subnet configuration\n",
    "        'widths': [128, 128, 128],   # Hidden layer widths\n",
    "        'activation': 'silu',        # Activation function\n",
    "        'dropout': 0.1               # Dropout rate\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úì Inference network (CouplingFlow) created\")\n",
    "\n",
    "# Create the BasicWorkflow\n",
    "workflow = bf.workflows.BasicWorkflow(\n",
    "    simulator=hmm_simulator,\n",
    "    adapter=adapter,\n",
    "    inference_network=inference_network,\n",
    "    summary_network=summary_network,\n",
    "    initial_learning_rate=5e-4,\n",
    "    inference_variables='inference_variables',\n",
    "    summary_variables='summary_variables'\n",
    ")\n",
    "\n",
    "print(\"‚úì BasicWorkflow created successfully!\")\n",
    "\n",
    "# Test the complete workflow with a small sample\n",
    "print(\"\\nTESTING COMPLETE WORKFLOW:\")\n",
    "test_batch_size = 2\n",
    "test_sequence_length = 20\n",
    "\n",
    "# Generate test data using the workflow\n",
    "test_simulation = workflow.simulate(\n",
    "    batch_shape=(test_batch_size,),\n",
    "    sequence_length=test_sequence_length\n",
    ")\n",
    "\n",
    "print(f\"Simulated data keys: {list(test_simulation.keys())}\")\n",
    "for key, value in test_simulation.items():\n",
    "    print(f\"  {key}: shape {value.shape}, dtype {value.dtype}\")\n",
    "\n",
    "# Apply adapter to the simulated data\n",
    "adapted_data = workflow.adapter(test_simulation)\n",
    "print(f\"\\nAdapted data keys: {list(adapted_data.keys())}\")\n",
    "for key, value in adapted_data.items():\n",
    "    print(f\"  {key}: shape {value.shape}, dtype {value.dtype}\")\n",
    "\n",
    "print(\"\\n‚úì Complete workflow pipeline tested successfully!\")\n",
    "print(\"Ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a13d546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Custom ProteinSummaryNetwork class defined\n"
     ]
    }
   ],
   "source": [
    "# CUSTOM PROTEIN SUMMARY NETWORK\n",
    "\n",
    "class ProteinSummaryNetwork(bf.networks.SummaryNetwork):\n",
    "    \"\"\"\n",
    "    Custom summary network for protein amino acid sequences.\n",
    "    \n",
    "    This network is specifically designed for the protein secondary structure task:\n",
    "    - Embeds amino acid indices into dense representations\n",
    "    - Uses bidirectional LSTM to capture sequential dependencies\n",
    "    - Applies attention mechanism to focus on important positions\n",
    "    - Outputs summary statistics for the entire sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size=20,              # Number of amino acids\n",
    "                 embedding_dim=32,           # Amino acid embedding dimension\n",
    "                 lstm_units=64,              # LSTM hidden units\n",
    "                 attention_dim=32,           # Attention mechanism dimension\n",
    "                 summary_dim=64,             # Output summary dimension\n",
    "                 dropout_rate=0.1,           # Dropout rate\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.attention_dim = attention_dim\n",
    "        self.summary_dim = summary_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Amino acid embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            mask_zero=False,  # Don't mask zero values as amino acid 'A' has index 0\n",
    "            name='amino_acid_embedding'\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM for sequence processing\n",
    "        self.lstm = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(\n",
    "                lstm_units,\n",
    "                return_sequences=True,  # Return full sequence for attention\n",
    "                dropout=dropout_rate,\n",
    "                recurrent_dropout=dropout_rate,\n",
    "                name='sequence_lstm'\n",
    "            ),\n",
    "            name='bidirectional_lstm'\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism layers\n",
    "        self.attention_dense = tf.keras.layers.Dense(\n",
    "            attention_dim, \n",
    "            activation='tanh',\n",
    "            name='attention_dense'\n",
    "        )\n",
    "        self.attention_weights = tf.keras.layers.Dense(\n",
    "            1, \n",
    "            activation=None,  # Don't use softmax here, apply it later\n",
    "            name='attention_weights'\n",
    "        )\n",
    "        \n",
    "        # Final summary layers\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.summary_dense1 = tf.keras.layers.Dense(\n",
    "            summary_dim * 2,\n",
    "            activation='silu',\n",
    "            name='summary_dense1'\n",
    "        )\n",
    "        self.summary_dense2 = tf.keras.layers.Dense(\n",
    "            summary_dim,\n",
    "            activation='silu', \n",
    "            name='summary_dense2'\n",
    "        )\n",
    "        \n",
    "    def call(self, x, training=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass of the protein summary network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, sequence_length, 1) containing amino acid indices\n",
    "            training: Whether in training mode\n",
    "            \n",
    "        Returns:\n",
    "            Summary tensor of shape (batch_size, summary_dim)\n",
    "        \"\"\"\n",
    "        # Remove the last dimension if present: (batch_size, seq_len, 1) -> (batch_size, seq_len)\n",
    "        if x.shape[-1] == 1:\n",
    "            x = tf.squeeze(x, axis=-1)\n",
    "            \n",
    "        # Convert to integer indices for embedding\n",
    "        x = tf.cast(x, tf.int32)\n",
    "        \n",
    "        # Embed amino acid indices: (batch_size, seq_len) -> (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Process with bidirectional LSTM: (batch_size, seq_len, embedding_dim) -> (batch_size, seq_len, 2*lstm_units)\n",
    "        lstm_output = self.lstm(embedded, training=training)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        # Compute attention scores: (batch_size, seq_len, 2*lstm_units) -> (batch_size, seq_len, attention_dim)\n",
    "        attention_scores = self.attention_dense(lstm_output)\n",
    "        \n",
    "        # Compute attention weights: (batch_size, seq_len, attention_dim) -> (batch_size, seq_len, 1)\n",
    "        attention_logits = self.attention_weights(attention_scores)\n",
    "        \n",
    "        # Apply softmax along the sequence dimension to get proper attention weights\n",
    "        attention_weights = tf.nn.softmax(attention_logits, axis=1)  # Softmax over sequence dimension\n",
    "        \n",
    "        # Apply attention: weighted sum of LSTM outputs\n",
    "        # (batch_size, seq_len, 2*lstm_units) * (batch_size, seq_len, 1) -> (batch_size, 2*lstm_units)\n",
    "        attended_output = tf.reduce_sum(lstm_output * attention_weights, axis=1)\n",
    "        \n",
    "        # Apply dropout\n",
    "        attended_output = self.dropout(attended_output, training=training)\n",
    "        \n",
    "        # Generate final summary through dense layers\n",
    "        summary = self.summary_dense1(attended_output)\n",
    "        summary = self.dropout(summary, training=training)\n",
    "        summary = self.summary_dense2(summary)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"Return the configuration of the layer.\"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'lstm_units': self.lstm_units,\n",
    "            'attention_dim': self.attention_dim,\n",
    "            'summary_dim': self.summary_dim,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"Create layer from configuration.\"\"\"\n",
    "        return cls(**config)\n",
    "\n",
    "print(\"‚úì Custom ProteinSummaryNetwork class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d6903af",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2365904385.py, line 62)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mworkflow = bf.BasicWorkflow(\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# UPDATED BAYESFLOW WORKFLOW WITH CUSTOM SUMMARY NETWORK\n",
    "\n",
    "def create_protein_bayesflow_workflow(\n",
    "    param_dim=4,           # Transition probabilities (2x2)\n",
    "    seq_len=20,           # Sequence length\n",
    "    vocab_size=20,        # Number of amino acids\n",
    "    embedding_dim=32,     # Amino acid embedding dimension\n",
    "    lstm_units=64,        # LSTM hidden units\n",
    "    attention_dim=32,     # Attention dimension\n",
    "    summary_dim=64,       # Summary network output dimension\n",
    "    coupling_layers=8,    # Number of coupling layers\n",
    "    hidden_units=[128, 128],  # Hidden units for coupling networks\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Create BayesFlow workflow with custom protein summary network.\n",
    "    \n",
    "    Returns:\n",
    "        Configured BasicWorkflow ready for training\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. USE EXISTING SIMULATOR\n",
    "    # The hmm_simulator is already created and available\n",
    "    simulator = hmm_simulator\n",
    "    \n",
    "    # 2. CUSTOM SUMMARY NETWORK\n",
    "    protein_summary_net = ProteinSummaryNetwork(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        lstm_units=lstm_units,\n",
    "        attention_dim=attention_dim,\n",
    "        summary_dim=summary_dim,\n",
    "        name='ProteinSummaryNetwork'\n",
    "    )\n",
    "    \n",
    "    # 3. INFERENCE NETWORK (unchanged)\n",
    "    inference_net = bf.networks.CouplingFlow(\n",
    "        num_params=param_dim,\n",
    "        num_coupling_layers=coupling_layers,\n",
    "        coupling_settings={'units': hidden_units, 'activation': 'silu'},\n",
    "        name='ProteinInferenceNetwork'\n",
    "    )\n",
    "    \n",
    "    # 4. PROPER ADAPTER WITH TRANSFORMS\n",
    "    # Create adapter that handles the HMM simulation data properly\n",
    "    adapter_transforms = [\n",
    "        # Rename the data to match BayesFlow conventions\n",
    "        bf.adapters.transforms.Rename(from_key='amino_acids', to_key='summary_variables'),\n",
    "        bf.adapters.transforms.Rename(from_key='state_probs', to_key='inference_variables'),\n",
    "        \n",
    "        # Drop true_states as we don't need it for inference\n",
    "        bf.adapters.transforms.Drop(keys=['true_states']),\n",
    "        \n",
    "        # Convert dtypes to float32 for neural networks using MapTransform\n",
    "        bf.adapters.transforms.MapTransform({\n",
    "            'summary_variables': bf.adapters.transforms.ConvertDType(from_dtype='int64', to_dtype='float32'),\n",
    "            'inference_variables': bf.adapters.transforms.ConvertDType(from_dtype='float64', to_dtype='float32'),\n",
    "        }),\n",
    "    ]\n",
    "    \n",
    "    custom_adapter = bf.Adapter(transforms=adapter_transforms)\n",
    "        workflow = bf.BasicWorkflow(\n",
    "    # 5. CREATE WORKFLOW\n",
    "        adapter=custom_adapter,\n",
    "        simulator=simulator,\n",
    "        adapter=simple_adapter,\n",
    "        inference_network=inference_net,\n",
    "        summary_network=protein_summary_net\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Created BayesFlow workflow with custom protein summary network\")\n",
    "    print(f\"  - Summary network output dim: {summary_dim}\")\n",
    "    print(f\"  - Inference network params: {param_dim}\")\n",
    "    print(f\"  - Embedding dimension: {embedding_dim}\")\n",
    "    print(f\"  - LSTM units: {lstm_units}\")\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "# Create the workflow with custom summary network\n",
    "protein_workflow = create_protein_bayesflow_workflow()\n",
    "\n",
    "print(\"\\n‚úì Protein BayesFlow workflow created successfully with custom summary network!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56bf748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing custom protein summary network...\n",
      "‚úì Test data generated\n",
      "\n",
      "Test data structure:\n",
      "  amino_acids: shape (2, 50), dtype int64\n",
      "  true_states: shape (2, 50), dtype int64\n",
      "  state_probs: shape (2, 50, 2), dtype float64\n",
      "\n",
      "Test sequences shape: (2, 50)\n",
      "Test sequences dtype: int64\n",
      "\n",
      "Testing ProteinSummaryNetwork...\n",
      "Formatted sequences shape: (2, 50, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/keras/src/ops/nn.py:944: UserWarning: You are using a softmax over axis -1 of a tensor of shape (2, 50, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Summary network output shape: (2, 64)\n",
      "‚úì Summary network output dtype: <dtype: 'float32'>\n",
      "‚úì Expected shape: (batch_size=2, summary_dim=64)\n",
      "‚úÖ Custom summary network working correctly!\n"
     ]
    }
   ],
   "source": [
    "# TEST CUSTOM SUMMARY NETWORK WITH SAMPLE DATA\n",
    "\n",
    "print(\"Testing custom protein summary network...\")\n",
    "\n",
    "# Generate some test data using the existing simulator\n",
    "test_data = protein_workflow.simulate(2)  # Generate 2 samples\n",
    "print(\"‚úì Test data generated\")\n",
    "\n",
    "# Check the data structure\n",
    "print(\"\\nTest data structure:\")\n",
    "for key, value in test_data.items():\n",
    "    print(f\"  {key}: shape {value.shape}, dtype {value.dtype}\")\n",
    "\n",
    "# Extract the amino acid sequences for testing\n",
    "if 'summary_variables' in test_data:\n",
    "    test_sequences = test_data['summary_variables']\n",
    "elif 'amino_acids' in test_data:\n",
    "    test_sequences = test_data['amino_acids']\n",
    "else:\n",
    "    # Find the right key for amino acid sequences\n",
    "    for key, value in test_data.items():\n",
    "        if 'amino' in key.lower() or len(value.shape) == 2:\n",
    "            test_sequences = value\n",
    "            print(f\"Using {key} as amino acid sequences\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nTest sequences shape: {test_sequences.shape}\")\n",
    "print(f\"Test sequences dtype: {test_sequences.dtype}\")\n",
    "\n",
    "# Test the custom summary network directly\n",
    "print(\"\\nTesting ProteinSummaryNetwork...\")\n",
    "try:\n",
    "    # Ensure sequences are the right shape and type for our network\n",
    "    if len(test_sequences.shape) == 2:\n",
    "        # Add the feature dimension if needed: (batch, seq_len) -> (batch, seq_len, 1)\n",
    "        test_sequences_formatted = test_sequences[..., np.newaxis]\n",
    "    else:\n",
    "        test_sequences_formatted = test_sequences\n",
    "    \n",
    "    print(f\"Formatted sequences shape: {test_sequences_formatted.shape}\")\n",
    "    \n",
    "    # Create a standalone instance of our summary network to test\n",
    "    test_summary_net = ProteinSummaryNetwork(\n",
    "        vocab_size=20,\n",
    "        embedding_dim=32,\n",
    "        lstm_units=64,\n",
    "        summary_dim=64\n",
    "    )\n",
    "    \n",
    "    # Test the forward pass\n",
    "    summary_output = test_summary_net(test_sequences_formatted)\n",
    "    print(f\"‚úì Summary network output shape: {summary_output.shape}\")\n",
    "    print(f\"‚úì Summary network output dtype: {summary_output.dtype}\")\n",
    "    print(f\"‚úì Expected shape: (batch_size={test_sequences.shape[0]}, summary_dim=64)\")\n",
    "    \n",
    "    if summary_output.shape == (test_sequences.shape[0], 64):\n",
    "        print(\"‚úÖ Custom summary network working correctly!\")\n",
    "    else:\n",
    "        print(\"‚ùå Shape mismatch - need to debug\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error testing summary network: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "898c3ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bayesflow:Fitting on dataset instance of OnlineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training function defined\n",
      "\n",
      "üß™ Testing training with a small number of epochs...\n",
      "Starting training for 5 epochs with batch size 8\n",
      "============================================================\n",
      "Training configuration:\n",
      "  epochs: 5\n",
      "  batch_size: 8\n",
      "  validation_sims: 1000\n",
      "  checkpoint_interval: 1\n",
      "\n",
      "üöÄ Starting online training...\n",
      "‚ùå Training failed with error: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/1r/h80d31y92rn7dxwn7_1yfhsh0000gn/T/ipykernel_74054/3088207306.py\", line 49, in train_protein_workflow\n",
      "    training_info = workflow.fit_online(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/workflows/basic_workflow.py\", line 784, in fit_online\n",
      "    return self._fit(\n",
      "           ^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/workflows/basic_workflow.py\", line 954, in _fit\n",
      "    self.history = self.approximator.fit(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/approximators/continuous_approximator.py\", line 316, in fit\n",
      "    return super().fit(*args, **kwargs, adapter=self.adapter)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/approximators/approximator.py\", line 134, in fit\n",
      "    mock_data = dataset[0]\n",
      "                ~~~~~~~^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/datasets/online_dataset.py\", line 94, in __getitem__\n",
      "    batch = self.adapter(batch, stage=self.stage)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/adapters/adapter.py\", line 182, in __call__\n",
      "    return self.forward(data, stage=stage, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/adapters/adapter.py\", line 113, in forward\n",
      "    data = transform(data, stage=stage, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/adapters/transforms/transform.py\", line 16, in __call__\n",
      "    return self.forward(data, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/adapters/transforms/concatenate.py\", line 58, in forward\n",
      "    required_keys = set(self.keys)\n",
      "                    ^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "# TRAINING FUNCTION FOR CUSTOM PROTEIN WORKFLOW\n",
    "\n",
    "def train_protein_workflow(\n",
    "    workflow,\n",
    "    batch_size=16,\n",
    "    epochs=50,\n",
    "    print_every=10,\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the protein BayesFlow workflow with our custom summary network.\n",
    "    \n",
    "    Args:\n",
    "        workflow: The BayesFlow workflow to train\n",
    "        batch_size: Batch size for training\n",
    "        epochs: Number of training epochs\n",
    "        print_every: Print progress every N epochs\n",
    "        save_path: Path to save the trained model (optional)\n",
    "    \n",
    "    Returns:\n",
    "        training_history: Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting training for {epochs} epochs with batch size {batch_size}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    training_history = {\n",
    "        'epoch': [],\n",
    "        'loss': [],\n",
    "        'validation_loss': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Configure the workflow for training\n",
    "        config = {\n",
    "            'epochs': epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'validation_sims': 1000,  # Generate validation data\n",
    "            'checkpoint_interval': max(1, epochs // 10),  # Save checkpoints\n",
    "        }\n",
    "        \n",
    "        print(\"Training configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print()\n",
    "        \n",
    "        # Start online training\n",
    "        print(\"üöÄ Starting online training...\")\n",
    "        training_info = workflow.fit_online(\n",
    "            epochs=config['epochs'],\n",
    "            batch_size=config['batch_size'],\n",
    "            print_every=print_every\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Training completed successfully!\")\n",
    "        \n",
    "        # Extract training history if available\n",
    "        if hasattr(training_info, 'history') and training_info.history:\n",
    "            history = training_info.history\n",
    "            training_history['loss'] = history.get('loss', [])\n",
    "            training_history['validation_loss'] = history.get('val_loss', [])\n",
    "            training_history['epoch'] = list(range(1, len(training_history['loss']) + 1))\n",
    "        \n",
    "        # Save the model if path provided\n",
    "        if save_path:\n",
    "            print(f\"üíæ Saving model to {save_path}\")\n",
    "            workflow.save_model(save_path)\n",
    "            \n",
    "        return training_history\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return training_history\n",
    "\n",
    "print(\"‚úì Training function defined\")\n",
    "\n",
    "# Test training with a few epochs\n",
    "print(\"\\nüß™ Testing training with a small number of epochs...\")\n",
    "test_training_history = train_protein_workflow(\n",
    "    workflow=protein_workflow,\n",
    "    batch_size=8,\n",
    "    epochs=5,\n",
    "    print_every=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ea9e9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvertDType signature:\n",
      "(self, from_dtype: str, to_dtype: str)\n",
      "\n",
      "Available transforms:\n",
      "['AsSet', 'AsTimeSeries', 'Broadcast', 'Concatenate', 'Constrain', 'ConvertDType', 'Drop', 'ElementwiseTransform', 'ExpandDims', 'FilterTransform', 'Group', 'Keep', 'Log', 'MapTransform', 'NNPE', 'NanToNum', 'NumpyTransform', 'OneHot', 'RandomSubsample', 'Rename', 'Scale', 'SerializableCustomTransform', 'Shift', 'Split', 'Sqrt', 'Squeeze', 'Standardize', 'Take', 'ToArray', 'ToDict', 'Transform', 'Ungroup', 'as_set', 'as_time_series', 'broadcast', 'concatenate', 'constrain', 'convert_dtype', 'drop', 'elementwise_transform', 'expand_dims', 'filter_transform', 'group', 'keep', 'log', 'map_transform', 'nan_to_num', 'nnpe', 'numpy_transform', 'one_hot', 'random_subsample', 'rename', 'scale', 'serializable_custom_transform', 'shift', 'split', 'sqrt', 'squeeze', 'standardize', 'take', 'to_array', 'to_dict', 'transform', 'ungroup']\n",
      "\n",
      "MapTransform signature:\n",
      "(self, transform_map: dict[str, bayesflow.adapters.transforms.elementwise_transform.ElementwiseTransform])\n"
     ]
    }
   ],
   "source": [
    "# CHECK CONVERTDTYPE SIGNATURE\n",
    "print(\"ConvertDType signature:\")\n",
    "print(inspect.signature(bf.adapters.transforms.ConvertDType.__init__))\n",
    "\n",
    "# Check what transforms are available\n",
    "print(\"\\nAvailable transforms:\")\n",
    "transforms_list = [attr for attr in dir(bf.adapters.transforms) if not attr.startswith('_')]\n",
    "print(transforms_list)\n",
    "\n",
    "# Check MapTransform as an alternative\n",
    "if hasattr(bf.adapters.transforms, 'MapTransform'):\n",
    "    print(\"\\nMapTransform signature:\")\n",
    "    print(inspect.signature(bf.adapters.transforms.MapTransform.__init__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea6096d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Creating corrected protein BayesFlow workflow...\n",
      "‚úì Using existing HMM simulator\n",
      "‚úì Custom summary network created\n",
      "‚úì Inference network created\n",
      "‚úì Adapter with transforms created\n",
      "‚úì BayesFlow workflow created\n",
      "============================================================\n",
      "üéâ Corrected protein BayesFlow workflow created successfully!\n"
     ]
    }
   ],
   "source": [
    "# CORRECTED PROTEIN BAYESFLOW WORKFLOW CREATION\n",
    "\n",
    "def create_corrected_protein_workflow():\n",
    "    \"\"\"\n",
    "    Create BayesFlow workflow with custom protein summary network and proper adapter.\n",
    "    \"\"\"\n",
    "    print(\"Creating corrected protein BayesFlow workflow...\")\n",
    "    \n",
    "    # 1. USE EXISTING SIMULATOR\n",
    "    simulator = hmm_simulator\n",
    "    print(\"‚úì Using existing HMM simulator\")\n",
    "    \n",
    "    # 2. CUSTOM SUMMARY NETWORK\n",
    "    protein_summary_net = ProteinSummaryNetwork(\n",
    "        vocab_size=20,\n",
    "        embedding_dim=32,\n",
    "        lstm_units=64,\n",
    "        attention_dim=32,\n",
    "        summary_dim=64,\n",
    "        name='ProteinSummaryNetwork'\n",
    "    )\n",
    "    print(\"‚úì Custom summary network created\")\n",
    "    \n",
    "    # 3. INFERENCE NETWORK\n",
    "    inference_net = bf.networks.CouplingFlow(\n",
    "        num_params=4,  # HMM transition probabilities\n",
    "        num_coupling_layers=8,\n",
    "        coupling_settings={'units': [128, 128], 'activation': 'silu'},\n",
    "        name='ProteinInferenceNetwork'\n",
    "    )\n",
    "    print(\"‚úì Inference network created\")\n",
    "    \n",
    "    # 4. ADAPTER WITH CORRECT TRANSFORMS\n",
    "    adapter_transforms = [\n",
    "        # Rename variables to BayesFlow conventions\n",
    "        bf.adapters.transforms.Rename(from_key='amino_acids', to_key='summary_variables'),\n",
    "        bf.adapters.transforms.Rename(from_key='state_probs', to_key='inference_variables'),\n",
    "        \n",
    "        # Drop unused variables\n",
    "        bf.adapters.transforms.Drop(keys=['true_states']),\n",
    "        \n",
    "        # Convert data types using MapTransform\n",
    "        bf.adapters.transforms.MapTransform({\n",
    "            'summary_variables': bf.adapters.transforms.ConvertDType(\n",
    "                from_dtype='int64', to_dtype='float32'\n",
    "            ),\n",
    "            'inference_variables': bf.adapters.transforms.ConvertDType(\n",
    "                from_dtype='float64', to_dtype='float32'\n",
    "            ),\n",
    "        }),\n",
    "    ]\n",
    "    \n",
    "    adapter = bf.Adapter(transforms=adapter_transforms)\n",
    "    print(\"‚úì Adapter with transforms created\")\n",
    "    \n",
    "    # 5. CREATE WORKFLOW\n",
    "    workflow = bf.BasicWorkflow(\n",
    "        simulator=simulator,\n",
    "        adapter=adapter,\n",
    "        inference_network=inference_net,\n",
    "        summary_network=protein_summary_net\n",
    "    )\n",
    "    print(\"‚úì BayesFlow workflow created\")\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "# Create the corrected workflow\n",
    "print(\"=\" * 60)\n",
    "corrected_protein_workflow = create_corrected_protein_workflow()\n",
    "print(\"=\" * 60)\n",
    "print(\"üéâ Corrected protein BayesFlow workflow created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8fa8c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing training with corrected workflow...\n",
      "==================================================\n",
      "1. Testing simulation...\n",
      "‚úì Simulation successful\n",
      "   Data keys: ['amino_acids', 'true_states', 'state_probs']\n",
      "   amino_acids: shape (2, 50), dtype int64\n",
      "   true_states: shape (2, 50), dtype int64\n",
      "   state_probs: shape (2, 50, 2), dtype float64\n",
      "\n",
      "2. Testing data adaptation...\n",
      "‚úì Adaptation successful\n",
      "   Adapted keys: ['summary_variables', 'inference_variables']\n",
      "   summary_variables: shape (2, 50), dtype float32\n",
      "   inference_variables: shape (2, 50, 2), dtype float32\n",
      "\n",
      "3. Testing training...\n",
      "‚ùå Training test failed: name 'train_protein_workflow' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/1r/h80d31y92rn7dxwn7_1yfhsh0000gn/T/ipykernel_97426/1707173917.py\", line 25, in <module>\n",
      "    test_history = train_protein_workflow(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'train_protein_workflow' is not defined. Did you mean: 'corrected_protein_workflow'?\n"
     ]
    }
   ],
   "source": [
    "# TEST TRAINING WITH CORRECTED WORKFLOW\n",
    "\n",
    "print(\"üß™ Testing training with corrected workflow...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test simulation first\n",
    "print(\"1. Testing simulation...\")\n",
    "test_sim_data = corrected_protein_workflow.simulate(2)\n",
    "print(\"‚úì Simulation successful\")\n",
    "print(\"   Data keys:\", list(test_sim_data.keys()))\n",
    "for key, value in test_sim_data.items():\n",
    "    print(f\"   {key}: shape {value.shape}, dtype {value.dtype}\")\n",
    "\n",
    "# Test adaptation\n",
    "print(\"\\n2. Testing data adaptation...\")\n",
    "test_adapted = corrected_protein_workflow.adapter(test_sim_data)\n",
    "print(\"‚úì Adaptation successful\")\n",
    "print(\"   Adapted keys:\", list(test_adapted.keys()))\n",
    "for key, value in test_adapted.items():\n",
    "    print(f\"   {key}: shape {value.shape}, dtype {value.dtype}\")\n",
    "\n",
    "# Now test a few training epochs\n",
    "print(\"\\n3. Testing training...\")\n",
    "try:\n",
    "    test_history = train_protein_workflow(\n",
    "        workflow=corrected_protein_workflow,\n",
    "        batch_size=4,  # Small batch for testing\n",
    "        epochs=3,      # Just a few epochs for testing\n",
    "        print_every=1\n",
    "    )\n",
    "    print(\"‚úÖ Training test successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faf947fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesFlow module attributes:\n",
      "['Adapter', 'BasicWorkflow', 'ContinuousApproximator', 'DiskDataset', 'OfflineDataset', 'OnlineDataset', 'PointApproximator', 'adapters', 'approximators', 'datasets', 'diagnostics', 'distributions', 'experimental', 'links', 'make_simulator', 'metrics', 'networks', 'scores', 'simulators', 'types', 'utils', 'workflows', 'wrappers']\n",
      "\n",
      "Checking for simulation module...\n",
      "‚úó bf.simulation not found\n",
      "\n",
      "Checking for simulators module...\n",
      "‚úì bf.simulators exists\n",
      "  Attributes: ['BernoulliGLM', 'BernoulliGLMRaw', 'GaussianLinear', 'GaussianLinearUniform', 'GaussianMixture', 'HierarchicalSimulator', 'InverseKinematics', 'LambdaSimulator', 'LotkaVolterra', 'ModelComparisonSimulator', 'SIR', 'SLCP', 'SLCPDistractors', 'SequentialSimulator', 'Simulator', 'TwoMoons', 'benchmark_simulators', 'hierarchical_simulator', 'lambda_simulator', 'make_simulator', 'model_comparison_simulator', 'sequential_simulator', 'simulator']\n",
      "\n",
      "Checking other potential simulation-related modules...\n",
      "‚úì bf.simulators exists\n",
      "  Relevant attributes: ['HierarchicalSimulator', 'LambdaSimulator', 'ModelComparisonSimulator', 'SequentialSimulator', 'Simulator']\n",
      "‚úó bf.simulation not found\n",
      "‚úó bf.training not found\n",
      "‚úó bf.trainers not found\n"
     ]
    }
   ],
   "source": [
    "# CHECK BAYESFLOW MODULE STRUCTURE\n",
    "print(\"BayesFlow module attributes:\")\n",
    "print([attr for attr in dir(bf) if not attr.startswith('_')])\n",
    "\n",
    "print(\"\\nChecking for simulation module...\")\n",
    "if hasattr(bf, 'simulation'):\n",
    "    print(\"‚úì bf.simulation exists\")\n",
    "    print(\"  Attributes:\", [attr for attr in dir(bf.simulation) if not attr.startswith('_')])\n",
    "else:\n",
    "    print(\"‚úó bf.simulation not found\")\n",
    "\n",
    "print(\"\\nChecking for simulators module...\")\n",
    "if hasattr(bf, 'simulators'):\n",
    "    print(\"‚úì bf.simulators exists\")\n",
    "    print(\"  Attributes:\", [attr for attr in dir(bf.simulators) if not attr.startswith('_')])\n",
    "else:\n",
    "    print(\"‚úó bf.simulators not found\")\n",
    "\n",
    "print(\"\\nChecking other potential simulation-related modules...\")\n",
    "for module_name in ['simulators', 'simulation', 'training', 'trainers']:\n",
    "    if hasattr(bf, module_name):\n",
    "        module = getattr(bf, module_name)\n",
    "        print(f\"‚úì bf.{module_name} exists\")\n",
    "        attrs = [attr for attr in dir(module) if not attr.startswith('_')]\n",
    "        if 'Simulator' in ' '.join(attrs) or 'Lambda' in ' '.join(attrs):\n",
    "            print(f\"  Relevant attributes: {[attr for attr in attrs if 'Simulator' in attr or 'Lambda' in attr]}\")\n",
    "    else:\n",
    "        print(f\"‚úó bf.{module_name} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7093dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicWorkflow attributes:\n",
      "['adapter', 'build_graph', 'build_optimizer', 'compute_custom_diagnostics', 'compute_default_diagnostics', 'compute_diagnostics', 'default_adapter', 'estimate', 'fit', 'fit_disk', 'fit_offline', 'fit_online', 'log_prob', 'make_simulator', 'plot_custom_diagnostics', 'plot_default_diagnostics', 'plot_diagnostics', 'sample', 'samples_to_data_frame', 'simulate', 'simulate_adapted']\n",
      "\n",
      "BasicWorkflow docstring:\n",
      "None\n",
      "\n",
      "BasicWorkflow init signature:\n",
      "(self, simulator: bayesflow.simulators.simulator.Simulator = None, adapter: bayesflow.adapters.adapter.Adapter = None, inference_network: bayesflow.networks.inference_network.InferenceNetwork | str = 'coupling_flow', summary_network: bayesflow.networks.summary_network.SummaryNetwork | str = None, initial_learning_rate: float = 0.0005, optimizer: keras.src.optimizers.optimizer.Optimizer | type = None, checkpoint_filepath: str = None, checkpoint_name: str = 'model', save_weights_only: bool = False, save_best_only: bool = False, inference_variables: collections.abc.Sequence[str] | str = None, inference_conditions: collections.abc.Sequence[str] | str = None, summary_variables: collections.abc.Sequence[str] | str = None, standardize: collections.abc.Sequence[str] | str | None = 'inference_variables', **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# CHECK BASICWORKFLOW STRUCTURE\n",
    "print(\"BasicWorkflow attributes:\")\n",
    "print([attr for attr in dir(bf.BasicWorkflow) if not attr.startswith('_')])\n",
    "\n",
    "print(\"\\nBasicWorkflow docstring:\")\n",
    "print(bf.BasicWorkflow.__doc__)\n",
    "\n",
    "print(\"\\nBasicWorkflow init signature:\")\n",
    "import inspect\n",
    "print(inspect.signature(bf.BasicWorkflow.__init__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9452642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesFlow adapters module:\n",
      "['Adapter', 'adapter', 'transforms']\n",
      "\n",
      "Adapter class signature:\n",
      "(self, transforms: collections.abc.Sequence[bayesflow.adapters.transforms.transform.Transform] | None = None)\n"
     ]
    }
   ],
   "source": [
    "# CHECK AVAILABLE ADAPTERS\n",
    "print(\"BayesFlow adapters module:\")\n",
    "print([attr for attr in dir(bf.adapters) if not attr.startswith('_')])\n",
    "\n",
    "print(\"\\nAdapter class signature:\")\n",
    "print(inspect.signature(bf.Adapter.__init__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deb71258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for hmm_prior function...\n",
      "‚úó hmm_prior not found\n",
      "\n",
      "Checking for hmm_simulator...\n",
      "‚úì hmm_simulator found\n",
      "  Type: <class 'bayesflow.simulators.lambda_simulator.LambdaSimulator'>\n",
      "\n",
      "Existing simulator-related variables:\n",
      "  hmm_simulator: <class 'bayesflow.simulators.lambda_simulator.LambdaSimulator'>\n",
      "  hmm_model: <class 'hmmlearn.hmm.CategoricalHMM'>\n",
      "\n",
      "Simulator details:\n",
      "  Type: <class 'bayesflow.simulators.lambda_simulator.LambdaSimulator'>\n"
     ]
    }
   ],
   "source": [
    "# CHECK EXISTING HMM FUNCTIONS\n",
    "print(\"Checking for hmm_prior function...\")\n",
    "if 'hmm_prior' in globals():\n",
    "    print(\"‚úì hmm_prior found\")\n",
    "else:\n",
    "    print(\"‚úó hmm_prior not found\")\n",
    "\n",
    "print(\"\\nChecking for hmm_simulator...\")\n",
    "if 'hmm_simulator' in globals():\n",
    "    print(\"‚úì hmm_simulator found\")\n",
    "    print(\"  Type:\", type(hmm_simulator))\n",
    "else:\n",
    "    print(\"‚úó hmm_simulator not found\")\n",
    "\n",
    "print(\"\\nExisting simulator-related variables:\")\n",
    "for var_name in ['hmm_simulator', 'hmm_model']:\n",
    "    if var_name in globals():\n",
    "        print(f\"  {var_name}: {type(globals()[var_name])}\")\n",
    "        \n",
    "# Let's check if the existing hmm_simulator is a LambdaSimulator\n",
    "if 'hmm_simulator' in globals():\n",
    "    simulator = globals()['hmm_simulator']\n",
    "    print(f\"\\nSimulator details:\")\n",
    "    print(f\"  Type: {type(simulator)}\")\n",
    "    if hasattr(simulator, 'prior_fun'):\n",
    "        print(f\"  Has prior_fun: {simulator.prior_fun}\")\n",
    "    if hasattr(simulator, 'simulator_fun'):\n",
    "        print(f\"  Has simulator_fun: {simulator.simulator_fun}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4207977a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING CONFIGURATION:\n",
      "  strategy: online\n",
      "  epochs: 50\n",
      "  batch_size: 32\n",
      "  num_batches_per_epoch: 100\n",
      "  sequence_length: 50\n",
      "  total_samples: 10000\n",
      "  validation_samples: 1000\n",
      "\n",
      "RUNNING QUICK TRAINING DEMO:\n",
      "STARTING BAYESFLOW TRAINING:\n",
      "Training strategy: online\n",
      "Epochs: 5\n",
      "Batch size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bayesflow:Fitting on dataset instance of OnlineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:GPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [16,50,1] vs. shape[1] = [16,64] [Op:ConcatV2] name: concat\n",
      "This might occur due to shape mismatches - let's debug the data flow...\n",
      "\n",
      "DEBUG - Raw simulation output:\n",
      "  amino_acids: (2, 20)\n",
      "  true_states: (2, 20)\n",
      "  state_probs: (2, 20, 2)\n",
      "\n",
      "DEBUG - Adapted data:\n",
      "  inference_variables: (2, 20, 2)\n",
      "  summary_variables: (2, 20, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 12:36:42.016133: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: ConcatOp : Ranks of all input tensors should match: shape[0] = [16,50,1] vs. shape[1] = [16,64]\n"
     ]
    }
   ],
   "source": [
    "# BAYESFLOW TRAINING IMPLEMENTATION\n",
    "\n",
    "def train_bayesflow_workflow(workflow, training_config):\n",
    "    \"\"\"\n",
    "    Train the BayesFlow workflow with proper configuration.\n",
    "    \n",
    "    Args:\n",
    "        workflow: BayesFlow BasicWorkflow instance\n",
    "        training_config: Dictionary with training parameters\n",
    "        \n",
    "    Returns:\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    print(f\"STARTING BAYESFLOW TRAINING:\")\n",
    "    print(f\"Training strategy: {training_config['strategy']}\")\n",
    "    print(f\"Epochs: {training_config['epochs']}\")\n",
    "    print(f\"Batch size: {training_config['batch_size']}\")\n",
    "    \n",
    "    if training_config['strategy'] == 'online':\n",
    "        # Online training - generate data on-the-fly\n",
    "        history = workflow.fit_online(\n",
    "            epochs=training_config['epochs'],\n",
    "            num_batches_per_epoch=training_config['num_batches_per_epoch'],\n",
    "            batch_size=training_config['batch_size'],\n",
    "            validation_data=training_config.get('validation_samples', None),\n",
    "            sequence_length=training_config['sequence_length']\n",
    "        )\n",
    "    else:\n",
    "        # Offline training - pre-generate training data\n",
    "        print(\"Generating training dataset...\")\n",
    "        training_data = workflow.simulate(\n",
    "            batch_shape=(training_config['total_samples'],),\n",
    "            sequence_length=training_config['sequence_length']\n",
    "        )\n",
    "        \n",
    "        # Generate validation data if specified\n",
    "        validation_data = None\n",
    "        if training_config.get('validation_samples'):\n",
    "            print(\"Generating validation dataset...\")\n",
    "            validation_data = workflow.simulate(\n",
    "                batch_shape=(training_config['validation_samples'],),\n",
    "                sequence_length=training_config['sequence_length']\n",
    "            )\n",
    "        \n",
    "        # Train offline with pre-generated data\n",
    "        history = workflow.fit_offline(\n",
    "            data=training_data,\n",
    "            epochs=training_config['epochs'],\n",
    "            batch_size=training_config['batch_size'],\n",
    "            validation_data=validation_data\n",
    "        )\n",
    "    \n",
    "    print(\"‚úì Training completed!\")\n",
    "    return history\n",
    "\n",
    "# Define training configuration\n",
    "training_config = {\n",
    "    'strategy': 'online',           # 'online' or 'offline'\n",
    "    'epochs': 50,                   # Number of training epochs\n",
    "    'batch_size': 32,               # Batch size for training\n",
    "    'num_batches_per_epoch': 100,   # For online training\n",
    "    'sequence_length': 50,          # Length of amino acid sequences\n",
    "    'total_samples': 10000,         # For offline training\n",
    "    'validation_samples': 1000      # Number of validation samples\n",
    "}\n",
    "\n",
    "print(\"TRAINING CONFIGURATION:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Run a quick training test with reduced parameters for demonstration\n",
    "quick_config = training_config.copy()\n",
    "quick_config.update({\n",
    "    'epochs': 5,                    # Reduced for quick demo\n",
    "    'batch_size': 16,              # Smaller batch size\n",
    "    'num_batches_per_epoch': 20,   # Fewer batches per epoch\n",
    "    'sequence_length': 30          # Shorter sequences\n",
    "})\n",
    "\n",
    "print(f\"\\nRUNNING QUICK TRAINING DEMO:\")\n",
    "try:\n",
    "    # Train the workflow\n",
    "    training_history = train_bayesflow_workflow(workflow, quick_config)\n",
    "    \n",
    "    # Display training results\n",
    "    print(f\"\\nTRAINING RESULTS:\")\n",
    "    print(f\"Training completed successfully!\")\n",
    "    print(f\"Final loss: {training_history.history['loss'][-1]:.4f}\")\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_history.history['loss'], label='Training Loss')\n",
    "    if 'val_loss' in training_history.history:\n",
    "        plt.plot(training_history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('BayesFlow Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Quick training demo completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    print(\"This might occur due to shape mismatches - let's debug the data flow...\")\n",
    "    \n",
    "    # Debug data shapes\n",
    "    debug_data = workflow.simulate(batch_shape=(2,), sequence_length=20)\n",
    "    print(f\"\\nDEBUG - Raw simulation output:\")\n",
    "    for key, value in debug_data.items():\n",
    "        print(f\"  {key}: {value.shape}\")\n",
    "    \n",
    "    debug_adapted = workflow.adapter(debug_data)\n",
    "    print(f\"\\nDEBUG - Adapted data:\")\n",
    "    for key, value in debug_adapted.items():\n",
    "        print(f\"  {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76106520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAYESFLOW INFERENCE AND EVALUATION\n",
    "\n",
    "def perform_inference(workflow, amino_acid_sequence, num_samples=1000):\n",
    "    \"\"\"\n",
    "    Perform posterior inference on a given amino acid sequence.\n",
    "    \n",
    "    Args:\n",
    "        workflow: Trained BayesFlow workflow\n",
    "        amino_acid_sequence: Array of amino acid indices\n",
    "        num_samples: Number of posterior samples to draw\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with posterior samples and statistics\n",
    "    \"\"\"\n",
    "    # Ensure input is properly shaped (batch_size=1, sequence_length)\n",
    "    if amino_acid_sequence.ndim == 1:\n",
    "        amino_acid_sequence = amino_acid_sequence[np.newaxis, :]\n",
    "    \n",
    "    # Create conditions dictionary for inference\n",
    "    conditions = {\n",
    "        'amino_acids': amino_acid_sequence.astype(np.float32)\n",
    "    }\n",
    "    \n",
    "    # Apply adapter to get proper format for inference\n",
    "    adapted_conditions = workflow.adapter(conditions)\n",
    "    \n",
    "    # Sample from posterior\n",
    "    posterior_samples = workflow.sample(\n",
    "        num_samples=num_samples,\n",
    "        conditions=adapted_conditions\n",
    "    )\n",
    "    \n",
    "    # Compute posterior statistics\n",
    "    inference_vars = posterior_samples['inference_variables']\n",
    "    \n",
    "    # Reshape to (num_samples, sequence_length, num_states)\n",
    "    if inference_vars.ndim == 2:\n",
    "        sequence_length = amino_acid_sequence.shape[1]\n",
    "        num_states = 2  # alpha-helix, other\n",
    "        inference_vars = inference_vars.reshape(num_samples, sequence_length, num_states)\n",
    "    \n",
    "    # Compute statistics\n",
    "    posterior_mean = np.mean(inference_vars, axis=0)\n",
    "    posterior_std = np.std(inference_vars, axis=0)\n",
    "    posterior_quantiles = np.quantile(inference_vars, [0.025, 0.5, 0.975], axis=0)\n",
    "    \n",
    "    return {\n",
    "        'samples': inference_vars,\n",
    "        'mean': posterior_mean,\n",
    "        'std': posterior_std,\n",
    "        'quantiles': posterior_quantiles,\n",
    "        'conditions': conditions\n",
    "    }\n",
    "\n",
    "def visualize_inference_results(inference_results, amino_acid_sequence):\n",
    "    \"\"\"\n",
    "    Visualize the results of posterior inference.\n",
    "    \n",
    "    Args:\n",
    "        inference_results: Output from perform_inference()\n",
    "        amino_acid_sequence: Original amino acid sequence\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 10))\n",
    "    \n",
    "    sequence_length = len(amino_acid_sequence)\n",
    "    positions = np.arange(sequence_length)\n",
    "    \n",
    "    # Plot 1: Amino acid sequence\n",
    "    axes[0].scatter(positions, amino_acid_sequence, c='darkblue', alpha=0.7, s=50)\n",
    "    axes[0].set_ylabel('Amino Acid Index')\n",
    "    axes[0].set_title('Input Amino Acid Sequence')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add amino acid letters as labels\n",
    "    amino_acid_letters = [AMINO_ACIDS[int(idx)] for idx in amino_acid_sequence]\n",
    "    for i, letter in enumerate(amino_acid_letters):\n",
    "        axes[0].text(i, amino_acid_sequence[i] + 0.5, letter, \n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Plot 2: Posterior mean probabilities\n",
    "    mean_probs = inference_results['mean']\n",
    "    axes[1].plot(positions, mean_probs[:, 0], 'b-', linewidth=2, label='P(Alpha-helix)', marker='o')\n",
    "    axes[1].plot(positions, mean_probs[:, 1], 'r-', linewidth=2, label='P(Other)', marker='s')\n",
    "    \n",
    "    # Add uncertainty bands\n",
    "    std_probs = inference_results['std']\n",
    "    axes[1].fill_between(positions, \n",
    "                        mean_probs[:, 0] - std_probs[:, 0],\n",
    "                        mean_probs[:, 0] + std_probs[:, 0], \n",
    "                        alpha=0.3, color='blue')\n",
    "    axes[1].fill_between(positions,\n",
    "                        mean_probs[:, 1] - std_probs[:, 1], \n",
    "                        mean_probs[:, 1] + std_probs[:, 1],\n",
    "                        alpha=0.3, color='red')\n",
    "    \n",
    "    axes[1].set_ylabel('Posterior Probability')\n",
    "    axes[1].set_title('Posterior Mean State Probabilities (with uncertainty)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    # Plot 3: Credible intervals\n",
    "    quantiles = inference_results['quantiles']\n",
    "    axes[2].fill_between(positions, quantiles[0, :, 0], quantiles[2, :, 0], \n",
    "                        alpha=0.4, color='blue', label='Alpha-helix 95% CI')\n",
    "    axes[2].fill_between(positions, quantiles[0, :, 1], quantiles[2, :, 1],\n",
    "                        alpha=0.4, color='red', label='Other 95% CI') \n",
    "    axes[2].plot(positions, quantiles[1, :, 0], 'b-', linewidth=2, label='Alpha-helix median')\n",
    "    axes[2].plot(positions, quantiles[1, :, 1], 'r-', linewidth=2, label='Other median')\n",
    "    \n",
    "    axes[2].set_xlabel('Position in Sequence')\n",
    "    axes[2].set_ylabel('Probability')\n",
    "    axes[2].set_title('Posterior Credible Intervals (95%)')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"INFERENCE SUMMARY:\")\n",
    "    print(f\"Sequence length: {sequence_length}\")\n",
    "    print(f\"Mean alpha-helix probability: {np.mean(mean_probs[:, 0]):.3f}\")\n",
    "    print(f\"Mean other probability: {np.mean(mean_probs[:, 1]):.3f}\")\n",
    "    print(f\"Positions with high alpha-helix confidence (>0.7): {np.sum(mean_probs[:, 0] > 0.7)}\")\n",
    "    print(f\"Positions with high other confidence (>0.7): {np.sum(mean_probs[:, 1] > 0.7)}\")\n",
    "\n",
    "# Example usage after training\n",
    "print(\"INFERENCE AND EVALUATION SETUP COMPLETE!\")\n",
    "print(\"\\nTo use after training:\")\n",
    "print(\"1. Generate or provide an amino acid sequence\")\n",
    "print(\"2. Run: results = perform_inference(workflow, sequence)\")\n",
    "print(\"3. Visualize: visualize_inference_results(results, sequence)\")\n",
    "\n",
    "# Prepare a test sequence for inference (when workflow is trained)\n",
    "print(f\"\\nSample amino acid sequence for testing:\")\n",
    "test_sequence = np.array([0, 5, 10, 2, 15, 8, 12, 3, 17, 6])  # Mix of amino acids\n",
    "test_letters = [AMINO_ACIDS[idx] for idx in test_sequence]\n",
    "print(f\"Amino acids: {test_letters}\")\n",
    "print(f\"Indices: {test_sequence}\")\n",
    "\n",
    "print(\"\\n‚úì Inference and evaluation functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf1fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff019c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0bff03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c1ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85476d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe28a7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee1b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72435f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b66a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1198a2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1012a879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ea5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
