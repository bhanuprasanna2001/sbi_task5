{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d6fab79",
   "metadata": {},
   "source": [
    "## Understanding Task 5\n",
    "\n",
    "- Protiens have totally 4 structures, but our focus is on Protein Secondary structure:\n",
    "  1. Secondary (Our focus in this project) - Contains many patterns but focus on 2 states only for this project.\n",
    "  2. But dividing them into only 2 pattern Alpha Helix and Others (include rest of the patterns).\n",
    "  3. Focus specifically on predicting the alpha helix patterns using a 2 state HMM.\n",
    "  4. The 2 states are \"alpha helix\" and \"other\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a1bb165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 17:20:11.885169: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-07-12 17:20:11.885205: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-07-12 17:20:11.885213: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752333611.885226 4489514 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1752333611.885252 4489514 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "INFO:bayesflow:Using backend 'tensorflow'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.keras is using the 'tensorflow' backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import bayesflow as bf\n",
    "from bayesflow.adapters import Adapter\n",
    "from bayesflow.datasets import OnlineDataset\n",
    "from bayesflow.workflows import BasicWorkflow\n",
    "from bayesflow.adapters.transforms import OneHot\n",
    "from bayesflow.approximators import ContinuousApproximator\n",
    "from bayesflow.adapters.transforms import MapTransform, ExpandDims\n",
    "\n",
    "from hmmlearn import hmm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "current_backend = tf.keras.backend.backend()\n",
    "print(f\"tf.keras is using the '{current_backend}' backend.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "457c75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinHMMSimulator:\n",
    "    \"\"\"\n",
    "    A simulator for protein secondary structure prediction using HMM.\n",
    "    \n",
    "    This class implements a two-state HMM (alpha-helix vs other) with fixed\n",
    "    emission and transition probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define amino acids\n",
    "        self.amino_acids = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I',\n",
    "                           'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "        \n",
    "        # State definitions: 0 = \"other\", 1 = \"alpha-helix\"\n",
    "        self.states = ['other', 'alpha-helix']\n",
    "        self.n_states = 2\n",
    "        self.n_features = len(self.amino_acids)\n",
    "        \n",
    "        # Initialize label encoder for amino acids\n",
    "        self.aa_encoder = LabelEncoder()\n",
    "        self.aa_encoder.fit(self.amino_acids)\n",
    "        \n",
    "        # Define emission probabilities\n",
    "        self.emission_probs = self._setup_emission_probabilities()\n",
    "        \n",
    "        # Define transition probabilities\n",
    "        self.transition_probs = self._setup_transition_probabilities()\n",
    "        \n",
    "        # Initialize start probabilities (always start in \"other\" state)\n",
    "        self.start_probs = np.array([1.0, 0.0])  # [other, alpha-helix]\n",
    "        \n",
    "        # Setup HMM model\n",
    "        self.hmm_model = self._setup_hmm_model()\n",
    "        \n",
    "    def _setup_emission_probabilities(self) -> np.ndarray:\n",
    "        \"\"\"Setup emission probability matrix.\"\"\"\n",
    "        \n",
    "        # Alpha-helix probabilities (state 1)\n",
    "        alpha_helix_probs = [\n",
    "            12, 6, 3, 5, 1, 9, 5, 4, 2, 7,  # A, R, N, D, C, E, Q, G, H, I\n",
    "            12, 6, 3, 4, 2, 5, 4, 1, 3, 6   # L, K, M, F, P, S, T, W, Y, V\n",
    "        ]\n",
    "        \n",
    "        # Other probabilities (state 0)\n",
    "        other_probs = [\n",
    "            6, 5, 5, 6, 2, 5, 3, 9, 3, 5,   # A, R, N, D, C, E, Q, G, H, I\n",
    "            8, 6, 2, 4, 6, 7, 6, 1, 4, 7    # L, K, M, F, P, S, T, W, Y, V\n",
    "        ]\n",
    "        \n",
    "        # Convert percentages to probabilities\n",
    "        alpha_helix_probs = np.array(alpha_helix_probs) / 100.0\n",
    "        other_probs = np.array(other_probs) / 100.0\n",
    "        \n",
    "        # Ensure probabilities sum to 1 (normalize if needed)\n",
    "        alpha_helix_probs = alpha_helix_probs / np.sum(alpha_helix_probs)\n",
    "        other_probs = other_probs / np.sum(other_probs)\n",
    "        \n",
    "        # Create emission matrix: [n_states, n_features]\n",
    "        emission_matrix = np.array([other_probs, alpha_helix_probs])\n",
    "        \n",
    "        return emission_matrix\n",
    "    \n",
    "    def _setup_transition_probabilities(self) -> np.ndarray:\n",
    "        \"\"\"Setup transition probability matrix.\"\"\"\n",
    "        \n",
    "        # Transition probabilities:\n",
    "        # From \"other\" (state 0): 95% stay, 5% to alpha-helix\n",
    "        # From \"alpha-helix\" (state 1): 10% to other, 90% stay\n",
    "        \n",
    "        transition_matrix = np.array([\n",
    "            [0.95, 0.05],  # From \"other\" to [\"other\", \"alpha-helix\"]\n",
    "            [0.10, 0.90]   # From \"alpha-helix\" to [\"other\", \"alpha-helix\"]\n",
    "        ])\n",
    "        \n",
    "        return transition_matrix\n",
    "    \n",
    "    def _setup_hmm_model(self) -> hmm.CategoricalHMM:\n",
    "        \"\"\"Initialize and configure the HMM model.\"\"\"\n",
    "        \n",
    "        model = hmm.CategoricalHMM(\n",
    "            n_components=self.n_states,\n",
    "            random_state=42,\n",
    "            init_params=\"\",  # Don't initialize parameters randomly\n",
    "            params=\"\"        # Don't update parameters during fitting\n",
    "        )\n",
    "        \n",
    "        # Set the fixed probabilities\n",
    "        model.startprob_ = self.start_probs\n",
    "        model.transmat_ = self.transition_probs\n",
    "        model.emissionprob_ = self.emission_probs\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def generate_sequence(self, length: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate a single amino acid sequence with corresponding states.\n",
    "        \n",
    "        Args:\n",
    "            length: Length of the sequence to generate\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (amino_acid_sequence, state_sequence)\n",
    "        \"\"\"\n",
    "        # Generate sequence using HMM\n",
    "        amino_acid_indices, state_sequence = self.hmm_model.sample(length)\n",
    "        \n",
    "        # Convert indices back to amino acid letters\n",
    "        amino_acid_sequence = self.aa_encoder.inverse_transform(amino_acid_indices.flatten())\n",
    "        \n",
    "        return amino_acid_sequence, state_sequence.flatten()\n",
    "    \n",
    "    def get_state_probabilities(self, amino_acid_sequence: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get state probabilities for a given amino acid sequence using Forward-Backward algorithm.\n",
    "        \n",
    "        Args:\n",
    "            amino_acid_sequence: Array of amino acid characters\n",
    "            \n",
    "        Returns:\n",
    "            Array of state probabilities [n_positions, n_states]\n",
    "        \"\"\"\n",
    "        # Convert amino acids to indices\n",
    "        aa_indices = self.aa_encoder.transform(amino_acid_sequence).reshape(-1, 1)\n",
    "        \n",
    "        # Use predict_proba to get state probabilities (Forward-Backward algorithm)\n",
    "        state_probs = self.hmm_model.predict_proba(aa_indices)\n",
    "        \n",
    "        return state_probs\n",
    "    \n",
    "    def sample_batch(self, batch_size: int, min_length: int = 50, max_length: int = 300) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate a batch of sequences for training.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of sequences to generate\n",
    "            min_length: Minimum sequence length\n",
    "            max_length: Maximum sequence length\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with amino acid sequences and state probabilities\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        state_probs_batch = []\n",
    "        sequence_lengths = []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            # Random sequence length\n",
    "            length = np.random.randint(min_length, max_length + 1)\n",
    "            \n",
    "            # Generate sequence\n",
    "            aa_seq, true_states = self.generate_sequence(length)\n",
    "            \n",
    "            # Get state probabilities using Forward-Backward\n",
    "            state_probs = self.get_state_probabilities(aa_seq)\n",
    "            \n",
    "            sequences.append(aa_seq)\n",
    "            state_probs_batch.append(state_probs)\n",
    "            sequence_lengths.append(length)\n",
    "        \n",
    "        return {\n",
    "            'amino_acid_sequences': sequences,\n",
    "            'state_probabilities': state_probs_batch,\n",
    "            'sequence_lengths': sequence_lengths\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9875f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesFlowProteinSimulator(bf.simulators.Simulator):\n",
    "    \"\"\"\n",
    "    BayesFlow-compatible simulator for protein secondary structure prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_length: int = 300, min_length: int = 50):\n",
    "        super().__init__()\n",
    "        self.protein_hmm = ProteinHMMSimulator()\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "        \n",
    "        # For padding sequences to fixed length\n",
    "        self.pad_token = 20  # Use index 20 for padding (beyond 20 amino acids)\n",
    "        \n",
    "    def _pad_sequence(self, sequence: np.ndarray, target_length: int) -> np.ndarray:\n",
    "        \"\"\"Pad sequence to target length.\"\"\"\n",
    "        if len(sequence) >= target_length:\n",
    "            return sequence[:target_length]\n",
    "        else:\n",
    "            # Pad with pad_token\n",
    "            padded = np.full(target_length, self.pad_token)\n",
    "            padded[:len(sequence)] = sequence\n",
    "            return padded\n",
    "    \n",
    "    def _encode_amino_acids(self, aa_sequence: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Convert amino acid letters to indices.\"\"\"\n",
    "        return self.protein_hmm.aa_encoder.transform(aa_sequence)\n",
    "    \n",
    "    def sample(self, batch_size: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate a batch of protein sequences and their state probabilities.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of sequences to generate\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with encoded sequences and state probabilities\n",
    "        \"\"\"\n",
    "        if isinstance(batch_size, tuple):\n",
    "            batch_size = int(np.prod(batch_size))\n",
    "            \n",
    "        # Generate sequences\n",
    "        batch_data = self.protein_hmm.sample_batch(\n",
    "            batch_size=batch_size,\n",
    "            min_length=self.min_length,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        # Process sequences for fixed-length input\n",
    "        encoded_sequences = []\n",
    "        state_probs_padded = []\n",
    "        valid_lengths = []\n",
    "        masks = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            aa_seq = batch_data['amino_acid_sequences'][i]\n",
    "            state_probs = batch_data['state_probabilities'][i]\n",
    "            seq_len = len(aa_seq)\n",
    "            \n",
    "            # Encode amino acids\n",
    "            encoded_aa = self._encode_amino_acids(aa_seq)\n",
    "            \n",
    "            # Pad sequences to max_length\n",
    "            padded_sequence = self._pad_sequence(encoded_aa, self.max_length)\n",
    "            \n",
    "            # Pad state probabilities (pad with zeros)\n",
    "            padded_state_probs = np.zeros((self.max_length, 2))\n",
    "            padded_state_probs[:seq_len] = state_probs\n",
    "            \n",
    "            # Create mask: 1 for valid positions, 0 for padded positions\n",
    "            mask = np.zeros(self.max_length)\n",
    "            mask[:seq_len] = 1\n",
    "            \n",
    "            encoded_sequences.append(padded_sequence)\n",
    "            state_probs_padded.append(padded_state_probs)\n",
    "            valid_lengths.append(seq_len)\n",
    "            masks.append(mask)\n",
    "        \n",
    "        return {\n",
    "            'amino_acid_sequences': np.array(encoded_sequences),\n",
    "            'state_probabilities': np.array(state_probs_padded),\n",
    "            'sequence_lengths': np.array(valid_lengths),\n",
    "            'alpha_helix_probs': np.array(state_probs_padded)[:, :, 1],  # Extract alpha-helix probabilities\n",
    "            'mask': np.array(masks)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56cb47ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of sample data items:\n",
      "amino_acid_sequences: (30, 1632) (dtype: int64)\n",
      "state_probabilities: (30, 1632, 2) (dtype: float64)\n",
      "sequence_lengths: (30,) (dtype: int64)\n",
      "alpha_helix_probs: (30, 1632) (dtype: float64)\n",
      "mask: (30, 1632) (dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "simulator = BayesFlowProteinSimulator(max_length=1632, min_length=20)\n",
    "\n",
    "def _flat(x):\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    return tf.reshape(x, (tf.shape(x)[0], -1))\n",
    "\n",
    "adapter = ContinuousApproximator.build_adapter(\n",
    "    inference_variables={\"state_probabilities\" : MapTransform(_flat)},\n",
    "    inference_conditions=\"mask\",\n",
    "    summary_variables=\"amino_acid_sequences\",\n",
    ")\n",
    "\n",
    "dataset = OnlineDataset(\n",
    "    simulator=simulator,\n",
    "    adapter=adapter,\n",
    "    batch_size=64,\n",
    "    num_batches=1000\n",
    ")\n",
    "\n",
    "test_sample_data = dataset.simulator.sample(30)\n",
    "print(\"Shapes of sample data items:\")\n",
    "for key, value in test_sample_data.items():\n",
    "    print(f\"{key}: {value.shape} (dtype: {value.dtype})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18776aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adapter([0: ToArray -> 1: ConvertDType -> 2: Concatenate(['state_probabilities'] -> 'inference_variables') -> 3: Rename('mask' -> 'inference_conditions') -> 4: AsSet -> 5: Rename('amino_acid_sequences' -> 'summary_variables') -> 6: Keep(['inference_variables', 'inference_conditions', 'summary_variables', 'sample_weight'])])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32b8953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesflow.networks import TimeSeriesNetwork\n",
    "\n",
    "# Summary network: LSTNet-like for sequences of length 1632 (single channel)\n",
    "summary_network = TimeSeriesNetwork(\n",
    "    summary_dim=32,           # embed into 32-dimensional summary space\n",
    "    filters=[64, 64],         # two conv layers with 64 filters each\n",
    "    kernel_sizes=[5, 3],      # first conv kernel=5, second=3\n",
    "    strides=[1, 1],\n",
    "    activation=\"relu\",\n",
    "    recurrent_type=\"lstm\",    # capture dependencies via LSTM\n",
    "    recurrent_dim=64,\n",
    "    bidirectional=True,\n",
    "    dropout=0.1,\n",
    "    skip_steps=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e945ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesflow.networks import CouplingFlow, MLP\n",
    "\n",
    "# Inference network: invertible flow for 2-dimensional state probs per position\n",
    "inference_network = CouplingFlow(\n",
    "    subnet=MLP(\n",
    "        widths=[512, 512, 256],  # Increased capacity and correct output dim\n",
    "        activation=\"mish\",\n",
    "        dropout=0.05\n",
    "    ),\n",
    "    depth=4,                          # fewer coupling layers\n",
    "    subnet_kwargs={\"widths\": [64, 64], \"activation\": \"relu\"},\n",
    "    transform=\"affine\",\n",
    "    permutation=\"random\",\n",
    "    use_actnorm=True,\n",
    "    base_distribution=\"normal\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88d46616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesflow.workflows import BasicWorkflow\n",
    "\n",
    "workflow = BasicWorkflow(\n",
    "    simulator=simulator,\n",
    "    adapter=adapter,\n",
    "    inference_network=inference_network,\n",
    "    summary_network=summary_network,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20e463c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bayesflow:Fitting on dataset instance of OnlineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "2025-07-12 17:23:36.174115: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: ConcatOp : Ranks of all input tensors should match: shape[0] = [64,1632,1] vs. shape[1] = [64,1664]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:GPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [64,1632,1] vs. shape[1] = [64,1664] [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_online\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# total passes through the data\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_batches_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# batches generated per epoch\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/workflows/basic_workflow.py:784\u001b[39m, in \u001b[36mBasicWorkflow.fit_online\u001b[39m\u001b[34m(self, epochs, num_batches_per_epoch, batch_size, keep_optimizer, validation_data, augmentations, **kwargs)\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    737\u001b[39m \u001b[33;03mTrain the approximator using an online data-generating process. The dataset is dynamically generated during\u001b[39;00m\n\u001b[32m    738\u001b[39m \u001b[33;03mtraining, making this approach suitable for scenarios where generating new simulations is computationally cheap.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    773\u001b[39m \u001b[33;03m    metric evolution over epochs.\u001b[39;00m\n\u001b[32m    774\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    776\u001b[39m dataset = OnlineDataset(\n\u001b[32m    777\u001b[39m     simulator=\u001b[38;5;28mself\u001b[39m.simulator,\n\u001b[32m    778\u001b[39m     batch_size=batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    781\u001b[39m     augmentations=augmentations,\n\u001b[32m    782\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43monline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_optimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/workflows/basic_workflow.py:954\u001b[39m, in \u001b[36mBasicWorkflow._fit\u001b[39m\u001b[34m(self, dataset, epochs, strategy, keep_optimizer, validation_data, **kwargs)\u001b[39m\n\u001b[32m    951\u001b[39m     \u001b[38;5;28mself\u001b[39m.approximator.compile(optimizer=\u001b[38;5;28mself\u001b[39m.optimizer, metrics=kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m     \u001b[38;5;28mself\u001b[39m.history = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapproximator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    957\u001b[39m     \u001b[38;5;28mself\u001b[39m._on_training_finished()\n\u001b[32m    958\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.history\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/approximators/continuous_approximator.py:316\u001b[39m, in \u001b[36mContinuousApproximator.fit\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    265\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[33;03m    Trains the approximator on the provided dataset or on-demand data generated from the given simulator.\u001b[39;00m\n\u001b[32m    267\u001b[39m \u001b[33;03m    If `dataset` is not provided, a dataset is built from the `simulator`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m \u001b[33;03m        If both `dataset` and `simulator` are provided or neither is provided.\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madapter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/approximators/approximator.py:137\u001b[39m, in \u001b[36mApproximator.fit\u001b[39m\u001b[34m(self, dataset, simulator, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m     mock_data = keras.tree.map_structure(keras.ops.convert_to_tensor, mock_data)\n\u001b[32m    136\u001b[39m     mock_data_shapes = keras.tree.map_structure(keras.ops.shape, mock_data)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_data_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().fit(dataset=dataset, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/keras/src/layers/layer.py:232\u001b[39m, in \u001b[36mLayer.__new__.<locals>.build_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m obj._open_name_scope():\n\u001b[32m    231\u001b[39m     obj._path = current_path()\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[32m    234\u001b[39m signature = inspect.signature(original_build_method)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/approximators/continuous_approximator.py:90\u001b[39m, in \u001b[36mContinuousApproximator.build\u001b[39m\u001b[34m(self, data_shapes)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Build inference network if needed\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.inference_network.built:\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference_network\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_shapes\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minference_variables\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_conditions_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Set up standardization layers if requested\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.standardize == \u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     94\u001b[39m     \u001b[38;5;66;03m# Only include variables present in data_shapes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/keras/src/layers/layer.py:232\u001b[39m, in \u001b[36mLayer.__new__.<locals>.build_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m obj._open_name_scope():\n\u001b[32m    231\u001b[39m     obj._path = current_path()\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[32m    234\u001b[39m signature = inspect.signature(original_build_method)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/networks/coupling_flow/coupling_flow.py:124\u001b[39m, in \u001b[36mCouplingFlow.build\u001b[39m\u001b[34m(self, xz_shape, conditions_shape)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, xz_shape, conditions_shape=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.invertible_layers:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxz_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxz_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconditions_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_distribution.build(xz_shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/keras/src/layers/layer.py:232\u001b[39m, in \u001b[36mLayer.__new__.<locals>.build_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m obj._open_name_scope():\n\u001b[32m    231\u001b[39m     obj._path = current_path()\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[32m    234\u001b[39m signature = inspect.signature(original_build_method)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/networks/coupling_flow/couplings/dual_coupling.py:49\u001b[39m, in \u001b[36mDualCoupling.build\u001b[39m\u001b[34m(self, xz_shape, conditions_shape)\u001b[39m\n\u001b[32m     46\u001b[39m     conditions = keras.ops.zeros(conditions_shape)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# build nested layers with forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/networks/coupling_flow/couplings/dual_coupling.py:56\u001b[39m, in \u001b[36mDualCoupling.call\u001b[39m\u001b[34m(self, xz, conditions, inverse, training, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inverse:\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inverse(xz, conditions=conditions, training=training, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/networks/coupling_flow/couplings/dual_coupling.py:61\u001b[39m, in \u001b[36mDualCoupling._forward\u001b[39m\u001b[34m(self, x, conditions, training, **kwargs)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transform (x1, x2) -> (g(x1; f(x2; x1)), f(x2; x1))\"\"\"\u001b[39;00m\n\u001b[32m     60\u001b[39m x1, x2 = x[..., : \u001b[38;5;28mself\u001b[39m.pivot], x[..., \u001b[38;5;28mself\u001b[39m.pivot :]\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m (z1, z2), log_det1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcoupling1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m (z2, z1), log_det2 = \u001b[38;5;28mself\u001b[39m.coupling2(z2, z1, conditions=conditions, training=training, **kwargs)\n\u001b[32m     64\u001b[39m z = keras.ops.concatenate([z1, z2], axis=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/networks/coupling_flow/couplings/single_coupling.py:79\u001b[39m, in \u001b[36mSingleCoupling.build\u001b[39m\u001b[34m(self, x1_shape, x2_shape, conditions_shape)\u001b[39m\n\u001b[32m     76\u001b[39m     conditions = keras.ops.zeros(conditions_shape)\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# build nested layers with forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/networks/coupling_flow/couplings/single_coupling.py:86\u001b[39m, in \u001b[36mSingleCoupling.call\u001b[39m\u001b[34m(self, x1, x2, conditions, inverse, training, **kwargs)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inverse:\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inverse(x1, x2, conditions=conditions, training=training, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/networks/coupling_flow/couplings/single_coupling.py:93\u001b[39m, in \u001b[36mSingleCoupling._forward\u001b[39m\u001b[34m(self, x1, x2, conditions, training, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transform (x1, x2) -> (x1, f(x2; x1))\"\"\"\u001b[39;00m\n\u001b[32m     92\u001b[39m z1 = x1\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m parameters = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m z2, log_det = \u001b[38;5;28mself\u001b[39m.transform(x2, parameters=parameters)\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (z1, z2), log_det\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/networks/coupling_flow/couplings/single_coupling.py:115\u001b[39m, in \u001b[36mSingleCoupling.get_parameters\u001b[39m\u001b[34m(self, x, conditions, training, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Applies the inner neural network to obtain the transformation parameters, for instance,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03mif affine transformations, then [s, t] = NN(inputs), followed by a constraint, e.g., s = exp(s).\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conditions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     x = \u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m parameters = \u001b[38;5;28mself\u001b[39m.output_projector(\u001b[38;5;28mself\u001b[39m.subnet(x, training=training, **filter_kwargs(kwargs, \u001b[38;5;28mself\u001b[39m.subnet.call)))\n\u001b[32m    118\u001b[39m parameters = \u001b[38;5;28mself\u001b[39m.transform.split_parameters(parameters)\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:GPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [64,1632,1] vs. shape[1] = [64,1664] [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "history = workflow.fit_online(\n",
    "    epochs=50,                  # total passes through the data\n",
    "    num_batches_per_epoch=200,  # batches generated per epoch\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ea948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37148e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec436c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59691d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e8c1db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68df2b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b8913a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9967cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's test the basic ProteinHMMSimulator\n",
    "def test_basic_simulator():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TESTING BASIC PROTEIN HMM SIMULATOR\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize the simulator\n",
    "    simulator = ProteinHMMSimulator()\n",
    "    \n",
    "    # Print basic information about the simulator\n",
    "    print(\"Amino acids:\", simulator.amino_acids)\n",
    "    print(\"States:\", simulator.states)\n",
    "    print(\"Number of states:\", simulator.n_states)\n",
    "    print(\"Number of features (amino acids):\", simulator.n_features)\n",
    "    print()\n",
    "    \n",
    "    # Show emission probabilities\n",
    "    print(\"Emission Probabilities:\")\n",
    "    print(\"Shape:\", simulator.emission_probs.shape)\n",
    "    emission_df = pd.DataFrame(\n",
    "        simulator.emission_probs.T,\n",
    "        index=simulator.amino_acids,\n",
    "        columns=simulator.states\n",
    "    )\n",
    "    print(emission_df.round(4))\n",
    "    print()\n",
    "    \n",
    "    # Show transition probabilities\n",
    "    print(\"Transition Probabilities:\")\n",
    "    transition_df = pd.DataFrame(\n",
    "        simulator.transition_probs,\n",
    "        index=simulator.states,\n",
    "        columns=simulator.states\n",
    "    )\n",
    "    print(transition_df)\n",
    "    print()\n",
    "    \n",
    "    # Show start probabilities\n",
    "    print(\"Start Probabilities:\")\n",
    "    start_df = pd.DataFrame(\n",
    "        simulator.start_probs.reshape(1, -1),\n",
    "        columns=simulator.states\n",
    "    )\n",
    "    print(start_df)\n",
    "    print()\n",
    "\n",
    "def test_sequence_generation():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TESTING SEQUENCE GENERATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    simulator = ProteinHMMSimulator()\n",
    "    \n",
    "    # Generate a few sequences of different lengths\n",
    "    lengths = [20, 50, 100]\n",
    "    \n",
    "    for length in lengths:\n",
    "        print(f\"\\nGenerating sequence of length {length}:\")\n",
    "        aa_seq, state_seq = simulator.generate_sequence(length)\n",
    "        \n",
    "        print(f\"Amino acid sequence: {''.join(aa_seq)}\")\n",
    "        print(f\"State sequence: {state_seq}\")\n",
    "        print(f\"State names: {[simulator.states[s] for s in state_seq]}\")\n",
    "        \n",
    "        # Count states\n",
    "        other_count = np.sum(state_seq == 0)\n",
    "        alpha_count = np.sum(state_seq == 1)\n",
    "        print(f\"State counts - Other: {other_count}, Alpha-helix: {alpha_count}\")\n",
    "        print(f\"Alpha-helix percentage: {alpha_count/length*100:.1f}%\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "def test_state_probabilities():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TESTING STATE PROBABILITY CALCULATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    simulator = ProteinHMMSimulator()\n",
    "    \n",
    "    # Generate a sequence\n",
    "    aa_seq, true_states = simulator.generate_sequence(30)\n",
    "    \n",
    "    print(f\"Generated sequence: {''.join(aa_seq)}\")\n",
    "    print(f\"True states: {true_states}\")\n",
    "    print(f\"True state names: {[simulator.states[s] for s in true_states]}\")\n",
    "    print()\n",
    "    \n",
    "    # Get state probabilities using Forward-Backward algorithm\n",
    "    state_probs = simulator.get_state_probabilities(aa_seq)\n",
    "    \n",
    "    print(\"State Probabilities (Forward-Backward):\")\n",
    "    print(\"Shape:\", state_probs.shape)\n",
    "    \n",
    "    # Create a detailed view\n",
    "    prob_df = pd.DataFrame({\n",
    "        'Position': range(len(aa_seq)),\n",
    "        'AA': aa_seq,\n",
    "        'True_State': [simulator.states[s] for s in true_states],\n",
    "        'P(Other)': state_probs[:, 0],\n",
    "        'P(Alpha-helix)': state_probs[:, 1],\n",
    "        'Predicted_State': [simulator.states[np.argmax(probs)] for probs in state_probs]\n",
    "    })\n",
    "    \n",
    "    print(prob_df.round(4))\n",
    "    print()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    predicted_states = np.argmax(state_probs, axis=1)\n",
    "    accuracy = np.mean(predicted_states == true_states)\n",
    "    print(f\"Prediction accuracy: {accuracy:.2f}\")\n",
    "\n",
    "def test_batch_generation():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TESTING BATCH GENERATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    simulator = ProteinHMMSimulator()\n",
    "    \n",
    "    # Generate a batch\n",
    "    batch_size = 5\n",
    "    batch_data = simulator.sample_batch(batch_size, min_length=20, max_length=50)\n",
    "    \n",
    "    print(f\"Generated batch with {batch_size} sequences:\")\n",
    "    print(f\"Keys in batch data: {list(batch_data.keys())}\")\n",
    "    print()\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        aa_seq = batch_data['amino_acid_sequences'][i]\n",
    "        state_probs = batch_data['state_probabilities'][i]\n",
    "        seq_len = batch_data['sequence_lengths'][i]\n",
    "        \n",
    "        print(f\"Sequence {i+1}:\")\n",
    "        print(f\"  Length: {seq_len}\")\n",
    "        print(f\"  Amino acids: {''.join(aa_seq)}\")\n",
    "        print(f\"  State probabilities shape: {state_probs.shape}\")\n",
    "        print(f\"  Average alpha-helix probability: {np.mean(state_probs[:, 1]):.3f}\")\n",
    "        print()\n",
    "\n",
    "def test_bayesflow_simulator():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TESTING BAYESFLOW SIMULATOR\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize BayesFlow simulator\n",
    "    bf_simulator = BayesFlowProteinSimulator(max_length=100, min_length=20)\n",
    "    \n",
    "    print(\"BayesFlow Simulator initialized:\")\n",
    "    print(f\"  Max length: {bf_simulator.max_length}\")\n",
    "    print(f\"  Min length: {bf_simulator.min_length}\")\n",
    "    print(f\"  Pad token: {bf_simulator.pad_token}\")\n",
    "    print()\n",
    "    \n",
    "    # Generate a small batch\n",
    "    batch_size = 3\n",
    "    batch_data = bf_simulator.sample(batch_size)\n",
    "    \n",
    "    print(f\"Generated BayesFlow batch with {batch_size} sequences:\")\n",
    "    print(f\"Keys in batch data: {list(batch_data.keys())}\")\n",
    "    print()\n",
    "    \n",
    "    for key, value in batch_data.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            print(f\"  {key}: shape = {value.shape}, dtype = {value.dtype}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {type(value)}\")\n",
    "    print()\n",
    "    \n",
    "    # Show details for first sequence\n",
    "    print(\"Details for first sequence:\")\n",
    "    seq_idx = 0\n",
    "    encoded_seq = batch_data['amino_acid_sequences'][seq_idx]\n",
    "    state_probs = batch_data['state_probabilities'][seq_idx]\n",
    "    seq_len = batch_data['sequence_lengths'][seq_idx]\n",
    "    alpha_probs = batch_data['alpha_helix_probs'][seq_idx]\n",
    "    mask = batch_data['mask'][seq_idx]\n",
    "    \n",
    "    print(f\"  Sequence length: {seq_len}\")\n",
    "    print(f\"  Encoded sequence (first 20): {encoded_seq[:20]}\")\n",
    "    print(f\"  Valid positions (mask sum): {np.sum(mask)}\")\n",
    "    print(f\"  State probabilities shape: {state_probs.shape}\")\n",
    "    print(f\"  Alpha-helix probabilities shape: {alpha_probs.shape}\")\n",
    "    \n",
    "    # Show valid part of sequence\n",
    "    valid_encoded = encoded_seq[:seq_len]\n",
    "    valid_alpha_probs = alpha_probs[:seq_len]\n",
    "    \n",
    "    # Convert back to amino acids (excluding padding)\n",
    "    valid_amino_acids = []\n",
    "    for idx in valid_encoded:\n",
    "        if idx < len(bf_simulator.protein_hmm.amino_acids):\n",
    "            valid_amino_acids.append(bf_simulator.protein_hmm.amino_acids[idx])\n",
    "        else:\n",
    "            valid_amino_acids.append('PAD')\n",
    "    \n",
    "    print(f\"  Valid amino acids: {''.join(valid_amino_acids)}\")\n",
    "    print(f\"  Valid alpha-helix probs (first 10): {valid_alpha_probs[:10].round(3)}\")\n",
    "    print(f\"  Average alpha-helix probability: {np.mean(valid_alpha_probs):.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run all tests\n",
    "    test_basic_simulator()\n",
    "    test_sequence_generation()\n",
    "    test_state_probabilities()\n",
    "    test_batch_generation()\n",
    "    test_bayesflow_simulator()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ALL TESTS COMPLETED!\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fc4620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def visualize_emission_probabilities():\n",
    "    \"\"\"Visualize the emission probabilities for both states.\"\"\"\n",
    "    simulator = ProteinHMMSimulator()\n",
    "    \n",
    "    # Create a heatmap of emission probabilities\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Emission probabilities as DataFrame\n",
    "    emission_df = pd.DataFrame(\n",
    "        simulator.emission_probs.T,\n",
    "        index=simulator.amino_acids,\n",
    "        columns=simulator.states\n",
    "    )\n",
    "    \n",
    "    # Heatmap\n",
    "    sns.heatmap(emission_df, annot=True, fmt='.3f', cmap='Blues', ax=ax1)\n",
    "    ax1.set_title('Emission Probabilities by State')\n",
    "    ax1.set_xlabel('State')\n",
    "    ax1.set_ylabel('Amino Acid')\n",
    "    \n",
    "    # Bar plot comparison\n",
    "    x = np.arange(len(simulator.amino_acids))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax2.bar(x - width/2, emission_df['other'], width, label='Other', alpha=0.7)\n",
    "    ax2.bar(x + width/2, emission_df['alpha-helix'], width, label='Alpha-helix', alpha=0.7)\n",
    "    \n",
    "    ax2.set_xlabel('Amino Acid')\n",
    "    ax2.set_ylabel('Emission Probability')\n",
    "    ax2.set_title('Emission Probabilities Comparison')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(simulator.amino_acids)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_sequence_example():\n",
    "    \"\"\"Visualize a generated sequence and its state probabilities.\"\"\"\n",
    "    simulator = ProteinHMMSimulator()\n",
    "    \n",
    "    # Generate a sequence\n",
    "    aa_seq, true_states = simulator.generate_sequence(100)\n",
    "    state_probs = simulator.get_state_probabilities(aa_seq)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: True states\n",
    "    positions = np.arange(len(aa_seq))\n",
    "    colors = ['lightblue' if s == 0 else 'lightcoral' for s in true_states]\n",
    "    ax1.bar(positions, np.ones(len(positions)), color=colors, alpha=0.7)\n",
    "    ax1.set_title('True Secondary Structure States')\n",
    "    ax1.set_ylabel('State')\n",
    "    ax1.set_yticks([0, 1])\n",
    "    ax1.set_yticklabels(['', 'Other/Alpha-helix'])\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add amino acid labels\n",
    "    for i, aa in enumerate(aa_seq):\n",
    "        ax1.text(i, 0.5, aa, ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # Plot 2: State probabilities\n",
    "    ax2.plot(positions, state_probs[:, 0], label='P(Other)', color='blue', linewidth=2)\n",
    "    ax2.plot(positions, state_probs[:, 1], label='P(Alpha-helix)', color='red', linewidth=2)\n",
    "    ax2.fill_between(positions, state_probs[:, 0], alpha=0.3, color='blue')\n",
    "    ax2.fill_between(positions, state_probs[:, 1], alpha=0.3, color='red')\n",
    "    ax2.set_title('State Probabilities (Forward-Backward Algorithm)')\n",
    "    ax2.set_ylabel('Probability')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Plot 3: Prediction accuracy\n",
    "    predicted_states = np.argmax(state_probs, axis=1)\n",
    "    correct = (predicted_states == true_states).astype(int)\n",
    "    ax3.bar(positions, correct, color=['green' if c else 'red' for c in correct], alpha=0.7)\n",
    "    ax3.set_title('Prediction Accuracy (Green=Correct, Red=Incorrect)')\n",
    "    ax3.set_xlabel('Position')\n",
    "    ax3.set_ylabel('Correct')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    overall_accuracy = np.mean(correct)\n",
    "    ax3.text(0.02, 0.95, f'Overall Accuracy: {overall_accuracy:.2%}', \n",
    "             transform=ax3.transAxes, fontsize=12, \n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return aa_seq, true_states, state_probs, overall_accuracy\n",
    "\n",
    "def visualize_batch_statistics():\n",
    "    \"\"\"Visualize statistics from a batch of sequences.\"\"\"\n",
    "    simulator = ProteinHMMSimulator()\n",
    "    \n",
    "    # Generate a larger batch\n",
    "    batch_size = 50\n",
    "    batch_data = simulator.sample_batch(batch_size, min_length=50, max_length=200)\n",
    "    \n",
    "    # Extract statistics\n",
    "    sequence_lengths = batch_data['sequence_lengths']\n",
    "    alpha_helix_percentages = []\n",
    "    avg_alpha_probs = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        state_probs = batch_data['state_probabilities'][i]\n",
    "        alpha_probs = state_probs[:, 1]\n",
    "        \n",
    "        # Calculate percentage of positions with high alpha-helix probability\n",
    "        alpha_helix_pct = np.mean(alpha_probs > 0.5) * 100\n",
    "        alpha_helix_percentages.append(alpha_helix_pct)\n",
    "        \n",
    "        # Average alpha-helix probability\n",
    "        avg_alpha_probs.append(np.mean(alpha_probs))\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Sequence lengths\n",
    "    ax1.hist(sequence_lengths, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.set_title('Distribution of Sequence Lengths')\n",
    "    ax1.set_xlabel('Sequence Length')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Alpha-helix percentages\n",
    "    ax2.hist(alpha_helix_percentages, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    ax2.set_title('Distribution of Alpha-helix Percentages')\n",
    "    ax2.set_xlabel('Alpha-helix Percentage (%)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Average alpha-helix probabilities\n",
    "    ax3.hist(avg_alpha_probs, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    ax3.set_title('Distribution of Average Alpha-helix Probabilities')\n",
    "    ax3.set_xlabel('Average Alpha-helix Probability')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Relationship between length and alpha-helix content\n",
    "    ax4.scatter(sequence_lengths, alpha_helix_percentages, alpha=0.6, color='purple')\n",
    "    ax4.set_title('Sequence Length vs Alpha-helix Content')\n",
    "    ax4.set_xlabel('Sequence Length')\n",
    "    ax4.set_ylabel('Alpha-helix Percentage (%)')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    correlation = np.corrcoef(sequence_lengths, alpha_helix_percentages)[0, 1]\n",
    "    ax4.text(0.02, 0.95, f'Correlation: {correlation:.3f}', \n",
    "             transform=ax4.transAxes, fontsize=12,\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"BATCH STATISTICS SUMMARY:\")\n",
    "    print(f\"Number of sequences: {batch_size}\")\n",
    "    print(f\"Sequence lengths - Mean: {np.mean(sequence_lengths):.1f}, Std: {np.std(sequence_lengths):.1f}\")\n",
    "    print(f\"Alpha-helix percentages - Mean: {np.mean(alpha_helix_percentages):.1f}%, Std: {np.std(alpha_helix_percentages):.1f}%\")\n",
    "    print(f\"Average alpha-helix probabilities - Mean: {np.mean(avg_alpha_probs):.3f}, Std: {np.std(avg_alpha_probs):.3f}\")\n",
    "    print(f\"Correlation (length vs alpha-helix): {correlation:.3f}\")\n",
    "\n",
    "def compare_amino_acid_distributions():\n",
    "    \"\"\"Compare amino acid distributions between different states.\"\"\"\n",
    "    simulator = ProteinHMMSimulator()\n",
    "    \n",
    "    # Generate a large batch to get good statistics\n",
    "    batch_size = 100\n",
    "    batch_data = simulator.sample_batch(batch_size, min_length=100, max_length=200)\n",
    "    \n",
    "    # Collect amino acids by predicted state\n",
    "    other_amino_acids = []\n",
    "    alpha_amino_acids = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        aa_seq = batch_data['amino_acid_sequences'][i]\n",
    "        state_probs = batch_data['state_probabilities'][i]\n",
    "        \n",
    "        # Classify positions based on highest probability\n",
    "        predicted_states = np.argmax(state_probs, axis=1)\n",
    "        \n",
    "        for j, aa in enumerate(aa_seq):\n",
    "            if predicted_states[j] == 0:\n",
    "                other_amino_acids.append(aa)\n",
    "            else:\n",
    "                alpha_amino_acids.append(aa)\n",
    "    \n",
    "    # Count amino acids\n",
    "    other_counts = pd.Series(other_amino_acids).value_counts()\n",
    "    alpha_counts = pd.Series(alpha_amino_acids).value_counts()\n",
    "    \n",
    "    # Normalize to percentages\n",
    "    other_pct = other_counts / len(other_amino_acids) * 100\n",
    "    alpha_pct = alpha_counts / len(alpha_amino_acids) * 100\n",
    "    \n",
    "    # Create comparison plot\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    \n",
    "    amino_acids = simulator.amino_acids\n",
    "    x = np.arange(len(amino_acids))\n",
    "    width = 0.35\n",
    "    \n",
    "    other_values = [other_pct.get(aa, 0) for aa in amino_acids]\n",
    "    alpha_values = [alpha_pct.get(aa, 0) for aa in amino_acids]\n",
    "    \n",
    "    ax.bar(x - width/2, other_values, width, label='Other State', alpha=0.7, color='blue')\n",
    "    ax.bar(x + width/2, alpha_values, width, label='Alpha-helix State', alpha=0.7, color='red')\n",
    "    \n",
    "    ax.set_xlabel('Amino Acid')\n",
    "    ax.set_ylabel('Percentage (%)')\n",
    "    ax.set_title('Amino Acid Distribution by Predicted State')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(amino_acids)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add difference annotation\n",
    "    for i, aa in enumerate(amino_acids):\n",
    "        diff = alpha_values[i] - other_values[i]\n",
    "        if abs(diff) > 0.5:  # Only show significant differences\n",
    "            ax.annotate(f'{diff:+.1f}', xy=(i, max(other_values[i], alpha_values[i]) + 0.2),\n",
    "                       ha='center', va='bottom', fontsize=8, \n",
    "                       color='green' if diff > 0 else 'red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"AMINO ACID DISTRIBUTION ANALYSIS:\")\n",
    "    print(f\"Total positions analyzed: {len(other_amino_acids) + len(alpha_amino_acids)}\")\n",
    "    print(f\"Other state positions: {len(other_amino_acids)}\")\n",
    "    print(f\"Alpha-helix state positions: {len(alpha_amino_acids)}\")\n",
    "    print(\"\\nTop 5 amino acids in each state:\")\n",
    "    print(\"Other state:\", other_pct.head().round(2).to_dict())\n",
    "    print(\"Alpha-helix state:\", alpha_pct.head().round(2).to_dict())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting protein HMM data visualization...\")\n",
    "    \n",
    "    # Run visualizations\n",
    "    print(\"\\n1. Visualizing emission probabilities...\")\n",
    "    visualize_emission_probabilities()\n",
    "    \n",
    "    print(\"\\n2. Visualizing sequence example...\")\n",
    "    aa_seq, true_states, state_probs, accuracy = visualize_sequence_example()\n",
    "    print(f\"Example sequence accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    print(\"\\n3. Visualizing batch statistics...\")\n",
    "    visualize_batch_statistics()\n",
    "    \n",
    "    print(\"\\n4. Comparing amino acid distributions...\")\n",
    "    compare_amino_acid_distributions()\n",
    "    \n",
    "    print(\"\\nAll visualizations completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3364ea20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
