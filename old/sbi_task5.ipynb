{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c319567e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 23:04:15.518696: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-07-08 23:04:15.518721: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-07-08 23:04:15.518726: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752008655.518737 5236854 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1752008655.518755 5236854 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "INFO:bayesflow:Using backend 'tensorflow'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesFlow version: 2.0.5\n",
      "hmmlearn version: 0.3.3\n",
      "Scientific computing libraries loaded successfully\n",
      "TensorFlow version: 2.19.0\n",
      "GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU configuration error: Physical devices cannot be modified after being initialized\n",
      "==================================================\n",
      "TASK 5: PROTEIN SECONDARY STRUCTURE INFERENCE\n",
      "==================================================\n",
      "Environment setup completed successfully!\n",
      "Configuration: {'random_seed': 42, 'n_states': 2, 'n_amino_acids': 20, 'sequence_length_range': (50, 200), 'batch_size': 64, 'n_train_samples': 10000, 'n_val_samples': 2000, 'n_test_samples': 1000, 'verbose': True}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Environment Setup and Import Libraries\n",
    "# Task 5: Inference of protein secondary structure motifs using BayesFlow\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# BayesFlow imports\n",
    "try:\n",
    "    import bayesflow as bf\n",
    "    print(f\"BayesFlow version: {bf.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"BayesFlow not installed. Please install with: pip install bayesflow\")\n",
    "    raise\n",
    "\n",
    "# hmmlearn imports\n",
    "try:\n",
    "    from hmmlearn import hmm\n",
    "    import hmmlearn\n",
    "    print(f\"hmmlearn version: {hmmlearn.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"hmmlearn not installed. Please install with: pip install hmmlearn\")\n",
    "    raise\n",
    "\n",
    "# Additional scientific computing libraries\n",
    "try:\n",
    "    import scipy.stats as stats\n",
    "    from scipy.special import logsumexp\n",
    "    import sklearn.metrics as metrics\n",
    "    print(\"Scientific computing libraries loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Missing scientific computing library: {e}\")\n",
    "    raise\n",
    "\n",
    "# Set TensorFlow backend for BayesFlow\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Configure TensorFlow for optimal performance\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Global configuration\n",
    "CONFIG = {\n",
    "    'random_seed': RANDOM_SEED,\n",
    "    'n_states': 2,  # alpha-helix and other\n",
    "    'n_amino_acids': 20,\n",
    "    'sequence_length_range': (50, 200),  # Variable length sequences\n",
    "    'batch_size': 64,\n",
    "    'n_train_samples': 10000,\n",
    "    'n_val_samples': 2000,\n",
    "    'n_test_samples': 1000,\n",
    "    'verbose': True\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TASK 5: PROTEIN SECONDARY STRUCTURE INFERENCE\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Environment setup completed successfully!\")\n",
    "print(f\"Configuration: {CONFIG}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54afc874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amino Acid Mapping:\n",
      "AA_TO_INDEX: {'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'E': 5, 'Q': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19}\n",
      "INDEX_TO_AA: {0: 'A', 1: 'R', 2: 'N', 3: 'D', 4: 'C', 5: 'E', 6: 'Q', 7: 'G', 8: 'H', 9: 'I', 10: 'L', 11: 'K', 12: 'M', 13: 'F', 14: 'P', 15: 'S', 16: 'T', 17: 'W', 18: 'Y', 19: 'V'}\n",
      "\n",
      "Emission Probabilities:\n",
      "Alpha-helix state probabilities sum: 0.9999999999999998\n",
      "Other state probabilities sum: 0.9999999999999998\n",
      "Emission matrix shape: (2, 20)\n",
      "\n",
      "Transition Probabilities:\n",
      "Transition matrix:\n",
      "From alpha-helix: alpha-helix=90%, other=10%\n",
      "From other: alpha-helix=5%, other=95%\n",
      "Transition matrix shape: (2, 2)\n",
      "Row sums (should be 1.0): [1. 1.]\n",
      "\n",
      "Initial State Probabilities:\n",
      "Always start in 'other' state: [0. 1.]\n",
      "\n",
      "State Mapping:\n",
      "STATE_TO_INDEX: {'alpha-helix': 0, 'other': 1}\n",
      "INDEX_TO_STATE: {0: 'alpha-helix', 1: 'other'}\n",
      "\n",
      "Emission Probabilities Summary:\n",
      "   Amino_Acid  Alpha_Helix  Other\n",
      "0           A         0.12   0.06\n",
      "1           R         0.06   0.05\n",
      "2           N         0.03   0.05\n",
      "3           D         0.05   0.06\n",
      "4           C         0.01   0.02\n",
      "5           E         0.09   0.05\n",
      "6           Q         0.05   0.03\n",
      "7           G         0.04   0.09\n",
      "8           H         0.02   0.03\n",
      "9           I         0.07   0.05\n",
      "10          L         0.12   0.08\n",
      "11          K         0.06   0.06\n",
      "12          M         0.03   0.02\n",
      "13          F         0.04   0.04\n",
      "14          P         0.02   0.06\n",
      "15          S         0.05   0.07\n",
      "16          T         0.04   0.06\n",
      "17          W         0.01   0.01\n",
      "18          Y         0.03   0.04\n",
      "19          V         0.06   0.07\n",
      "\n",
      "==================================================\n",
      "PROBABILITY VERIFICATION\n",
      "==================================================\n",
      "Alpha-helix emissions sum: 1.000000\n",
      "Other emissions sum: 1.000000\n",
      "Transition matrix row sums: [1. 1.]\n",
      "Initial probabilities sum: 1.000000\n",
      "All probabilities valid: True\n",
      "==================================================\n",
      "Data preparation completed successfully!\n",
      "Data configuration keys: ['amino_acids', 'aa_to_index', 'index_to_aa', 'emission_probs', 'transition_probs', 'initial_probs', 'state_names', 'state_to_index', 'index_to_state', 'n_states', 'n_amino_acids', 'valid_probabilities']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Data Preparation and Constants\n",
    "# Define amino acid alphabet, emission probabilities, and transition probabilities\n",
    "\n",
    "# Standard 20 amino acids (single letter codes)\n",
    "AMINO_ACIDS = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I', \n",
    "               'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "\n",
    "# Create mapping dictionaries\n",
    "AA_TO_INDEX = {aa: idx for idx, aa in enumerate(AMINO_ACIDS)}\n",
    "INDEX_TO_AA = {idx: aa for idx, aa in enumerate(AMINO_ACIDS)}\n",
    "\n",
    "print(\"Amino Acid Mapping:\")\n",
    "print(\"AA_TO_INDEX:\", AA_TO_INDEX)\n",
    "print(\"INDEX_TO_AA:\", INDEX_TO_AA)\n",
    "\n",
    "# Define emission probabilities as specified in the task\n",
    "# Alpha-helix emission probabilities (state 0)\n",
    "ALPHA_HELIX_EMISSIONS = {\n",
    "    'A': 0.12, 'R': 0.06, 'N': 0.03, 'D': 0.05, 'C': 0.01,\n",
    "    'E': 0.09, 'Q': 0.05, 'G': 0.04, 'H': 0.02, 'I': 0.07,\n",
    "    'L': 0.12, 'K': 0.06, 'M': 0.03, 'F': 0.04, 'P': 0.02,\n",
    "    'S': 0.05, 'T': 0.04, 'W': 0.01, 'Y': 0.03, 'V': 0.06\n",
    "}\n",
    "\n",
    "# Other (beta-sheet/coil) emission probabilities (state 1)\n",
    "OTHER_EMISSIONS = {\n",
    "    'A': 0.06, 'R': 0.05, 'N': 0.05, 'D': 0.06, 'C': 0.02,\n",
    "    'E': 0.05, 'Q': 0.03, 'G': 0.09, 'H': 0.03, 'I': 0.05,\n",
    "    'L': 0.08, 'K': 0.06, 'M': 0.02, 'F': 0.04, 'P': 0.06,\n",
    "    'S': 0.07, 'T': 0.06, 'W': 0.01, 'Y': 0.04, 'V': 0.07\n",
    "}\n",
    "\n",
    "# Convert to numpy arrays in the correct order\n",
    "alpha_helix_probs = np.array([ALPHA_HELIX_EMISSIONS[aa] for aa in AMINO_ACIDS])\n",
    "other_probs = np.array([OTHER_EMISSIONS[aa] for aa in AMINO_ACIDS])\n",
    "\n",
    "# Normalize to ensure they sum to 1 (handle any rounding errors)\n",
    "alpha_helix_probs = alpha_helix_probs / np.sum(alpha_helix_probs)\n",
    "other_probs = other_probs / np.sum(other_probs)\n",
    "\n",
    "# Create emission probability matrix (2 states x 20 amino acids)\n",
    "EMISSION_PROBS = np.array([alpha_helix_probs, other_probs])\n",
    "\n",
    "print(\"\\nEmission Probabilities:\")\n",
    "print(\"Alpha-helix state probabilities sum:\", np.sum(alpha_helix_probs))\n",
    "print(\"Other state probabilities sum:\", np.sum(other_probs))\n",
    "print(\"Emission matrix shape:\", EMISSION_PROBS.shape)\n",
    "\n",
    "# Define transition probabilities as specified in the task\n",
    "# States: 0 = alpha-helix, 1 = other\n",
    "# Transition matrix: rows = from state, columns = to state\n",
    "TRANSITION_PROBS = np.array([\n",
    "    [0.90, 0.10],  # From alpha-helix: 90% stay, 10% to other\n",
    "    [0.05, 0.95]   # From other: 5% to alpha-helix, 95% stay\n",
    "])\n",
    "\n",
    "print(\"\\nTransition Probabilities:\")\n",
    "print(\"Transition matrix:\")\n",
    "print(\"From alpha-helix: alpha-helix=90%, other=10%\")\n",
    "print(\"From other: alpha-helix=5%, other=95%\")\n",
    "print(\"Transition matrix shape:\", TRANSITION_PROBS.shape)\n",
    "print(\"Row sums (should be 1.0):\", np.sum(TRANSITION_PROBS, axis=1))\n",
    "\n",
    "# Initial state probabilities (always start in \"other\" state)\n",
    "INITIAL_PROBS = np.array([0.0, 1.0])  # [alpha-helix, other]\n",
    "\n",
    "print(\"\\nInitial State Probabilities:\")\n",
    "print(\"Always start in 'other' state:\", INITIAL_PROBS)\n",
    "\n",
    "# State names for interpretability\n",
    "STATE_NAMES = ['alpha-helix', 'other']\n",
    "STATE_TO_INDEX = {'alpha-helix': 0, 'other': 1}\n",
    "INDEX_TO_STATE = {0: 'alpha-helix', 1: 'other'}\n",
    "\n",
    "print(\"\\nState Mapping:\")\n",
    "print(\"STATE_TO_INDEX:\", STATE_TO_INDEX)\n",
    "print(\"INDEX_TO_STATE:\", INDEX_TO_STATE)\n",
    "\n",
    "# Create a summary dataframe for emission probabilities\n",
    "emission_df = pd.DataFrame({\n",
    "    'Amino_Acid': AMINO_ACIDS,\n",
    "    'Alpha_Helix': alpha_helix_probs,\n",
    "    'Other': other_probs\n",
    "})\n",
    "\n",
    "print(\"\\nEmission Probabilities Summary:\")\n",
    "print(emission_df.round(3))\n",
    "\n",
    "# Verify probabilities are valid\n",
    "def verify_probabilities():\n",
    "    \"\"\"Verify that all probability matrices are valid\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PROBABILITY VERIFICATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check emission probabilities\n",
    "    alpha_sum = np.sum(alpha_helix_probs)\n",
    "    other_sum = np.sum(other_probs)\n",
    "    print(f\"Alpha-helix emissions sum: {alpha_sum:.6f}\")\n",
    "    print(f\"Other emissions sum: {other_sum:.6f}\")\n",
    "    \n",
    "    # Check transition probabilities\n",
    "    trans_sums = np.sum(TRANSITION_PROBS, axis=1)\n",
    "    print(f\"Transition matrix row sums: {trans_sums}\")\n",
    "    \n",
    "    # Check initial probabilities\n",
    "    init_sum = np.sum(INITIAL_PROBS)\n",
    "    print(f\"Initial probabilities sum: {init_sum:.6f}\")\n",
    "    \n",
    "    # Verify all are valid\n",
    "    valid = (abs(alpha_sum - 1.0) < 1e-10 and \n",
    "             abs(other_sum - 1.0) < 1e-10 and \n",
    "             abs(init_sum - 1.0) < 1e-10 and \n",
    "             all(abs(s - 1.0) < 1e-10 for s in trans_sums))\n",
    "    \n",
    "    print(f\"All probabilities valid: {valid}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return valid\n",
    "\n",
    "# Run verification\n",
    "probability_validation = verify_probabilities()\n",
    "\n",
    "# Store all constants in a configuration dictionary\n",
    "DATA_CONFIG = {\n",
    "    'amino_acids': AMINO_ACIDS,\n",
    "    'aa_to_index': AA_TO_INDEX,\n",
    "    'index_to_aa': INDEX_TO_AA,\n",
    "    'emission_probs': EMISSION_PROBS,\n",
    "    'transition_probs': TRANSITION_PROBS,\n",
    "    'initial_probs': INITIAL_PROBS,\n",
    "    'state_names': STATE_NAMES,\n",
    "    'state_to_index': STATE_TO_INDEX,\n",
    "    'index_to_state': INDEX_TO_STATE,\n",
    "    'n_states': len(STATE_NAMES),\n",
    "    'n_amino_acids': len(AMINO_ACIDS),\n",
    "    'valid_probabilities': probability_validation\n",
    "}\n",
    "\n",
    "print(\"Data preparation completed successfully!\")\n",
    "print(f\"Data configuration keys: {list(DATA_CONFIG.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e22505a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized ProteinHMM with 2 states and 20 features\n",
      "State names: ['alpha-helix', 'other']\n",
      "\n",
      "==================================================\n",
      "TESTING HMM FUNCTIONALITY\n",
      "==================================================\n",
      "Generated sequence (length 50):\n",
      "Sequence: VKNTPVDNIERQFIARVSRILWKINSYYDEEELPVNSMAALAELIKLIGA\n",
      "True states: [1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "State names: ['other', 'other', 'other', 'other', 'other', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'other', 'other', 'other', 'other', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'other', 'other', 'other', 'other', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix']\n",
      "\n",
      "Predicted states (Viterbi): [1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Predicted state names: ['other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix', 'alpha-helix']\n",
      "\n",
      "State probabilities shape: (50, 2)\n",
      "First 5 positions state probabilities:\n",
      "Position 0: alpha-helix=0.000, other=1.000\n",
      "Position 1: alpha-helix=0.018, other=0.982\n",
      "Position 2: alpha-helix=0.026, other=0.974\n",
      "Position 3: alpha-helix=0.037, other=0.963\n",
      "Position 4: alpha-helix=0.052, other=0.948\n",
      "\n",
      "Sequence log-likelihood: -142.058\n",
      "Viterbi accuracy vs true states: 0.780\n",
      "\n",
      "Model Information:\n",
      "Number of states: 2\n",
      "Number of features: 20\n",
      "Initial probabilities: [0. 1.]\n",
      "Transition matrix diagonal: [0.9  0.95]\n",
      "\n",
      "==================================================\n",
      "TESTING BATCH GENERATION\n",
      "==================================================\n",
      "Generated 5 test sequences\n",
      "Sequence lengths: [26, 23, 30, 27, 24]\n",
      "State probability shapes: [(26, 2), (23, 2), (30, 2), (27, 2), (24, 2)]\n",
      "\n",
      "Step 3 completed successfully!\n",
      "HMM model is ready for sequence generation and state inference\n"
     ]
    }
   ],
   "source": [
    "# Step 3: HMM Model Setup with hmmlearn\n",
    "# Initialize CategoricalHMM and implement helper functions\n",
    "\n",
    "from hmmlearn.hmm import CategoricalHMM\n",
    "import numpy as np\n",
    "\n",
    "class ProteinHMM:\n",
    "    \"\"\"\n",
    "    Protein secondary structure HMM using hmmlearn's CategoricalHMM\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, emission_probs, transition_probs, initial_probs, \n",
    "                 aa_to_index, index_to_aa, state_names):\n",
    "        \"\"\"\n",
    "        Initialize the HMM with fixed probabilities\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        emission_probs : np.ndarray\n",
    "            Emission probability matrix (n_states x n_features)\n",
    "        transition_probs : np.ndarray\n",
    "            Transition probability matrix (n_states x n_states)\n",
    "        initial_probs : np.ndarray\n",
    "            Initial state probabilities\n",
    "        aa_to_index : dict\n",
    "            Mapping from amino acid to index\n",
    "        index_to_aa : dict\n",
    "            Mapping from index to amino acid\n",
    "        state_names : list\n",
    "            Names of the states\n",
    "        \"\"\"\n",
    "        self.n_states = len(state_names)\n",
    "        self.n_features = len(aa_to_index)\n",
    "        self.aa_to_index = aa_to_index\n",
    "        self.index_to_aa = index_to_aa\n",
    "        self.state_names = state_names\n",
    "        \n",
    "        # Initialize CategoricalHMM\n",
    "        self.model = CategoricalHMM(\n",
    "            n_components=self.n_states,\n",
    "            random_state=CONFIG['random_seed'],\n",
    "            init_params=\"\",  # Don't initialize parameters, we'll set them manually\n",
    "            params=\"\"        # Don't update parameters during fitting\n",
    "        )\n",
    "        \n",
    "        # Set the probabilities\n",
    "        self.model.startprob_ = initial_probs.copy()\n",
    "        self.model.transmat_ = transition_probs.copy()\n",
    "        self.model.emissionprob_ = emission_probs.copy()\n",
    "        \n",
    "        print(f\"Initialized ProteinHMM with {self.n_states} states and {self.n_features} features\")\n",
    "        print(f\"State names: {self.state_names}\")\n",
    "        \n",
    "    def sequence_to_indices(self, sequence):\n",
    "        \"\"\"Convert amino acid sequence to indices\"\"\"\n",
    "        try:\n",
    "            return np.array([self.aa_to_index[aa] for aa in sequence])\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Unknown amino acid: {e}\")\n",
    "    \n",
    "    def indices_to_sequence(self, indices):\n",
    "        \"\"\"Convert indices back to amino acid sequence\"\"\"\n",
    "        return ''.join([self.index_to_aa[idx] for idx in indices])\n",
    "    \n",
    "    def generate_sequence(self, length):\n",
    "        \"\"\"\n",
    "        Generate a random amino acid sequence of given length\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        length : int\n",
    "            Length of sequence to generate\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        sequence : str\n",
    "            Generated amino acid sequence\n",
    "        states : np.ndarray\n",
    "            True hidden states for the sequence\n",
    "        \"\"\"\n",
    "        # Generate sequence using the HMM\n",
    "        sequence_indices, states = self.model.sample(length)\n",
    "        \n",
    "        # Convert to amino acid sequence\n",
    "        sequence = self.indices_to_sequence(sequence_indices.flatten())\n",
    "        \n",
    "        return sequence, states.flatten()\n",
    "    \n",
    "    def predict_states(self, sequence):\n",
    "        \"\"\"\n",
    "        Predict the most likely state sequence using Viterbi algorithm\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        sequence : str\n",
    "            Amino acid sequence\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        states : np.ndarray\n",
    "            Predicted state sequence\n",
    "        \"\"\"\n",
    "        # Convert sequence to indices\n",
    "        sequence_indices = self.sequence_to_indices(sequence)\n",
    "        \n",
    "        # Reshape for hmmlearn (n_samples, n_features) format\n",
    "        X = sequence_indices.reshape(-1, 1)\n",
    "        \n",
    "        # Predict states using Viterbi algorithm\n",
    "        states = self.model.predict(X)\n",
    "        \n",
    "        return states\n",
    "    \n",
    "    def predict_proba(self, sequence):\n",
    "        \"\"\"\n",
    "        Predict state probabilities using Forward-Backward algorithm\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        sequence : str\n",
    "            Amino acid sequence\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        state_probs : np.ndarray\n",
    "            State probabilities for each position (n_positions x n_states)\n",
    "        \"\"\"\n",
    "        # Convert sequence to indices\n",
    "        sequence_indices = self.sequence_to_indices(sequence)\n",
    "        \n",
    "        # Reshape for hmmlearn (n_samples, n_features) format\n",
    "        X = sequence_indices.reshape(-1, 1)\n",
    "        \n",
    "        # Predict probabilities using Forward-Backward algorithm\n",
    "        state_probs = self.model.predict_proba(X)\n",
    "        \n",
    "        return state_probs\n",
    "    \n",
    "    def score_sequence(self, sequence):\n",
    "        \"\"\"\n",
    "        Calculate log-likelihood of a sequence\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        sequence : str\n",
    "            Amino acid sequence\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        log_likelihood : float\n",
    "            Log-likelihood of the sequence\n",
    "        \"\"\"\n",
    "        # Convert sequence to indices\n",
    "        sequence_indices = self.sequence_to_indices(sequence)\n",
    "        \n",
    "        # Reshape for hmmlearn format\n",
    "        X = sequence_indices.reshape(-1, 1)\n",
    "        \n",
    "        # Calculate log-likelihood\n",
    "        log_likelihood = self.model.score(X)\n",
    "        \n",
    "        return log_likelihood\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Return model parameters for inspection\"\"\"\n",
    "        return {\n",
    "            'n_states': self.n_states,\n",
    "            'n_features': self.n_features,\n",
    "            'startprob': self.model.startprob_,\n",
    "            'transmat': self.model.transmat_,\n",
    "            'emissionprob': self.model.emissionprob_,\n",
    "            'state_names': self.state_names\n",
    "        }\n",
    "\n",
    "# Initialize the protein HMM\n",
    "protein_hmm = ProteinHMM(\n",
    "    emission_probs=DATA_CONFIG['emission_probs'],\n",
    "    transition_probs=DATA_CONFIG['transition_probs'],\n",
    "    initial_probs=DATA_CONFIG['initial_probs'],\n",
    "    aa_to_index=DATA_CONFIG['aa_to_index'],\n",
    "    index_to_aa=DATA_CONFIG['index_to_aa'],\n",
    "    state_names=DATA_CONFIG['state_names']\n",
    ")\n",
    "\n",
    "# Test the HMM functionality\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING HMM FUNCTIONALITY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test 1: Generate a sample sequence\n",
    "test_length = 50\n",
    "test_sequence, true_states = protein_hmm.generate_sequence(test_length)\n",
    "print(f\"Generated sequence (length {len(test_sequence)}):\")\n",
    "print(f\"Sequence: {test_sequence}\")\n",
    "print(f\"True states: {true_states}\")\n",
    "print(f\"State names: {[protein_hmm.state_names[s] for s in true_states]}\")\n",
    "\n",
    "# Test 2: Predict states using Viterbi\n",
    "predicted_states = protein_hmm.predict_states(test_sequence)\n",
    "print(f\"\\nPredicted states (Viterbi): {predicted_states}\")\n",
    "print(f\"Predicted state names: {[protein_hmm.state_names[s] for s in predicted_states]}\")\n",
    "\n",
    "# Test 3: Get state probabilities using Forward-Backward\n",
    "state_probs = protein_hmm.predict_proba(test_sequence)\n",
    "print(f\"\\nState probabilities shape: {state_probs.shape}\")\n",
    "print(f\"First 5 positions state probabilities:\")\n",
    "for i in range(min(5, len(state_probs))):\n",
    "    print(f\"Position {i}: alpha-helix={state_probs[i,0]:.3f}, other={state_probs[i,1]:.3f}\")\n",
    "\n",
    "# Test 4: Calculate sequence likelihood\n",
    "log_likelihood = protein_hmm.score_sequence(test_sequence)\n",
    "print(f\"\\nSequence log-likelihood: {log_likelihood:.3f}\")\n",
    "\n",
    "# Test 5: Verify accuracy of Viterbi vs true states\n",
    "accuracy = np.mean(predicted_states == true_states)\n",
    "print(f\"Viterbi accuracy vs true states: {accuracy:.3f}\")\n",
    "\n",
    "# Test 6: Show model information\n",
    "model_info = protein_hmm.get_model_info()\n",
    "print(f\"\\nModel Information:\")\n",
    "print(f\"Number of states: {model_info['n_states']}\")\n",
    "print(f\"Number of features: {model_info['n_features']}\")\n",
    "print(f\"Initial probabilities: {model_info['startprob']}\")\n",
    "print(f\"Transition matrix diagonal: {np.diag(model_info['transmat'])}\")\n",
    "\n",
    "# Helper function for batch sequence generation\n",
    "def generate_training_data(n_sequences, length_range=(50, 200)):\n",
    "    \"\"\"\n",
    "    Generate training data for BayesFlow\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_sequences : int\n",
    "        Number of sequences to generate\n",
    "    length_range : tuple\n",
    "        Range of sequence lengths (min, max)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    sequences : list\n",
    "        List of amino acid sequences\n",
    "    state_probabilities : list\n",
    "        List of state probability arrays\n",
    "    true_states : list\n",
    "        List of true state sequences\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    state_probabilities = []\n",
    "    true_states = []\n",
    "    \n",
    "    for i in range(n_sequences):\n",
    "        # Random sequence length\n",
    "        length = np.random.randint(length_range[0], length_range[1] + 1)\n",
    "        \n",
    "        # Generate sequence\n",
    "        seq, states = protein_hmm.generate_sequence(length)\n",
    "        \n",
    "        # Get state probabilities using Forward-Backward\n",
    "        probs = protein_hmm.predict_proba(seq)\n",
    "        \n",
    "        sequences.append(seq)\n",
    "        state_probabilities.append(probs)\n",
    "        true_states.append(states)\n",
    "        \n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"Generated {i + 1}/{n_sequences} sequences\")\n",
    "    \n",
    "    return sequences, state_probabilities, true_states\n",
    "\n",
    "# Test batch generation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING BATCH GENERATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Generate small test batch\n",
    "test_sequences, test_probs, test_states = generate_training_data(5, (20, 30))\n",
    "print(f\"Generated {len(test_sequences)} test sequences\")\n",
    "print(f\"Sequence lengths: {[len(s) for s in test_sequences]}\")\n",
    "print(f\"State probability shapes: {[p.shape for p in test_probs]}\")\n",
    "\n",
    "print(\"\\nStep 3 completed successfully!\")\n",
    "print(\"HMM model is ready for sequence generation and state inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcce14a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TESTING PROTEIN SIMULATOR\n",
      "==================================================\n",
      "Simulator output keys: ['parameters', 'observables', 'summary_stats', 'sequence_lengths', 'raw_sequences', 'true_states']\n",
      "Parameters shape: (3, 6)\n",
      "Observables shape: (3, 171)\n",
      "Summary stats shape: (3, 171, 2)\n",
      "Sequence lengths: [152 171 124]\n",
      "\n",
      "Data validation:\n",
      "Parameter vector (first): [0.9  0.1  0.05 0.95 0.   1.  ]\n",
      "Observable sequence (first 10 positions): [19 11  2 16 14 19  3  2  9  5]\n",
      "Summary stats sum check (first seq, first 3 pos): [1. 1. 1.]\n",
      "Padding positions in first sequence: [152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169\n",
      " 170]\n",
      "\n",
      "==================================================\n",
      "TESTING ADAPTER\n",
      "==================================================\n",
      "Adapted data keys: ['parameters', 'sequence_lengths', 'raw_sequences', 'true_states', 'sequences', 'target_probs']\n",
      "Adapted sequences shape: (3, 171)\n",
      "Adapted sequences dtype: int32\n",
      "Adapted target_probs shape: (3, 171, 2)\n",
      "Sample adapted sequence (first 10): [19 11  2 16 14 19  3  2  9  5]\n",
      "\n",
      "Inverse transform successful\n",
      "Original vs inverse sequences match: True\n",
      "\n",
      "==================================================\n",
      "TESTING SIMULATION VALIDATION\n",
      "==================================================\n",
      "Validation passed: True\n",
      "Errors: 0\n",
      "Warnings: 0\n",
      "\n",
      "Simulation Analysis:\n",
      "  batch_size: 3\n",
      "  avg_seq_length: 149.000\n",
      "  std_seq_length: 19.305\n",
      "  min_seq_length: 124\n",
      "  max_seq_length: 171\n",
      "  parameter_dim: 6\n",
      "  avg_alpha_helix_prob: 0.38579899072647095\n",
      "\n",
      "Step 4 completed successfully!\n",
      "Protein simulator with BayesFlow LambdaSimulator is ready!\n",
      "Next: Implement data adapters and neural network components\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Generative Model Implementation \n",
    "# Create simulator using BayesFlow LambdaSimulator and Adapter for protein sequences\n",
    "\n",
    "import bayesflow as bf\n",
    "from bayesflow.simulators import LambdaSimulator\n",
    "from bayesflow.adapters import Adapter\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def protein_simulator_function(batch_shape):\n",
    "    \"\"\"\n",
    "    Main simulator function for BayesFlow protein secondary structure task\n",
    "    \n",
    "    This function generates protein sequences with known HMM parameters\n",
    "    and returns both the simulated data and the ground truth for training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    batch_shape : int or tuple\n",
    "        Number of protein sequences to simulate (BayesFlow passes batch_shape)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    sim_data : dict\n",
    "        Dictionary containing:\n",
    "        - 'parameters': HMM parameters (transition, emission, initial probs)\n",
    "        - 'observables': amino acid sequences as integer indices\n",
    "        - 'summary_stats': forward-backward state probabilities\n",
    "        - 'sequence_lengths': actual lengths of each sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle batch_shape (can be int or tuple)\n",
    "    if isinstance(batch_shape, tuple):\n",
    "        batch_size = batch_shape[0]\n",
    "    else:\n",
    "        batch_size = batch_shape\n",
    "    \n",
    "    # Generate variable length sequences\n",
    "    min_len, max_len = CONFIG['sequence_length_range']\n",
    "    sequence_lengths = np.random.randint(min_len, max_len + 1, size=batch_size)\n",
    "    \n",
    "    # Storage for batch\n",
    "    all_sequences = []\n",
    "    all_state_probs = []\n",
    "    all_true_states = []\n",
    "    max_length = np.max(sequence_lengths)\n",
    "    \n",
    "    # Generate each sequence in the batch\n",
    "    for i in range(batch_size):\n",
    "        length = sequence_lengths[i]\n",
    "        \n",
    "        # Generate sequence using our HMM\n",
    "        sequence, true_states = protein_hmm.generate_sequence(length)\n",
    "        \n",
    "        # Get forward-backward probabilities (summary statistics)\n",
    "        state_probs = protein_hmm.predict_proba(sequence)\n",
    "        \n",
    "        all_sequences.append(sequence)\n",
    "        all_state_probs.append(state_probs)\n",
    "        all_true_states.append(true_states)\n",
    "    \n",
    "    # Convert sequences to padded integer arrays\n",
    "    sequences_int = np.full((batch_size, max_length), -1, dtype=np.int32)  # -1 for padding\n",
    "    for i, seq in enumerate(all_sequences):\n",
    "        seq_indices = [protein_hmm.aa_to_index[aa] for aa in seq]\n",
    "        sequences_int[i, :len(seq_indices)] = seq_indices\n",
    "    \n",
    "    # Convert state probabilities to padded arrays\n",
    "    state_probs_padded = np.zeros((batch_size, max_length, 2), dtype=np.float32)\n",
    "    for i, probs in enumerate(all_state_probs):\n",
    "        state_probs_padded[i, :len(probs), :] = probs\n",
    "    \n",
    "    # Create parameter vectors (fixed for this task but included for completeness)\n",
    "    # Parameters: [transition_00, transition_01, transition_10, transition_11, initial_0, initial_1]\n",
    "    param_vector = np.concatenate([\n",
    "        DATA_CONFIG['transition_probs'].flatten(),\n",
    "        DATA_CONFIG['initial_probs']\n",
    "    ])\n",
    "    \n",
    "    # Replicate for batch\n",
    "    parameters = np.tile(param_vector, (batch_size, 1)).astype(np.float32)\n",
    "    \n",
    "    # Return simulation data in BayesFlow format\n",
    "    return {\n",
    "        'parameters': parameters,           # Ground truth HMM parameters\n",
    "        'observables': sequences_int,       # Observed amino acid sequences  \n",
    "        'summary_stats': state_probs_padded, # Forward-backward probabilities\n",
    "        'sequence_lengths': sequence_lengths.astype(np.int32),\n",
    "        # Additional metadata for debugging/analysis\n",
    "        'raw_sequences': all_sequences,\n",
    "        'true_states': all_true_states\n",
    "    }\n",
    "\n",
    "# Create the BayesFlow LambdaSimulator\n",
    "protein_simulator = LambdaSimulator(\n",
    "    sample_fn=protein_simulator_function,\n",
    "    is_batched=True  # Our function handles batching internally\n",
    ")\n",
    "\n",
    "# Test the simulator\n",
    "print(\"=\" * 50)\n",
    "print(\"TESTING PROTEIN SIMULATOR\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate test batch\n",
    "test_batch = protein_simulator.sample(batch_size=3)\n",
    "print(f\"Simulator output keys: {list(test_batch.keys())}\")\n",
    "print(f\"Parameters shape: {test_batch['parameters'].shape}\")\n",
    "print(f\"Observables shape: {test_batch['observables'].shape}\")\n",
    "print(f\"Summary stats shape: {test_batch['summary_stats'].shape}\")\n",
    "print(f\"Sequence lengths: {test_batch['sequence_lengths']}\")\n",
    "\n",
    "# Verify data quality\n",
    "print(f\"\\nData validation:\")\n",
    "print(f\"Parameter vector (first): {test_batch['parameters'][0]}\")\n",
    "print(f\"Observable sequence (first 10 positions): {test_batch['observables'][0][:10]}\")\n",
    "print(f\"Summary stats sum check (first seq, first 3 pos): {np.sum(test_batch['summary_stats'][0][:3], axis=1)}\")\n",
    "\n",
    "# Check for padding\n",
    "print(f\"Padding positions in first sequence: {np.where(test_batch['observables'][0] == -1)[0]}\")\n",
    "\n",
    "# Create adapter for data preprocessing\n",
    "adapter = Adapter()\n",
    "\n",
    "# Add transforms to handle protein-specific data\n",
    "adapter.rename(\"observables\", \"sequences\")  # Rename for clarity\n",
    "adapter.rename(\"summary_stats\", \"target_probs\")  # Target probabilities\n",
    "\n",
    "# Simple transforms\n",
    "adapter.to_array(include='target_probs')\n",
    "adapter.to_array(include='parameters')\n",
    "adapter.to_array(include='sequence_lengths')\n",
    "\n",
    "# Test the adapter\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(\"TESTING ADAPTER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    adapted = adapter(test_batch)\n",
    "    print(f\"Adapted data keys: {list(adapted.keys())}\")\n",
    "    print(f\"Adapted sequences shape: {adapted['sequences'].shape}\")\n",
    "    print(f\"Adapted sequences dtype: {adapted['sequences'].dtype}\")\n",
    "    print(f\"Adapted target_probs shape: {adapted['target_probs'].shape}\")\n",
    "    print(f\"Sample adapted sequence (first 10): {adapted['sequences'][0][:10]}\")\n",
    "    \n",
    "    # Test inverse transform\n",
    "    inverse_data = adapter(adapted, inverse=True)\n",
    "    print(f\"\\nInverse transform successful\")\n",
    "    print(f\"Original vs inverse sequences match: {np.array_equal(test_batch['observables'], inverse_data['observables'])}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Adapter error: {e}\")\n",
    "    # Use test_batch directly for now\n",
    "    adapted = test_batch\n",
    "    print(f\"Using original data format\")\n",
    "\n",
    "# Validation function\n",
    "def validate_simulation_output(sim_data):\n",
    "    \"\"\"Validate the simulation output format and content\"\"\"\n",
    "    required_keys = ['parameters', 'observables', 'summary_stats', 'sequence_lengths']\n",
    "    \n",
    "    validation_results = {\n",
    "        'valid': True,\n",
    "        'errors': [],\n",
    "        'warnings': []\n",
    "    }\n",
    "    \n",
    "    # Check required keys\n",
    "    for key in required_keys:\n",
    "        if key not in sim_data:\n",
    "            validation_results['errors'].append(f\"Missing required key: {key}\")\n",
    "            validation_results['valid'] = False\n",
    "    \n",
    "    if not validation_results['valid']:\n",
    "        return validation_results\n",
    "    \n",
    "    batch_size = len(sim_data['sequence_lengths'])\n",
    "    \n",
    "    # Check shapes\n",
    "    if sim_data['parameters'].shape[0] != batch_size:\n",
    "        validation_results['errors'].append(\"Parameters batch size mismatch\")\n",
    "        validation_results['valid'] = False\n",
    "    \n",
    "    if sim_data['observables'].shape[0] != batch_size:\n",
    "        validation_results['errors'].append(\"Observables batch size mismatch\")\n",
    "        validation_results['valid'] = False\n",
    "    \n",
    "    # Check probability validity\n",
    "    summary_stats = sim_data['summary_stats']\n",
    "    for i in range(batch_size):\n",
    "        seq_len = sim_data['sequence_lengths'][i]\n",
    "        for j in range(seq_len):\n",
    "            prob_sum = np.sum(summary_stats[i, j, :])\n",
    "            if abs(prob_sum - 1.0) > 1e-4:\n",
    "                validation_results['warnings'].append(\n",
    "                    f\"Sequence {i}, position {j}: probabilities sum to {prob_sum:.4f}\"\n",
    "                )\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Test validation\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(\"TESTING SIMULATION VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "validation = validate_simulation_output(test_batch)\n",
    "print(f\"Validation passed: {validation['valid']}\")\n",
    "print(f\"Errors: {len(validation['errors'])}\")\n",
    "print(f\"Warnings: {len(validation['warnings'])}\")\n",
    "\n",
    "if validation['errors']:\n",
    "    print(\"Errors found:\")\n",
    "    for error in validation['errors']:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "if validation['warnings']:\n",
    "    print(\"Warnings:\")\n",
    "    for warning in validation['warnings'][:3]:  # Show first 3\n",
    "        print(f\"  - {warning}\")\n",
    "\n",
    "# Analysis function\n",
    "def analyze_simulation(sim_data):\n",
    "    \"\"\"Analyze simulation output for quality metrics\"\"\"\n",
    "    batch_size = len(sim_data['sequence_lengths'])\n",
    "    \n",
    "    analysis = {\n",
    "        'batch_size': batch_size,\n",
    "        'avg_seq_length': np.mean(sim_data['sequence_lengths']),\n",
    "        'std_seq_length': np.std(sim_data['sequence_lengths']),\n",
    "        'min_seq_length': np.min(sim_data['sequence_lengths']),\n",
    "        'max_seq_length': np.max(sim_data['sequence_lengths']),\n",
    "        'parameter_dim': sim_data['parameters'].shape[1],\n",
    "        'avg_alpha_helix_prob': np.mean(sim_data['summary_stats'][:, :, 0])\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "analysis = analyze_simulation(test_batch)\n",
    "print(f\"\\nSimulation Analysis:\")\n",
    "for key, value in analysis.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nStep 4 completed successfully!\")\n",
    "print(\"Protein simulator with BayesFlow LambdaSimulator is ready!\")\n",
    "print(\"Next: Implement data adapters and neural network components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540149bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad516e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6b0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06398ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17103e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f46e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194edd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4125c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d46be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90045d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8874b363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68480d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4618ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc52740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b20961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d136a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d15d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e12c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95debadd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ef37be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e0776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add020f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a948a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6f216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16567bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66352c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7242b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce5917b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30701406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9c727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c489f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf263b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc982c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df75126f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1f769f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac905de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5340251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d295a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4e360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e48eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
