{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1fa955e",
   "metadata": {},
   "source": [
    "# Protein Secondary Structure Inference with BayesFlow\n",
    "\n",
    "_Authors: Bhanu Prasanna, Simulation-Based Inference Course - Task 5_\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates amortized Bayesian inference for protein secondary structure prediction using a two-state Hidden Markov Model (HMM) and BayesFlow. The goal is to train a neural network to predict state membership probabilities (alpha-helix vs. other) from amino acid sequences, essentially learning an amortized approximation to the Forward-Backward algorithm.\n",
    "\n",
    "### Problem Setup\n",
    "\n",
    "We use a two-state HMM where:\n",
    "- **State 0 (\"other\")**: Beta-sheets and random coils  \n",
    "- **State 1 (\"alpha-helix\")**: Alpha-helix secondary structure\n",
    "\n",
    "The HMM has fixed emission and transition probabilities based on empirical data from protein structure analysis. Given an amino acid sequence, we want to infer the probability that each position belongs to an alpha-helix or other structure.\n",
    "\n",
    "### Approach\n",
    "\n",
    "1. **Simulator**: Generate amino acid sequences using the HMM generative model\n",
    "2. **Forward-Backward**: Compute true state probabilities using hmmlearn's `predict_proba`\n",
    "3. **BayesFlow**: Train a neural network to map sequences → state probabilities  \n",
    "4. **Validation**: Compare predictions to known protein structures (human insulin 1A7F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40af8faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'tensorflow' backend\n",
      "All libraries imported successfully!\n",
      "BayesFlow version: 2.0.5\n",
      "NumPy version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple, Any, List\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set backend for BayesFlow\n",
    "if \"KERAS_BACKEND\" not in os.environ:\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "else:\n",
    "    print(f\"Using '{os.environ['KERAS_BACKEND']}' backend\")\n",
    "\n",
    "# HMM library for forward-backward algorithm\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# BayesFlow imports\n",
    "import bayesflow as bf\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"BayesFlow version: {bf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f2a5648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amino acid alphabet: ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
      "Number of amino acids: 20\n",
      "Alpha-helix emissions sum: 1.000\n",
      "Other emissions sum: 1.000\n",
      "Emission matrix shape: (2, 20)\n",
      "Transition matrix shape: (2, 2)\n",
      "Transition probabilities:\n",
      "[[0.95 0.05]\n",
      " [0.1  0.9 ]]\n",
      "Start probabilities: [1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HMM Configuration and Data Setup\n",
    "# ============================================================================\n",
    "\n",
    "# Amino acid alphabet (20 standard amino acids)\n",
    "AMINO_ACIDS = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I', \n",
    "               'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "\n",
    "# Create amino acid to index mapping\n",
    "AA_TO_IDX = {aa: idx for idx, aa in enumerate(AMINO_ACIDS)}\n",
    "IDX_TO_AA = {idx: aa for idx, aa in enumerate(AMINO_ACIDS)}\n",
    "\n",
    "print(f\"Amino acid alphabet: {AMINO_ACIDS}\")\n",
    "print(f\"Number of amino acids: {len(AMINO_ACIDS)}\")\n",
    "\n",
    "# Emission probabilities from the task description\n",
    "# Alpha-helix emissions (State 1)\n",
    "alpha_helix_probs = [\n",
    "    12, 6, 3, 5, 1, 9, 5, 4, 2, 7,    # A R N D C E Q G H I\n",
    "    12, 6, 3, 4, 2, 5, 4, 1, 3, 6     # L K M F P S T W Y V\n",
    "]\n",
    "\n",
    "# Other emissions (State 0) \n",
    "other_probs = [\n",
    "    6, 5, 5, 6, 2, 5, 3, 9, 3, 5,     # A R N D C E Q G H I\n",
    "    8, 6, 2, 4, 6, 7, 6, 1, 4, 7      # L K M F P S T W Y V\n",
    "]\n",
    "\n",
    "# Convert to probabilities (normalize by 100)\n",
    "alpha_helix_emissions = np.array(alpha_helix_probs) / 100.0\n",
    "other_emissions = np.array(other_probs) / 100.0\n",
    "\n",
    "# Verify probabilities sum to 1\n",
    "print(f\"Alpha-helix emissions sum: {alpha_helix_emissions.sum():.3f}\")\n",
    "print(f\"Other emissions sum: {other_emissions.sum():.3f}\")\n",
    "\n",
    "# Emission matrix: shape (n_states, n_features)\n",
    "# State 0: \"other\", State 1: \"alpha-helix\"\n",
    "emission_probs = np.array([other_emissions, alpha_helix_emissions])\n",
    "print(f\"Emission matrix shape: {emission_probs.shape}\")\n",
    "\n",
    "# Transition probabilities from task description\n",
    "# State 0 = \"other\", State 1 = \"alpha-helix\"\n",
    "# Always starts in \"other\" state\n",
    "transition_probs = np.array([\n",
    "    [0.95, 0.05],  # From \"other\": 95% stay, 5% to alpha-helix\n",
    "    [0.10, 0.90]   # From \"alpha-helix\": 10% to other, 90% stay\n",
    "])\n",
    "\n",
    "# Initial state probabilities (always start in \"other\")\n",
    "start_probs = np.array([1.0, 0.0])\n",
    "\n",
    "print(f\"Transition matrix shape: {transition_probs.shape}\")\n",
    "print(f\"Transition probabilities:\\n{transition_probs}\")\n",
    "print(f\"Start probabilities: {start_probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b208a23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProteinHMM initialized with fixed parameters\n",
      "Start probabilities: [1. 0.]\n",
      "Transition matrix:\n",
      "[[0.95 0.05]\n",
      " [0.1  0.9 ]]\n",
      "Emission matrix shape: (2, 20)\n",
      "\n",
      "Test sequence generation:\n",
      "Sequence: WRYTNSAFKDALYHLDKRLL\n",
      "Length: 20\n",
      "Hidden states: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "State probabilities shape: (20, 2)\n",
      "Alpha-helix probabilities: [0.         0.03629551 0.05650999 0.08060925 0.11881799 0.18938437\n",
      " 0.29388266 0.33046914 0.36745596 0.4058222  0.46311606 0.46629057\n",
      " 0.44130009 0.4414953  0.47661556 0.48309802 0.5077631  0.53701772\n",
      " 0.55680926 0.55083721]\n",
      "Other probabilities: [1.         0.96370449 0.94349001 0.91939075 0.88118201 0.81061563\n",
      " 0.70611734 0.66953086 0.63254404 0.5941778  0.53688394 0.53370943\n",
      " 0.55869991 0.5585047  0.52338444 0.51690198 0.4922369  0.46298228\n",
      " 0.44319074 0.44916279]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HMM Model Setup using hmmlearn\n",
    "# ============================================================================\n",
    "\n",
    "class ProteinHMM:\n",
    "    \"\"\"Hidden Markov Model for protein secondary structure\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Create CategoricalHMM with 2 states\n",
    "        self.model = hmm.CategoricalHMM(n_components=2, random_state=42)\n",
    "        \n",
    "        # Set the fixed parameters\n",
    "        self.model.startprob_ = start_probs\n",
    "        self.model.transmat_ = transition_probs  \n",
    "        self.model.emissionprob_ = emission_probs\n",
    "        \n",
    "        # Prevent parameter updates during fitting\n",
    "        self.model.params = \"\"  # Empty string means no parameters will be updated\n",
    "        self.model.init_params = \"\"  # Empty string means no initialization\n",
    "        \n",
    "        print(\"ProteinHMM initialized with fixed parameters\")\n",
    "        print(f\"Start probabilities: {self.model.startprob_}\")\n",
    "        print(f\"Transition matrix:\\n{self.model.transmat_}\")\n",
    "        print(f\"Emission matrix shape: {self.model.emissionprob_.shape}\")\n",
    "    \n",
    "    def generate_sequence(self, length: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Generate amino acid sequence and corresponding hidden states\n",
    "        \n",
    "        Args:\n",
    "            length: Length of sequence to generate\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (amino_acid_sequence, hidden_states)\n",
    "        \"\"\"\n",
    "        # Sample from the HMM\n",
    "        amino_acids, states = self.model.sample(length, random_state=np.random.randint(0, 10000))\n",
    "        \n",
    "        return amino_acids.flatten(), states\n",
    "    \n",
    "    def get_state_probabilities(self, sequence: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute state membership probabilities using Forward-Backward algorithm\n",
    "        \n",
    "        Args:\n",
    "            sequence: Array of amino acid indices\n",
    "            \n",
    "        Returns:\n",
    "            state_probs: Array of shape (seq_length, n_states) with probabilities\n",
    "        \"\"\"\n",
    "        # Reshape sequence for hmmlearn (expects 2D)\n",
    "        seq_2d = sequence.reshape(-1, 1)\n",
    "        \n",
    "        # Use predict_proba to get state probabilities (Forward-Backward)\n",
    "        state_probs = self.model.predict_proba(seq_2d)\n",
    "        \n",
    "        return state_probs\n",
    "    \n",
    "    def sequence_to_string(self, sequence: np.ndarray) -> str:\n",
    "        \"\"\"Convert amino acid indices to string\"\"\"\n",
    "        return ''.join([IDX_TO_AA[idx] for idx in sequence])\n",
    "    \n",
    "    def string_to_sequence(self, seq_string: str) -> np.ndarray:\n",
    "        \"\"\"Convert amino acid string to indices\"\"\"\n",
    "        return np.array([AA_TO_IDX[aa] for aa in seq_string if aa in AA_TO_IDX])\n",
    "\n",
    "# Initialize the HMM\n",
    "protein_hmm = ProteinHMM()\n",
    "\n",
    "# Test sequence generation\n",
    "test_seq, test_states = protein_hmm.generate_sequence(20)\n",
    "test_seq_str = protein_hmm.sequence_to_string(test_seq)\n",
    "test_probs = protein_hmm.get_state_probabilities(test_seq)\n",
    "\n",
    "print(f\"\\nTest sequence generation:\")\n",
    "print(f\"Sequence: {test_seq_str}\")\n",
    "print(f\"Length: {len(test_seq)}\")\n",
    "print(f\"Hidden states: {test_states}\")\n",
    "print(f\"State probabilities shape: {test_probs.shape}\")\n",
    "print(f\"Alpha-helix probabilities: {test_probs[:, 1]}\")\n",
    "print(f\"Other probabilities: {test_probs[:, 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a52ac785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing BayesFlow simulator...\n",
      "Sample data keys: ['seq_length', 'amino_acids', 'alpha_helix_probs', 'true_states']\n",
      "Sequence lengths: [[30]\n",
      " [42]\n",
      " [21]]\n",
      "Amino acids shape: (3, 50)\n",
      "Alpha-helix probabilities shape: (3, 50)\n",
      "True states shape: (3, 50)\n",
      "\n",
      "Example sequence 0:\n",
      "Length: [30]\n",
      "Amino acids: [ 8. 14. 18. 19.  0. 13. 19.  8.  3.  9. 15. 12.  5.  7.  8. 15.  3. 18.\n",
      "  3.  0.  9.  1.  0. 16. 18. 11.  1.  2. 19. 19.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Alpha-helix probs: [0.         0.01453173 0.05327759 0.10025357 0.15187754 0.1555573\n",
      " 0.15414822 0.15618935 0.17652062 0.20424414 0.20692506 0.22761549\n",
      " 0.21901613 0.16585737 0.16108303 0.17462648 0.20498015 0.2452934\n",
      " 0.30829096 0.39078447 0.41228017 0.41008446 0.39600596 0.32377043\n",
      " 0.28863704 0.27391347 0.25767338 0.22657956 0.2268793  0.23421608\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "True states: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Sequence string: HPYVAFVHDISMEGHSDYDAIRATYKRNVV\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BayesFlow Simulator Setup\n",
    "# ============================================================================\n",
    "\n",
    "def sequence_length_prior():\n",
    "    \"\"\"Sample sequence length from a reasonable range\"\"\"\n",
    "    # Protein sequences typically range from 10-500 amino acids\n",
    "    # We'll use a smaller range for computational efficiency\n",
    "    length = np.random.randint(10, 51)  # 10-50 amino acids\n",
    "    return dict(seq_length=length)\n",
    "\n",
    "def protein_sequence_simulator(seq_length):\n",
    "    \"\"\"Generate protein sequence and compute state probabilities\n",
    "    \n",
    "    Args:\n",
    "        seq_length: Length of sequence to generate\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains amino acid sequence and state probabilities\n",
    "    \"\"\"\n",
    "    # Fixed maximum length for padding (must be consistent across batch)\n",
    "    MAX_LENGTH = 50\n",
    "    \n",
    "    # Generate sequence using our HMM\n",
    "    amino_acid_seq, hidden_states = protein_hmm.generate_sequence(seq_length)\n",
    "    \n",
    "    # Compute state probabilities using Forward-Backward\n",
    "    state_probs = protein_hmm.get_state_probabilities(amino_acid_seq)\n",
    "    \n",
    "    # Extract alpha-helix probabilities (what we want to predict)\n",
    "    alpha_helix_probs = state_probs[:, 1]  # State 1 = alpha-helix\n",
    "    \n",
    "    # Pad sequences to fixed length with value 0 (will be treated as amino acid A)\n",
    "    # This is simpler and avoids issues with negative padding values\n",
    "    \n",
    "    # Pad amino acid sequence\n",
    "    padded_amino_acids = np.zeros(MAX_LENGTH, dtype=np.float32)\n",
    "    padded_amino_acids[:seq_length] = amino_acid_seq.astype(np.float32)\n",
    "    \n",
    "    # Pad alpha-helix probabilities with 0 (neutral probability)\n",
    "    padded_alpha_probs = np.zeros(MAX_LENGTH, dtype=np.float32)\n",
    "    padded_alpha_probs[:seq_length] = alpha_helix_probs.astype(np.float32)\n",
    "    \n",
    "    # Pad hidden states with 0\n",
    "    padded_states = np.zeros(MAX_LENGTH, dtype=np.float32)\n",
    "    padded_states[:seq_length] = hidden_states.astype(np.float32)\n",
    "    \n",
    "    return dict(\n",
    "        amino_acids=padded_amino_acids,\n",
    "        alpha_helix_probs=padded_alpha_probs,\n",
    "        true_states=padded_states\n",
    "    )\n",
    "\n",
    "# Create BayesFlow simulator\n",
    "simulator = bf.simulators.make_simulator(\n",
    "    [sequence_length_prior, protein_sequence_simulator]\n",
    ")\n",
    "\n",
    "# Test the simulator\n",
    "print(\"Testing BayesFlow simulator...\")\n",
    "sample_data = simulator.sample(3)\n",
    "\n",
    "print(f\"Sample data keys: {list(sample_data.keys())}\")\n",
    "print(f\"Sequence lengths: {sample_data['seq_length']}\")\n",
    "print(f\"Amino acids shape: {sample_data['amino_acids'].shape}\")\n",
    "print(f\"Alpha-helix probabilities shape: {sample_data['alpha_helix_probs'].shape}\")\n",
    "print(f\"True states shape: {sample_data['true_states'].shape}\")\n",
    "\n",
    "# Show example\n",
    "idx = 0\n",
    "print(f\"\\nExample sequence {idx}:\")\n",
    "print(f\"Length: {sample_data['seq_length'][idx]}\")\n",
    "print(f\"Amino acids: {sample_data['amino_acids'][idx]}\")\n",
    "print(f\"Alpha-helix probs: {sample_data['alpha_helix_probs'][idx]}\")\n",
    "print(f\"True states: {sample_data['true_states'][idx]}\")\n",
    "\n",
    "# Convert to amino acid string for visualization (only actual sequence length)\n",
    "actual_length = int(sample_data['seq_length'][idx])\n",
    "valid_indices = sample_data['amino_acids'][idx][:actual_length].astype(int)\n",
    "seq_str = protein_hmm.sequence_to_string(valid_indices)\n",
    "print(f\"Sequence string: {seq_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4652d51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing BayesFlow adapter...\n",
      "Original simulator output keys: ['seq_length', 'amino_acids', 'alpha_helix_probs', 'true_states']\n",
      "seq_length: (3, 1) (dtype: int64)\n",
      "amino_acids: (3, 50) (dtype: float32)\n",
      "alpha_helix_probs: (3, 50) (dtype: float32)\n",
      "true_states: (3, 50) (dtype: float32)\n",
      "\n",
      "Adapter successful! Output keys: ['inference_variables', 'summary_variables', 'inference_conditions']\n",
      "\n",
      "Data shapes after adaptation:\n",
      "inference_variables: (3, 50) (dtype: float32)\n",
      "summary_variables: (3, 50) (dtype: float32)\n",
      "inference_conditions: (3, 1) (dtype: float32)\n",
      "\n",
      "Example adapted data:\n",
      "summary_variables (amino acids): [16.  7.  6.  8.  7.  3. 13. 16.  6.  5.]\n",
      "inference_variables (alpha-helix probs): [0.         0.024004   0.07333158 0.08906678 0.11779314 0.2010955\n",
      " 0.30366385 0.4083966  0.58317083 0.688051  ]\n",
      "inference_conditions (seq_length): [31.]\n",
      "\n",
      "Value ranges:\n",
      "Amino acids: [0.0, 19.0]\n",
      "Alpha-helix probs: [0.000, 0.868]\n",
      "Sequence lengths: [16, 31]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BayesFlow Adapter Setup\n",
    "# ============================================================================\n",
    "\n",
    "# The simulator outputs individual keys, not sim_data!\n",
    "# Let's create the proper adapter using standard BayesFlow transforms\n",
    "\n",
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    # Convert any non-arrays to numpy arrays\n",
    "    .to_array()\n",
    "    # Convert from numpy's default float64 to deep learning friendly float32\n",
    "    .convert_dtype(\"float64\", \"float32\")\n",
    "    # Set up the data flow for BayesFlow\n",
    "    .rename(\"alpha_helix_probs\", \"inference_variables\")  # Target: state probabilities  \n",
    "    .rename(\"amino_acids\", \"summary_variables\")          # Input: amino acid sequences for summary network\n",
    "    .rename(\"seq_length\", \"inference_conditions\")       # Condition: sequence length for inference network\n",
    "    # Drop unnecessary variables\n",
    "    .drop([\"true_states\"])\n",
    ")\n",
    "\n",
    "# Test the adapter\n",
    "print(\"Testing BayesFlow adapter...\")\n",
    "sample_data = simulator.sample(3)\n",
    "print(f\"Original simulator output keys: {list(sample_data.keys())}\")\n",
    "\n",
    "for key, value in sample_data.items():\n",
    "    print(f\"{key}: {value.shape} (dtype: {value.dtype})\")\n",
    "\n",
    "# Test the adapter\n",
    "try:\n",
    "    adapted_data = adapter(sample_data)\n",
    "    print(f\"\\nAdapter successful! Output keys: {list(adapted_data.keys())}\")\n",
    "    \n",
    "    print(f\"\\nData shapes after adaptation:\")\n",
    "    for key, value in adapted_data.items():\n",
    "        print(f\"{key}: {value.shape} (dtype: {value.dtype})\")\n",
    "\n",
    "    print(f\"\\nExample adapted data:\")\n",
    "    print(f\"summary_variables (amino acids): {adapted_data['summary_variables'][0][:10]}\")\n",
    "    print(f\"inference_variables (alpha-helix probs): {adapted_data['inference_variables'][0][:10]}\")\n",
    "    print(f\"inference_conditions (seq_length): {adapted_data['inference_conditions'][0]}\")\n",
    "\n",
    "    # Check value ranges\n",
    "    print(f\"\\nValue ranges:\")\n",
    "    print(f\"Amino acids: [{adapted_data['summary_variables'].min():.1f}, {adapted_data['summary_variables'].max():.1f}]\")\n",
    "    print(f\"Alpha-helix probs: [{adapted_data['inference_variables'].min():.3f}, {adapted_data['inference_variables'].max():.3f}]\")\n",
    "    print(f\"Sequence lengths: [{adapted_data['inference_conditions'].min():.0f}, {adapted_data['inference_conditions'].max():.0f}]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Adapter error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01d30d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BayesFlow neural networks...\n",
      "BayesFlow networks created successfully!\n",
      "Summary network: DeepSet\n",
      "  - Element MLP (phi): [64, 32]\n",
      "  - Aggregation MLP (rho): [32, 16]\n",
      "  - Purpose: Extract features from amino acid sequences\n",
      "\n",
      "Inference network: InferenceNetwork\n",
      "  - MLP units: [32, 32, 50]\n",
      "  - Purpose: Predict alpha-helix probabilities from sequence summary + length\n",
      "\n",
      "Architecture overview:\n",
      "  Input: Amino acid sequences (batch_size, 50)\n",
      "  Summary Network: sequences -> 16D summary vector\n",
      "  Inference Network: [summary, seq_length] -> alpha-helix probabilities (50D)\n",
      "  Output: Probability distribution over sequence positions\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BayesFlow Neural Architecture Setup\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Creating BayesFlow neural networks...\")\n",
    "\n",
    "# Create summary network using DeepSet for processing amino acid sequences\n",
    "summary_net = bf.networks.DeepSet(\n",
    "    # Each element in the set (amino acid) goes through this MLP\n",
    "    phi_kwargs={\"units\": [64, 32], \"activation\": \"relu\"},\n",
    "    # The pooled result goes through this MLP to get final summary\n",
    "    rho_kwargs={\"units\": [32, 16], \"activation\": \"relu\"}\n",
    ")\n",
    "\n",
    "# Create inference network for predicting alpha-helix probabilities\n",
    "inference_net = bf.networks.InferenceNetwork(\n",
    "    mlp_kwargs={\"units\": [32, 32, 50], \"activation\": \"relu\"}  # Output 50 probabilities\n",
    ")\n",
    "\n",
    "print(\"BayesFlow networks created successfully!\")\n",
    "print(f\"Summary network: {type(summary_net).__name__}\")\n",
    "print(f\"  - Element MLP (phi): [64, 32]\")\n",
    "print(f\"  - Aggregation MLP (rho): [32, 16]\")\n",
    "print(f\"  - Purpose: Extract features from amino acid sequences\")\n",
    "\n",
    "print(f\"\\nInference network: {type(inference_net).__name__}\")\n",
    "print(f\"  - MLP units: [32, 32, 50]\")\n",
    "print(f\"  - Purpose: Predict alpha-helix probabilities from sequence summary + length\")\n",
    "\n",
    "print(f\"\\nArchitecture overview:\")\n",
    "print(f\"  Input: Amino acid sequences (batch_size, 50)\")\n",
    "print(f\"  Summary Network: sequences -> 16D summary vector\")\n",
    "print(f\"  Inference Network: [summary, seq_length] -> alpha-helix probabilities (50D)\")\n",
    "print(f\"  Output: Probability distribution over sequence positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "16d8a5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesFlow BasicWorkflow created successfully!\n",
      "Workflow components:\n",
      "  - Simulator: SequentialSimulator\n",
      "  - Inference Network: InferenceNetwork\n",
      "  - Summary Network: DeepSet\n",
      "  - Adapter: Adapter\n",
      "\n",
      "Training configuration:\n",
      "  - Batch size: 32\n",
      "  - Epochs: 5\n",
      "  - Learning rate: 0.001\n",
      "\n",
      "Testing workflow with small batch...\n",
      "Training data generated successfully!\n",
      "Batch shapes:\n",
      "  inference_variables: (32, 50)\n",
      "  summary_variables: (32, 50)\n",
      "  inference_conditions: (32, 1)\n",
      "Error during workflow testing: Exception encountered when calling Residual.call().\n",
      "\n",
      "\u001b[1mInput 0 of layer \"projector\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (64,)\u001b[0m\n",
      "\n",
      "Arguments received by Residual.call():\n",
      "  • x=tf.Tensor(shape=(64,), dtype=float32)\n",
      "  • training=False\n",
      "  • mask=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/1r/h80d31y92rn7dxwn7_1yfhsh0000gn/T/ipykernel_51683/2789587064.py\", line 43, in <module>\n",
      "    summary_output = summary_net(adapted_train_data['summary_variables'])\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/utils/decorators.py\", line 95, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/networks/summary_network.py\", line 18, in build\n",
      "    z = self.call(x)\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/networks/deep_set/deep_set.py\", line 146, in call\n",
      "    x = em(x, training=training)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/utils/decorators.py\", line 95, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/networks/deep_set/equivariant_layer.py\", line 103, in build\n",
      "    self.call(keras.ops.zeros(input_shape))\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/networks/deep_set/equivariant_layer.py\", line 131, in call\n",
      "    invariant_summary = self.invariant_module(input_set, training=training)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/utils/decorators.py\", line 95, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/networks/deep_set/invariant_layer.py\", line 119, in build\n",
      "    self.call(keras.ops.zeros(input_shape))\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/networks/deep_set/invariant_layer.py\", line 113, in call\n",
      "    set_summary = self.outer_fc(set_summary, training=training)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/networks/sequential/sequential.py\", line 54, in call\n",
      "    x = layer(x, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/networks/residual/residual.py\", line 53, in call\n",
      "    return self.projector(x) + super().call(x, training=training, mask=mask)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "ValueError: Exception encountered when calling Residual.call().\n",
      "\n",
      "\u001b[1mInput 0 of layer \"projector\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (64,)\u001b[0m\n",
      "\n",
      "Arguments received by Residual.call():\n",
      "  • x=tf.Tensor(shape=(64,), dtype=float32)\n",
      "  • training=False\n",
      "  • mask=None\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BayesFlow Training Workflow Setup\n",
    "# ============================================================================\n",
    "\n",
    "# Create the BasicWorkflow that ties everything together\n",
    "workflow = bf.workflows.BasicWorkflow(\n",
    "    simulator=simulator,\n",
    "    adapter=adapter,\n",
    "    inference_network=inference_net,\n",
    "    summary_network=summary_net\n",
    ")\n",
    "\n",
    "print(\"BayesFlow BasicWorkflow created successfully!\")\n",
    "print(f\"Workflow components:\")\n",
    "print(f\"  - Simulator: {type(workflow.simulator).__name__}\")\n",
    "print(f\"  - Inference Network: {type(inference_net).__name__}\")  \n",
    "print(f\"  - Summary Network: {type(summary_net).__name__}\")\n",
    "print(f\"  - Adapter: {type(workflow.adapter).__name__}\")\n",
    "\n",
    "# Configure training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5  # Start with fewer epochs for initial testing\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Epochs: {EPOCHS}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "\n",
    "# Test workflow with a small batch\n",
    "print(\"\\nTesting workflow with small batch...\")\n",
    "try:\n",
    "    # Generate training data\n",
    "    train_data = workflow.simulate(BATCH_SIZE)\n",
    "    adapted_train_data = workflow.adapter(train_data)\n",
    "    \n",
    "    print(f\"Training data generated successfully!\")\n",
    "    print(f\"Batch shapes:\")\n",
    "    for key, value in adapted_train_data.items():\n",
    "        print(f\"  {key}: {value.shape}\")\n",
    "        \n",
    "    # Test the summary network\n",
    "    summary_output = summary_net(adapted_train_data['summary_variables'])\n",
    "    print(f\"\\nSummary network output shape: {summary_output.shape}\")\n",
    "    print(f\"Expected: (batch_size={BATCH_SIZE}, n_summary=16)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during workflow testing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1034976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BayesFlow training...\n",
      "This may take a few minutes depending on your hardware.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bayesflow:Fitting on dataset instance of OnlineDataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot compute summaries from summary_variables without a summary network.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis may take a few minutes depending on your hardware.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Train the workflow using online training (generates data on-the-fly)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m training_history = \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_online\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_batches_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of training batches per epoch\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Number of validation simulations\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/workflows/basic_workflow.py:784\u001b[39m, in \u001b[36mBasicWorkflow.fit_online\u001b[39m\u001b[34m(self, epochs, num_batches_per_epoch, batch_size, keep_optimizer, validation_data, augmentations, **kwargs)\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    737\u001b[39m \u001b[33;03mTrain the approximator using an online data-generating process. The dataset is dynamically generated during\u001b[39;00m\n\u001b[32m    738\u001b[39m \u001b[33;03mtraining, making this approach suitable for scenarios where generating new simulations is computationally cheap.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    773\u001b[39m \u001b[33;03m    metric evolution over epochs.\u001b[39;00m\n\u001b[32m    774\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    776\u001b[39m dataset = OnlineDataset(\n\u001b[32m    777\u001b[39m     simulator=\u001b[38;5;28mself\u001b[39m.simulator,\n\u001b[32m    778\u001b[39m     batch_size=batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    781\u001b[39m     augmentations=augmentations,\n\u001b[32m    782\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43monline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_optimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/workflows/basic_workflow.py:954\u001b[39m, in \u001b[36mBasicWorkflow._fit\u001b[39m\u001b[34m(self, dataset, epochs, strategy, keep_optimizer, validation_data, **kwargs)\u001b[39m\n\u001b[32m    951\u001b[39m     \u001b[38;5;28mself\u001b[39m.approximator.compile(optimizer=\u001b[38;5;28mself\u001b[39m.optimizer, metrics=kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m     \u001b[38;5;28mself\u001b[39m.history = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapproximator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    957\u001b[39m     \u001b[38;5;28mself\u001b[39m._on_training_finished()\n\u001b[32m    958\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.history\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/approximators/continuous_approximator.py:316\u001b[39m, in \u001b[36mContinuousApproximator.fit\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    265\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[33;03m    Trains the approximator on the provided dataset or on-demand data generated from the given simulator.\u001b[39;00m\n\u001b[32m    267\u001b[39m \u001b[33;03m    If `dataset` is not provided, a dataset is built from the `simulator`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m \u001b[33;03m        If both `dataset` and `simulator` are provided or neither is provided.\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madapter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/approximators/approximator.py:139\u001b[39m, in \u001b[36mApproximator.fit\u001b[39m\u001b[34m(self, dataset, simulator, **kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m     mock_data_shapes = keras.tree.map_structure(keras.ops.shape, mock_data)\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m.build(mock_data_shapes)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/approximators/backend_approximators/backend_approximator.py:20\u001b[39m, in \u001b[36mBackendApproximator.fit\u001b[39m\u001b[34m(self, dataset, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, *, dataset: keras.utils.PyDataset, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfilter_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/approximators/backend_approximators/tensorflow_approximator.py:87\u001b[39m, in \u001b[36mTensorFlowApproximator.train_step\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tf.GradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[32m     86\u001b[39m     kwargs = filter_kwargs(data | {\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m\"\u001b[39m}, \u001b[38;5;28mself\u001b[39m.compute_metrics)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m loss = metrics[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     91\u001b[39m grads = tape.gradient(loss, \u001b[38;5;28mself\u001b[39m.trainable_variables)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/approximators/continuous_approximator.py:217\u001b[39m, in \u001b[36mContinuousApproximator.compute_metrics\u001b[39m\u001b[34m(self, inference_variables, inference_conditions, summary_variables, sample_weight, stage)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_metrics\u001b[39m(\n\u001b[32m    177\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    178\u001b[39m     inference_variables: Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m     stage: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    183\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[32m    184\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[33;03m    Computes loss and tracks metrics for the inference and summary networks.\u001b[39;00m\n\u001b[32m    186\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    214\u001b[39m \u001b[33;03m        \"inference_\" or \"summary_\" to indicate its source.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     summary_metrics, summary_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_summary_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minference_conditions\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.standardize:\n\u001b[32m    220\u001b[39m         inference_conditions = \u001b[38;5;28mself\u001b[39m.standardize_layers[\u001b[33m\"\u001b[39m\u001b[33minference_conditions\u001b[39m\u001b[33m\"\u001b[39m](inference_conditions, stage=stage)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ukk/lib/python3.12/site-packages/bayesflow/approximators/continuous_approximator.py:244\u001b[39m, in \u001b[36mContinuousApproximator._compute_summary_metrics\u001b[39m\u001b[34m(self, summary_variables, stage)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.summary_network \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m summary_variables \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot compute summaries from summary_variables without a summary network.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m summary_variables \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Cannot compute summaries from summary_variables without a summary network."
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BayesFlow Training\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Starting BayesFlow training...\")\n",
    "print(\"This may take a few minutes depending on your hardware.\\n\")\n",
    "\n",
    "# Train the workflow using online training (generates data on-the-fly)\n",
    "training_history = workflow.fit_online(\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_batches_per_epoch=100,  # Number of training batches per epoch\n",
    "    validation_data=500        # Number of validation simulations\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed successfully!\")\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Training loss\n",
    "axes[0].plot(training_history.history['loss'])\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Validation loss (if available)\n",
    "if 'val_loss' in training_history.history:\n",
    "    axes[1].plot(training_history.history['val_loss'])\n",
    "    axes[1].set_title('Validation Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].grid(True)\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No validation loss available', \n",
    "                ha='center', va='center', transform=axes[1].transAxes)\n",
    "    axes[1].set_title('Validation Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {training_history.history['loss'][-1]:.4f}\")\n",
    "if 'val_loss' in training_history.history:\n",
    "    print(f\"Final validation loss: {training_history.history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "# Test the trained model\n",
    "print(\"\\nTesting trained model...\")\n",
    "test_data = workflow.simulate(5)\n",
    "\n",
    "# Get predictions from the trained model using the workflow's sample method\n",
    "test_conditions = {\n",
    "    'summary_variables': test_data['amino_acids'],\n",
    "    'inference_conditions': test_data['seq_length']\n",
    "}\n",
    "\n",
    "# Apply adapter to test conditions\n",
    "adapted_test_conditions = workflow.adapter(test_conditions)\n",
    "\n",
    "predictions = workflow.sample(\n",
    "    num_samples=100,  # Number of posterior samples\n",
    "    conditions=adapted_test_conditions\n",
    ")\n",
    "\n",
    "print(f\"Predictions shape: {predictions['inference_variables'].shape}\")\n",
    "print(\"Model training and testing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c18f927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual data extraction from sim_data:\n",
      "Extracted amino_acids shape: (2, 50)\n",
      "Extracted alpha_helix_probs shape: (2, 50)\n",
      "\n",
      "Manual data structure:\n",
      "  inference_conditions: (2, 50)\n",
      "  inference_variables: (2, 50)\n",
      "\n",
      "Sample inference_conditions (amino acids):\n",
      "[0.59666985 0.56946033 0.5465842  0.4928938  0.4201785  0.38633034\n",
      " 0.36481977 0.3564731  0.34724712 0.36692783]\n",
      "\n",
      "Sample inference_variables (alpha helix probs):\n",
      "[0.40333015 0.43053967 0.4534158  0.50710624 0.57982147 0.6136697\n",
      " 0.63518023 0.6435269  0.6527529  0.6330722 ]\n"
     ]
    }
   ],
   "source": [
    "# Let's create a simpler approach that directly extracts what we need\n",
    "print(\"Manual data extraction from sim_data:\")\n",
    "\n",
    "# Manually extract the data we need\n",
    "amino_acids = test_sim['sim_data'][..., 0]  # First channel\n",
    "alpha_helix_probs = test_sim['sim_data'][..., 1]  # Second channel\n",
    "\n",
    "print(\"Extracted amino_acids shape:\", amino_acids.shape)\n",
    "print(\"Extracted alpha_helix_probs shape:\", alpha_helix_probs.shape)\n",
    "\n",
    "# Create a simple data structure that BayesFlow expects\n",
    "manual_data = {\n",
    "    'inference_conditions': amino_acids.astype(np.float32),\n",
    "    'inference_variables': alpha_helix_probs.astype(np.float32)\n",
    "}\n",
    "\n",
    "print(\"\\nManual data structure:\")\n",
    "for key, value in manual_data.items():\n",
    "    print(f\"  {key}: {value.shape}\")\n",
    "\n",
    "print(\"\\nSample inference_conditions (amino acids):\")\n",
    "print(manual_data['inference_conditions'][0][:10])\n",
    "print(\"\\nSample inference_variables (alpha helix probs):\")\n",
    "print(manual_data['inference_variables'][0][:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
